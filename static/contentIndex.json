{"blog/2024-GitLab-유저-컨퍼런스-후기---다양한-GitLab-적용-사례":{"title":"2024 GitLab 유저 컨퍼런스 참석 후기 - 다양한 GitLab 적용 사례","links":[],"tags":["Conference"],"content":"\n저는 지난 6월 25일에 인포그랩과 GitLab이 주최한 2024 GitLab 유저 컨퍼런스의 오프라인 밋업 기회를 얻게 되어 다녀왔는데요.😊\nGitLab의 최신 소식뿐만 아니라 다양한 기업의 흥미로운 GitLab 도입기를 들을 수 있어 너무나 유익한 시간을 보내고 왔습니다.\n이번 글에선 GitLab 유저 컨퍼런스 참석 후기를 들려드릴게요.\nAI 기반 DevSecOps 구축을 도와주는 GitLab Duo\n컨퍼런스의 첫 번째 순서는 GitLab에서 선보이는 AI 기반 서비스 GitLab Duo에 대한 소개였습니다.\nGitLab Duo는 GitLab상의 전반적인 개발 라이프사이클에 AI의 도움을 받을 수 있는 기능들로 구성되어 있는데요.\n개발 중 코드를 추천해주는 것은 물론, 개발 후 리뷰를 받을 때 리뷰어가 제안을 효율적으로 이해할 수 있도록 요약해주고 코드에 취약점이 있다면 이에 대해서 설명해주기도 한다고 합니다.\nGitLab Duo를 이용한다면 AI 기반의 DevSecOps 플랫폼을 구축이 가능하겠다는 생각이 들던 세션이었습니다.\nSK텔레콤 사내 GitLab 서비스 구축 및 운영\n다음은 SK텔레콤의 사내 GitLab 서비스 구축 및 운영에 대한 세션이었는데요.\nSK텔레콤에선 클라우드 네이티브한 사내 개발 협업 도구를 구축하기 위해 GitLab을 포함한 모든 구성요소를 Helm으로 배포 및 관리했다고 합니다.\n서비스를 사용하는 사용자를 그룹으로 나눠 그룹별 권한을 부여하고 관리하는 것을 사용자 그룹(User Group)이라고 하는데요. GitLab에서는 이런 기능을 구현하는 것이 쉽지 않아서 Atlassian의 사용자 그룹 관리 서비스인 Crowd를 이용해서 GitLab과 연동하는 방식을 채택했다는 내용이 흥미로웠습니다.\n또한 서비스 이용 시 개인식별정보가 포함되지 않도록 미리 감지하고 가림(Masking) 처리하는 시스템도 함께 구축했다는 점도 인상깊었습니다.\n삼성생명의 클라우드 기반 DevOps 구축기\n삼성생명의 발표 세션에서는 DevOps 구축을 위해 GitLab의 도입 이유와 DevOps 도입 효과에 대해 들을 수 있었습니다.\n프로젝트 관리부터 개발, CI/CD, 배포, 모니터링, 보안까지 GitLab 하나로 커버할 수 있었기 때문에 도입했다는 내용에 저도 공감함과 동시에 GitLab의 매력을 다시 한 번 느낄 수 있었는데요.\n==DevOps 도입 효과에 대한 내용에서는 프로세스 통합으로 클릭 횟수 감소, 커뮤니케이션 빈도 증가 등의 정량적 수치로 도입 효과를 표현==한 것이 인상적이었습니다.\n그룹사 ITO를 위한 GitLab\n해당 세션에선 CJ 그룹사 내 수많은 조직에 DevOps 도입을 위해 정의한 방법론들이 흥미로웠는데요.\nDevOps 수행력에 대한 진단(DOCA: DevOps Capability Assessment), 성숙도 모델을 통한 DevOps 수준 정량 평가, DevOps 성숙도 모델 시각화 및 개선점 제공을 통해 다양한 조직의 DevOps 현황을 진단 및 평가하고 앞으로 어떤 영역에 개선이 필요한지 가이드하고 있다고 합니다.\n이런 방법론들은 DevOps 엔지니어로서 현재 속한 조직의 DevOps 성숙도와 개선점에 대한 인사이트를 얻을 수 있었기 때문에, 저에게도 크게 와닿은 내용이었습니다.\nGitLab으로 프로젝트 관리 통합\n다음은 프로젝트 관리를 GitLab으로 통합한 경험을 공유해주셨던 세션이었습니다.\n프로젝트 관리를 왜 GitLab으로 통합했는지, 어떤 과정을 거쳐 통합했는지, 통합 후 효과는 어땠는지를 자세히 설명해주신 덕에 흥미롭게 들었던 기억이 있는데요.\n특히 통합 과정에서 가장 먼저 검증 사항을 정의하고, 통합 후 검증 사항이 충족되었는지 확인하는 내용은 체계적인 시스템 개선에 대한 인사이트를 얻을 수 있었습니다.\nPrivate Cluster의 DevOps 표준 플랫폼 소개\n마지막 세션은 DevOps 플랫폼이 필요한 이유와 DevOps 플랫폼 구축 과정에 대한 내용이었는데요.\n해당 세션 역시 DevOps 플랫폼을 구축하기 전에, 잘 만든 DevOps 플랫폼이란 무엇인가에 대해 먼저 정의한 점이 크게 와닿았습니다.\n우리가 어떤 문제를 해결하거나 시스템을 개선하기 위해 새로운 도구를 도입하려 할 때, 자칫하면 문제 해결보다는 ‘새로운 도구를 도입한다’는 점에 더 집중해서 주객이 전도되는 위험이 생길 수 있을 텐데요.\n만약 ==도구 자체보다는 해결해야 할 문제에 대해 미리 충분히 정의가 된다면, 시스템 개선이나 새로운 도구 도입 여정에 나침반이 되어주지 않을까==란 생각이 들었던 시간이었습니다.\n\nReferences\n\nwww.infograb.net/1cc11900-15f3-4e66-88a3-3a29999149f1\nabout.gitlab.com/gitlab-duo\n"},"blog/2024-우아콘-참여-후기---1":{"title":"[2024 우아콘 후기] AI 데이터 분석가 '물어보새' 소개 및 실시간 추천 검색어 모델링 사례 공유","links":[],"tags":["Conference"],"content":"10월 30일, 우아한형제들에서 2024 우아콘을 개최했습니다. 저도 이 행사에 참여하게 되어서, 행사 참여 후기와 제가 참석한 세션 내용을 함께 공유드리려고 합니다.\n\n이번 행사는 참여형 트랙을 포함한 총 8가지 트랙으로 구성되고, 각 트랙은 시간별로 6가지 세션으로 나눠지기 때문에, 정말 다양한 주제로 발표가 이뤄졌는데요.\n시간대별로 한 가지 세션만 선택해서 참여하는 방식이었지만, 데이터와 AI, 인프라를 주제로 한 관심가는 세션들이 동시간대에 진행되는 경우가 많아서 고르기가 쉽지 않았습니다😅\n\n그렇게 고르고 골라서 참석한 세션이다보니, 발표 내용을 정리한 분량도 꽤 커서 이번 행사 참여 후기 글은 분량을 나눠서 공유하도록 하겠습니다.\n🗂️AI 데이터 분석가 ‘물어보새’ 등장: 데이터 리터러시 향상을 위한 나만의 데이터 분석가\n\n2024 우아콘에서 저는 가장 먼저, 우아한형제들의 내부 AI 기반 데이터 분석 서비스인 물어보새를 소개하는 세션에 참석했었습니다. 이 세션에서는 ‘물어보새’ 서비스를 개발하게 된 계기와, 개발 과정 중에서 마주한 고민들을 어떻게 해결해나갔는지에 대해 발표해주셔서 재밌게 들었는데요.\n‘물어보새’ 서비스는 우아한형제들의 BADA(Baemin Advanced Data Analytics)팀이 내부 구성원들의 데이터 활용 능력을 향상시키기 위해 제공하는 AI 기반의 데이터 쿼리 생성 및 분석 툴이라고 합니다.\n‘물어보새’ 서비스가 탄생했던 계기는 아래와 같은 고민들이었다고 하는데요.\n\n사용하려는 데이터가 제대로 추출된 것이 맞을까?\n원하는 데이터 생성을 위해 어떤 테이블과 조건을 사용해야 할까?\n전달받은 쿼리문을 좀 더 쉽게 이해할 수 있는 방법은 없을까?\n\nBADA팀 이런 고민들이 생겨난 원인은 구성원들의 데이터 활용 능력이 부족하기 때문이라고 파악했고, 먼저 데이터 리터러시를 아래와 같이 정의했다고 합니다.\n\n데이터 이해\n데이터 생성\n데이터 분석\n데이터 기반 의사소통\n\n그래서 BADA팀은 데이터 기반 의사결정 역량을 강화하면 구성원들의 업무 효율이 증가할 것으로 예상하고, 이를 위해 아래와 같이 ‘물어보새’의 기능을 정의했습니다.\n\n데이터 디스커버리\n\n데이터 쿼리문 해설\n\n\nText-To-SQL\n\n데이터 쿼리문 생성 및 생성된 쿼리 요약 설명\n\n\nAgentic Analytics\nKnowledge Sharing\n\n이런 배경으로 ‘물어보새’가 개발되었고, 현재 위 기능 중 데이터 디스커버리와 Text-To-SQL을 서비스 중이라고 합니다.\n그렇다면 BADA팀에선 ‘물어보새 서비스를 구현하기 전에 어떤 요소들을 고려했을까요? 총 4개의 키워드로 나눈 고려 요소는 아래와 같습니다.\n\n체계화 - 체계적인 정보 수집\n\n데이터 플랫폼, 거버넌스, 엔지니어링 지원을 통한 정보 관리\n\n\n효율화 - 사내 정보를 쉽고 빠르게 검색\n\n사내 정보 검색 기술을 개발\n\n\n자동화 - 365일 24시간 상시 응답\n\n데이터 담당자 없이도 언제든 이용 가능하도록 지원\n\n\n접근성 - 익숙한 채널로 소통 연결\n\n업무 주 도구를 적극 활용\n\n\n\n\n(우형 기술 블로그에서도 확인 가능한 ‘물어보새’의 아키텍처입니다.)\nBADA팀은 ‘물어보새’가 개발되는 과정에서도 여러 고민들을 마주했고, 또 해결했다고 하는데요.\n이번 세션에서 공유해주신 3가지 고민과 그 해결방식을 정리하면 아래와 같습니다.\n고민 1.  우아한형제들에서 어떤 데이터를 어떻게 활용하고 있는지 LLM에게 잘 알려줄 수 있는 방법은?\n해결방식:\n\nRAG를 위해 참고할 수 있는 여러 Vector DB 구축\n\n테이블 DDL: 테이블명과 컬럼명, 컬럼 설명, 컬럼별 데이터 타입과 값 정보\n\nLLM이 테이블 구조에 대해 이해할 수 있음\n\n\n테이블 Meta: 테이블별 주요 사용 목적과 관련 서비스, 주요 키워드, 관련 질문 등\n\n질문에 답변하기 위해 필요한 테이블을 더 잘 검색할 수 있음\n\n\n비즈니스 용어 사전: 우형에서 쓰는 용어에 대한 설명\n\nLLM이 우형만의 용어를 잘 이해할 수 있게 됨\n\n\nSQL Few Shot: 우형에서 쓰는 비즈니스 로직이 반영된 쿼리문 예시\n\n\n\n고민 2. LLM이 다양하고 복잡한 질문들에 대해 더 똑똑하게 답변할 수 있는 방법은?\n해결방식:\n\nRouter Superviser로 질문 분류\n\n데이터 관련 질문인지 여부\n데이터 질문 분류\n\n질문 유형 분류\n\n\n\n\n프롬프트 최적화\n\nDynamic Prompting\n성능 평가 지표 활용\n\n\n\n고민 3. 이 서비스를 편리하게 제공할 방법은?\n해결방식:\n\n접근성\n\n가장 많이 사용되는 채털인 Slack을 통해 서비스 제공\n\n\n신속성\n\nGPT Cache DB를 활용한 빠른 답변 제공\n\n\n안정성\n\nAzure OpenAI Multi Region Load Balancing 구현\nGrafana를 활용한 서비스 실시간 모니터링\n\n\n고도화\n\n사용자 피드백 버튼 기반의 의견 수집\nLLMOps 기반 서비스 지속 고도화\n\n\n\n마지막으로, ‘물어보새’ 향후 방향성에 대해서도 아래와 같이 공유해주셨는데요.\nAgentic Analytics\n\nChain 기반에서 Graph 기반으로 변경\n데이터 분석 서비스 기능 제공\n\nKnowledge Sharing\n\n사내 모든 데이터로 지식 범위를 확장\n데이터 의사소통 전문가로서의 역할로 확장\n\n‘물어보새’가 현재는 업무 지원 단계에 있지만, 최종적으로는 ‘물어보새’가 의사결정을 담당하는 업무 자율화 단계로 거듭나고자 한다는 내용을 마지막으로 이번 세션은 마무리되었습니다.\n개발한 서비스를 유지보수하는 것을 넘어서, 더 나은 방법론을 채택하고 더 넓은 영역으로 서비스를 확장하려는 우아한형제들의 고민이 생생하게 느껴지는 세션이었습니다.\n💻그래프, 텍스트 인코더를 활용한 실시간 추천 검색어 모델링\n\n제가 두 번째로 참여한 세션은, ‘배달의민족’ 앱의 추천 검색어 기능에 사용된 모델 적용 기법을 소개하는 자리였습니다.\n‘배달의민족’의 여러 핵심 페이지를 소개하는 내용을 시작으로, 추천 검색어 기능을 도입하게된 계기인 퀵 커머스(Quick Commerce)에 대해 공유해주셨는데요.\n퀵 커머스란 1시간 이내에 원하는 장소에서 원하는 상품을 배송받는 서비스를 의미하며, 퀵 커머스의 특징은 아래와 같이 정리할 수 있습니다.\n\nExplore - 사용자는 여러 개의 상품을 둘러보는 움직임을 보임\nReal-time - 사용자의 관심사는 실시간으로 변함\nSearch - 사용자는 원하는 상품에 접근하기 위해 검색에 의존\n\n이런 배경에서 ‘만약 원하는 검색어를 타이핑 없이 클릭만으로 검색 가능하다면 편리하지 않을까?‘란 생각을 시작으로 추천 검색어 기능이 도입되었다고 합니다.\n추천 검색어 기능에 대한 소개 다음으로, 추천 검색어에 사용된 2가지 모델 적용 기법인 Batch와 Real-Time에 대해 공유해주셨는데요. 각 모델링 기법을 정리하면 아래와 같습니다.\nBatch\n\n데이터: 장기(Long-term) 히스토리\n추론: Batch Inference\n모델 사용 시점: 앱 최초 진입 시, 과거 클릭 데이터 활용 (24시간 동안 동일한 추론 결과 제공)\n프로세스\n\n데이터 불러오기\n차원 축소 (NMF)\n분류 (Multi-Layer Perceptron)\n추론 (Chunk로 나눈 다음 추론)\n후처리 작업\n\n\n\nReal-Time\n\n데이터: 단기(Short-term) 피드백\n추론: Real-time inference\n모델 사용 시점: 앱 진입 이후, 상품 클릭 및 검색 데이터를 실시간으로 활용\n프로세스\n\n인코더를 사용하여 사용자의 피드백을 임베딩 벡터로 변환\n시퀀스 모델이 임베딩 벡터를 시퀀스 임베딩으로 변환\n\n\n\n사용자의 앱 사용 시점별로 적절한 모델을 사용하여 추천 검색어 기능을 구현한 덕에, Batch 모델로 인한 CTR(Click-Through Rate, 클릭률) 개선율은 64%, Real-Time 모델로 인한 CTR 개선율은 134%를 기록했다고 하는데요. 각 모델 적용으로 눈에 띄는 변화가 있었기 때문에 유효한 개선을 이뤄냈다고 평가하셨습니다.\n해당 세션에서는 추천 검색어 기능에 적용되는 모델링 기법에 대해 기술적인 부분을 자세히 설명해주셨는데요. 비록 제가 관련 지식이 깊지 않아 모두 이해하기는 어려웠지만, 사용자의 앱 사용 시점에 따라 다른 모델링 기법을 적용하여 성과를 낸 점이 인상깊었던 세션이었습니다.\nReferences\n\ntechblog.woowahan.com/18144/\n"},"blog/2024-우아콘-참여-후기---2":{"title":"[2024 우아콘 후기] 프롬프트 엔지니어링, 클라우드 관리 및 컴플라이언스 관련 세션","links":["blog/2024-우아콘-참여-후기---1","blog/CNCF에서-졸업한-KubeEdge-프로젝트-소개"],"tags":["Conference"],"content":"지난 2024 우아콘 참여 후기 글에 이어서, 제가 참석한 컨퍼런스 세션 내용을 이번 글에서 마저 공유드리겠습니다.\n💻Fine-tuning 없이, 프롬프트 엔지니어링으로 메뉴 이미지 검수하기\n\n해당 세션은 메뉴 이미지 검수 작업을 ChatGPT 프롬프트 엔지니어링으로 수행하기 위해 어떤 시행착오를 거쳤는지 공유해주신 자리였습니다.\n우아한형제들 자체 리서치에서 ‘배달의민족’에서 메뉴 선택에 큰 영향을 주는 요소로 메뉴 이미지가 꼽혔지만, 이런 메뉴 이미지를 업로드할 때 반려되는 정책이 너무 다양하여 모든 이미지를 사람이 직접 검수하기 위해서는 많은 시간이 필요했다고 합니다. 그래서 이 문제를 해결하기 위해 ChatGPT의 프롬프트를 활용했다고 하는데요.\nChatGPT를 메뉴 이미지 검수 작업에 활용하기 위해, 먼저 아래와 같은  프롬프트 엔지니어링 과정을 거쳤다고 합니다.\n\nv1: 프롬프트 엔지니어링 없이 테스트\nv2: 프롬프트 엔지니어링 도입 시작\nv3: 프롬프트가 참고할 메뉴 이미지 검수 정책 구체화\nv4: 메뉴 이미지 검수 정책에 맹점이 없도록 세분화\n\n발표자분은 이런 과정을 거치면서 프롬프트 엔지니어링이란 단어 하나까지 신경써야 하는 세심하고 섬세한 작업이며, 문제 상황에 맞게 Task를 최적화해가고, ChatGPT와 상호작용하며 생각을 이해시켜가는 과정임을 느꼈다고 하셨습니다.\n또한 위 과정에서 멈추지 않고, 아래와 같이 프롬프트 엔지니어링을 이어나갔다고 하는데요.\n\nv6: 프롬프트 응답 출력 구조화\n\n응답 결과를 Json 포맷으로 요구\n\n\nv7: 일관된 답변을 위한 고도화\n\nCoT(Chain of Thought): ChatGPT 스스로 제출된 이미지를 설명하도록 한 다음 결과 반환\nEnsemble 기법: 여러 모델의 응답에서 다수결 기반으로 결정\n제한된 예산 내에서 효과적인 방법을 구현했지만, 좀 더 비용 절약이 필요\n\n\nv8: 허리띠 졸라매기\n\n한국어 쿼리를 영문으로 번역 후 제출하여 토큰 절약 → 응답 시간 감소 효과도 얻음\nChatGPT 호출 수를 줄이기 위해, ChatGPT를 사용하지 않아도 되는 이미지는 사내 보유 중인 모델을 바로 활용하는 방식을 사용\n다만 한국어 쿼리와 영문 쿼리의 토큰 사용량 차이는 GPT-4o 출시 이후로 많이 줄었다고 합니다.\n\n\n\n결국 이렇게 수많은 과정을 거친 결과,\n\n프롬프트의 기반이 되는 Instruction에서는 입력데이터에 대한 암시와 이중부정 표현 삭제, 정확한 행동 지침을 사용하게 되었고,\n프롬프트의 Parameter에서는 해당 서비스에 맞는 값을 찾아 조정하면서 JSON 포맷으로 응답 결과 출력 안정성을 보장했으며,\n프롬프트에서는 요구사항을 분리해서 입력하고, 결과를 한 문장으로 출력하라는 지시를 사용하여 응답을 더욱 빠르게 출력하며, 구체적이고 세분화된 정책항목과 함께 프롬프트의 응답 예시를 제공하여 더욱 정확한 응답을 이끌어냈다고 합니다.\n\n이렇게 여러 시행착오를 거쳐 서비스에 적합하도록 프롬프트를 최적화했지만, 여전히 프롬프트의 응답에서 과도한 확대가 발견되거나, 저작권 문제도 존재한다고 하는데요.\n이를 해결하기 위해 중간에 분기점을 두어, 특정 결과에 대해서는 사람이 검수하는 단계를 추가했다고 합니다.\n본 세션의 마지막 순서로, 우아한형제들이 AI를 활용하는 방식을 소개하는 시간이었는데요.\n우아한형제들에서는 먼저 생성형 AI를 안전하게 사용하기 위한 조건을 아래와 같이 정의했다고 합니다.\n\n정보가 정확한가\n공격적이거나 혐오를 담고있진 않은가\n주어진 예산 내에서 소화 가능한가\n서비스 요구사항에 수용 가능한 속도를 얻을 수 있는가\n\n본 세션 주제와 같은 생성형 AI를 활용하는 과제를 수행하면서, 위 고려 사항 중 ‘정보가 정확한가’에 대해서는 아쉬움이 남았다고 하셨습니다.\n그리고 우아한형제들에서는 생성형 AI에 대해 여전히 아래와 같은 고민이 있다고 하는데요.\n\n사람이 하던 일을 생성형 AI가 대신해서 생산성을 개선할 수 있는가?\nFine tuning하지 않은 생성형 AI는 어디까지 활용 가능할까?\n\n우아한형제들에서는 프롬프트 엔지니어링만으로 문제를 해결할 수 있을지 연구 중이며, 프롬프트 엔지니어링만으로 어렵다면 LLM 또는 LLM과 머신 러닝을 하이브리드로 적용하거나, 기존 머신러닝의 모델링도 문제 해결에 사용 중이라고 합니다. (이전 글에서 공유드렸던 ‘배달의민족’의 추천 검색어 기능도 모델링이 적용됐었죠🙂)\n프롬프트 엔지니어링은 저도 계속 관심을 가지고 있던 분야이다보니, 이번 세션은 프롬프트 엔지니어링 과정에서 마주친 문제와 개선 방식, 그 결과를 발표해주셔서 재밌었던 시간이었습니다.\n☁️0원으로 클라우드 비용, 장애, 보안까지 한 번에 관리하는 비밀 공개\n\n본 세션에서는 클라우드 리소스 정책 관리 툴 Cloud Custodian을 활용하여 사내 클라우드 리소스 정책을 체계적으로 표준화한 사례를 공유해주셨습니다.\n우아한형제들에서는 10년간 AWS 어카운트 수가 50개 이상, EC2 인스턴스의 수가 10000대 이상으로 빠르게 확대되다보니 클라우드 활용 정책을 체계적으로 관리할 필요성이 생겼다고 하는데요.\n기존에는 AWS Service Quotas로 클라우드 리소스 할당량을 관리했지만, 리소스에 대한 정책이 100여 개 이상으로 계속 추가되는 상황이었기 때문에 정책을 좀 더 체계적으로 관리하기 위해 별도의 툴을 도입하기로 했다고 합니다. 그 후보군은 아래와 같이 3가지였습니다.\n\nCloud Custodian: 비용 저렴 / 생산성 낮음 / 확장성 높음\nAWS Config: 비용 높음 / 생산성 중간 / 확장성 중간\n상용 솔루션: 비용은 솔루션마다 다름 / 생산성 높음 / 확장성 낮음\n\n그리고 이 중 오픈소스인 Cloud Custodian을 도입하게 되었는데, Cloud Custodian은 CNCF의 인큐베이팅 프로젝트여서 이미 검증된 정책 관리 툴이라고 판단했다고 합니다. CNCF의 프로젝트에 대해선 지난 KubeEdge 소개 글에서도 잠깐 언급한 적이 있었습니다.\nCloud Custodian로 대부분의 요구사항을 충족할 수 있었지만, Kubernetes 리소스에 대해서는 확인하기가 어렵다는 제약도 있었다는데요. 이를 보완하기 위해 Kubernetes 전용 정책 엔진인 Kyverno를 사용해서 Kubernetes Pod와 Image 정보를 확인했다고 합니다.\n클라우드를 운영하다보면 결국 리소스 사용에 대한 정책이 필요하게 될 텐데, 이런 정책을 어떻게 관리해야 하는가에 대해 고민해볼 수 있는 시간이었습니다.\n해당 세션에서 Cloud Custodian의 활용 예시에 대해서도 자세히 공유해주셔서 저도 많이 배웠는데요. 이 글에서 Cloud Custodian의 기술적인 부분까지 상세하게 넣진 않았지만, 클라우드 리소스 정책 관리에 대해서는 추후 좀 더 자세히 정리한 다음 다뤄보려 합니다.\n이번 시리즈를 마치며…\n지난 글에 이어서 2024 우아콘 참여 후기 및 세션 내용을 공유해드렸는데요. 이번 특집을 통해 우아한형제들이 문제를 어떻게 정의하고, 어떤 고민을 거쳐 해결했는지를 중점적으로 여러분께 공유드리고 싶었습니다.\n그럼 다음 글에서 또 다른 주제로 만나겠습니다. 감사합니다😊"},"blog/AWS-SNS와-SQS를-비교하며-살펴보기":{"title":"AWS SNS와 SQS를 비교하며 살펴보기","links":[],"tags":["AWS"],"content":"AWS에는 SNS(Simple Notification Service)라는 서비스와 SQS(Simple Queue Service)라는 서비스가 존재합니다. 두 서비스는 이름의 약자가 비슷하고 메시지를 전달 및 처리하는 데에 사용한다는 점이 동일해서 혼동하기 쉬운데요.\n그래서 이번 글에서는 AWS SNS와 SQS에 대해 비교하며 살펴보도록 하겠습니다.\n메시지(Message)란 어떤 걸 의미하나요?\nAWS SNS와 SQS에서 이야기하는 메시지는 말 그대로 문자열이 될 수도 있고, JSON 형식의 데이터일 수도 있습니다.\n이런 메시지를 전달하는 서비스가 따로 존재하는 이유는 최근 마이크로서비스 아키텍처가 보편화된 것과도 관련이 깊은데요.\n아키텍처 내에 존재하는 수많은 컴포넌트가 서로 데이터를 주고받아야 하는 상황이 빈번하다보니, 효율적이고 안전하게 데이터를 전송할 필요성이 높아진 거죠.\n이렇게 메시지를 전송할 때 주로 사용되는 패턴 2가지가 있습니다. 바로  발행/구독(Pub/Sub) 패턴과 메시지 큐(Queue) 패턴인데요.\n결론부터 이야기하자면, AWS SNS는 발행/구독 패턴을 따르고, AWS SQS는 메시지 큐 패턴을 따르는 서비스입니다.\n각 패턴의 구조를 그림으로 표현하면 아래와 같은데요. 두 구조에 대해서는 AWS SNS와 SQS 서비스에 대해 알아보면서 좀 더 자세히 살펴보겠습니다.\n\nAWS SNS는 어떤 서비스인가요?\nAWS SNS는 발행자(Publisher)에서 구독자(Subscriber)로 메시지를 전달할 수 있는 관리형 서비스입니다.\n애플리케이션과 애플리케이션 사이의(Application-to-Application, A2A) 알림 전송이 필요할 때 AWS SNS를 사용할 수 있는데요.\n뿐만 아니라 애플리케이션이 이용자에게(Application-to-Person, A2P) 알림을 전송할 때에도 활용 가능합니다. 이땐 SMS 문자나 푸시 알림, 이메일 등으로 알림 전송이 가능합니다.\nAWS SNS의 메시지 필터링이나 배치 전송, 중복 제거 등의 기능으로 아키텍처 간소화와 비용 절감 효과도 볼 수 있습니다.\n그럼 AWS SNS는 어떻게 동작할까요? 위에서 잠깐 이야기한 것처럼, AWS SNS는 발행/구독 패턴을 따르는데요.\n발행자는 토픽(Topic)이라는 논리적 접근 포인트에 메시지를 보내는 방식으로 구독자와 비동기적으로 통신할 수 있습니다.\n그리고 클라이언트는 AWS SNS의 토픽을 구독해서 토픽에 게시된 메시지를 아래 유형의 엔드포인트로 받아볼 수 있는 것입니다.\n\nAWS Lambda\nAWS SQS\nAWS Data Firehose\nHTTP\n이메일\n모바일 푸시 알림\n모바일 문자 메시지 (SMS)\n\n이런 AWS SNS의 동작 방식을 그림으로 표현하면 아래와 같습니다.\n\n(출처: AWS 공식 문서)\nAWS SNS는 메시지의 순서가 고정되도록 설정 가능하기 때문에 각 애플리케이션으로 보내지는 메시지의 정확성과 일관성 확보가 필요할 때 활용할 수 있는데요.\n그리고 이렇게 전송되는 메시지는 AWS Key Management Service(KMS)로 암호화되기 때문에, 전달 과정에 프라이버시 보호가 필요할 때에도 활용 가능합니다.\nAWS SNS는 분석, 컴퓨팅, 컨테이너, DB 등 다양한 AWS 서비스에서 발생한 이벤트를 클라이언트에게 알려야 할 때에도 유용하게 활용됩니다.\n그럼 AWS SQS는 AWS SNS랑 어떤 점에서 다른가요?\nAWS SQS는 마이크로서비스와 분산 시스템, 서버리스 애플리케이션과 함께 활용 가능한 관리형 메시지 큐 서비스입니다.\n\n(출처: AWS 공식 문서)\n대규모 데이터에 대해서도 메시지가 누락될 걱정 없이 안정적으로 전달 가능하다는 장점이 있는데요.\n또한 위의 AWS SNS와 마찬가지로, AWS Key Management 서비스를 활용해서 민감한 정보도 안전하게 전달 가능합니다.\nAWS SQS는 사용량에 따라 유연하고 비용 효율적으로 스케일 조정이 가능하다는 장점도 있습니다.\n그렇다면 AWS SQS는 어떻게 작동할까요? 아래 SQS에서 처리되는 메시지의 생명주기를 같이 살펴보며 알아봅시다.\n\n메시지 생산자가 메시지 A를 큐로 보내면 해당 메시지는 SQS 서버에 배포됩니다.\n메시지 소비자가 큐에서 메시지 A를 가져와 처리할 경우, 해당 메시지는 가시성 제한 시간(Visibility timeout) 동안 큐에 계속 남아있으나 다른 요청에 의해 전달되지 않는 상태가 됩니다.\n메시지 소비자는 가져온 메시지를 큐에서 삭제하여 가시성 제한 시간 이후에 해당 메시지가 다시 전달되거나 처리되지 않도록 합니다.\n\n참고로, 최대 메시지 보존 기간(Maximum message retention period)을 설정하면 큐에 해당 기간 이상 존재하는 메시지가 자동으로 삭제됩니다. 최대 메시지 보존 기간의 기본값은 4일입니다.\n\n\n\n\n(출처: AWS 공식 문서)\nAWS SQS는 처음에도 잠깐 언급했던 것처럼, 마이크로서비스의 각 컴포넌트를 큐 서버로 간소하고 안정적으로 연결해야 할 때 활용 가능합니다.\n그리고 프론트엔드 시스템과 백엔드 시스템을 분리시킬 때에도 AWS SQS를 활용할 수 있는데요.\n프론트엔드에선 사용자의 요청이 있을 때 즉각적인 반응을 보내고, 백엔드에선 그 요청과 관련된 복잡하거나 시간이 상대적으로 오래걸리는 작업을 수행하는 비동기적인 작업 처리에 AWS SQS를 활용할 수 있는 것입니다.\n마지막으로, 대규모 아키텍처에서 메시지의 순서는 유지하고 중복은 방지하며 메시지 처리가 필요할 때에도 AWS SQS를 활용 가능합니다.\n지금까지 AWS SNS와 SQS에 대해 살펴봤는데요. 두 서비스 모두 비동기 방식으로 메시지 전달이 가능하다는 공통점은 있지만…\n\nSNS는 하나의 메시지를 여러 구독자가 각기 다른 방식으로 활용하며,\nSQS는 하나의 메시지를 사용자가 하나의 방식으로 활용한다는 차이점이 있습니다.\n\n마치며…\nAWS에는 너무나 다양한 서비스가 존재하다보니, 정리하는 과정에서 내용이 참 방대하다는 느낌을 많이 받습니다. 이건 다른 공부나 일도 마찬가지일 텐데요.\n그래도 긴 호흡이 필요하단 것은, 그만큼 그 일의 의미가 가볍지 않기 때문이라고 생각합니다.\n지금 당장 결과가 눈에 보이지 않더라도, 낙담하지 않고 꾸준히 나아가는 한 주가 되셨으면 합니다.\n감사합니다.\nReferences\n\naws.amazon.com/sns/\ndocs.aws.amazon.com/sns/latest/dg/message-delivery.html\ndocs.aws.amazon.com/sns/latest/dg/welcome.html\ndocs.aws.amazon.com/sns/latest/dg/sns-event-sources-and-destinations.html\nseohyun0120.tistory.com/entry/AWS-SNS-vs-SQS-%EC%B0%A8%EC%9D%B4%EC%A0%90\n"},"blog/AWS-리소스-관리-툴-비교---AWS-Config-vs-Cloud-Custodian":{"title":"AWS 리소스 관리 툴 비교 - AWS Config vs Cloud Custodian","links":[],"tags":["AWS"],"content":"안녕하세요, 이번 글에서는 AWS 클라우드의 리소스를 관리할 수 있는 두 가지 툴, AWS Config와 Cloud Custodian에 대해 알아보겠습니다.\n\nAWS 리소스 관리가 왜 필요한가요?\nAWS 클라우드에서는 필요한 AWS 리소스(EC2, S3, IAM 등)를 자유롭게 구성할 수 있다는 장점이 있습니다. 하지만 클라우드를 사용하는 조직의 규모가 커질수록 AWS 리소스의 수가 많아지고 그 구성도 복잡해지기 쉬운데요.\n\n(AWS에는 다양한 리소스가 있어서 조직이나 서비스의 규모가 커질수록 체계적인 리소스 관리가 필요합니다.)\n체계적인 관리 없이 AWS 리소스가 얼기설기 얽힌 상황에서 문제라도 발생한다면… 문제 원인 찾기가 너무 힘들어지겠죠. 그래서 리소스 관리가 필요한 것입니다.\n이런 배경에서 등장한 AWS 리소스 관리 툴은 리소스 구성에 대한 규칙을 정의할 수도 있는데요. 보안을 강화하기 위해 지켜져야 할 규칙을 정의해서 AWS 리소스의 구성들이 그 규칙에 부합하는지 검사할 수 있는 것입니다.\n위에서 이야기한 두 가지 툴 중에 AWS가 웹 콘솔 상에서 리소스 관리를 할 수 있도록 제작한 서비스가 바로, AWS Config입니다.\nAWS Config에는 어떤 특징이 있나요?\nAWS Config는 AWS의 관리형 서비스이기 때문에 AWS 웹 콘솔에서 손쉽게 시작할 수 있고, 유지보수 걱정도 적습니다.\n그리고 수많은 AWS 리소스에 대해 완전히 지원하기 때문에, 문제가 발생했을 때 트러블슈팅에 유리하죠. 웹 콘솔 상에서 대시보드 화면으로 관련 정보를 쉽게 확인할 수도 있습니다.\n\n(AWS 웹 콘솔에서 확인할 수 있는 AWS Config의 대시보드 화면입니다.)\n게다가 리소스 구성의 과거 이력이 저장돼서 리소스에 대한 규칙이 준수되었는지 확인할 때 용이하답니다.\n하지만 AWS가 관리하는 서비스이기 때문에 사용에 따른 비용이 발생하는데요. AWS Config의 대상이 되는 리소스 구성 항목과 규칙당 비용이 책정되다보니, 조직의 예산 상황에 따라 큰 제약이 될 수도 있습니다.\n이와 대조적으로, 처음에 언급했던 또다른 AWS 리소스 관리 툴 Cloud Custodian은 오픈소스 프로젝트이기 때문에 툴 사용으로 인한 비용이 발생하지는 않습니다.\n그럼 Cloud Custodian이 더 좋은 건가요?\n두 가지 툴 모두 각자의 장단점이 있기 때문에 상황에 따라 고려하는 점이 다를 뿐이지, 절대적으로 무엇이 더 좋고 나쁘다고는 할 수 없습니다.\n그럼 이제 Cloud Custodian의 특징을 좀 더 살펴볼게요.\nCloud Custodian은 위에서 언급한 오픈소스라는 점 말고도 리소스 관리 규칙을 직접 정의하기가 좀 더 용이하다는 장점이 있습니다.\n웹 콘솔상에서 미리 정의된 규칙을 사용하는 AWS Config와는 달리 Cloud Custodian은 읽기 쉬운 YAML 양식 기반으로 리소스 규칙을 정의할 수 있기 때문인데요.\n참고로, Cloud Custodian은 AWS뿐만 아니라 Azure, GCP도 지원하고 있기 때문에 여러 클라우드 플랫폼을 사용하는 상황이라면 큰 장점이 될 것입니다.\n거기에 CNCF에서 인큐베이팅 단계로 속해있으니 이미 검증이 되고 있는 프로젝트라고 할 수 있겠습니다.\n\n(Cloud Custodian은 CNCF의 인큐베이팅 단계 프로젝트입니다.)\n하지만 오픈소스 프로젝트라는 특징으로 인해 생기는 단점도 있습니다. 바로 설치와 유지보수 문제인데요.\n관리형 서비스인 AWS Config와 다르게, Cloud Custodian을 잘 활용하려면 직접 툴을 설치하고 유지보수해야 하기 때문에 별도의 노력과 전문 지식이 필요합니다.\n게다가 Cloud Custodian은 대시보드를 따로 제공하지 않습니다. 만약 필요하다면 직접 구현해야 하는 거죠. 웹 콘솔 상에서 관련된 정보를 바로 확인 가능했던 AWS Config와 대비되는 점입니다.\n지금까지 AWS Config와 Cloud Custodian의 특징을 알아봤는데요. AWS 리소스 관리에 있어서 둘 중에 하나를 골라야 한다면, 두 가지 기준으로 고려해야 할 것입니다.\n바로 비용과 유연성인데요.\n만약 비용에 상관없이 바로 AWS 리소스를 관리하고 싶다면 AWS Config, 리소스 관리 규칙을 유연하게 정의하고 싶다면 Cloud Custodian이 더 적합할 것입니다.\nReferences\n\ncloudcustodian.io/\ndocs.aws.amazon.com/en_us/config/latest/developerguide/WhatIsConfig.html\nspoofing.medium.com/aws-config-cloud-custodian-or-both-98908e0b24ea\n"},"blog/AWS-클라우드의-모니터링-및-로깅-서비스-정리":{"title":"AWS 클라우드의 모니터링 및 로깅 서비스 정리","links":[],"tags":["AWS"],"content":"AWS에는 수많은 서비스가 존재합니다. 데이터베이스부터 네트워크, 컴퓨팅, 로그 모니터링 등 폭 넓은 분야를 다루는 AWS 서비스는 그 수만 200개가 넘는데요.\n같은 분야 안에 있는 서비스라 하더라도 쓰임새가 조금씩 달라서 각 차이를 명확히 알아야 올바르고 효율적으로 사용 가능합니다.\n그래서 이번 글에서는 AWS의 로그 모니터링 기능 분야의 서비스를 비교해보려 합니다.\nAWS의 대표적인 모니터링 및 로깅 서비스에는 3가지가 있습니다. 바로 CloudWatch, CloudTrail, X-Ray인데요.\n서비스의 이름만으로는 각기 어떤 역할을 하는지, 어떤 차이가 있는지 알기 어렵습니다.\n그래서 이번 글에서 이 3개의 AWS 서비스를 같이 살펴보고 비교해보도록 하려는데요. CloudWatch 먼저 알아볼게요.\nCloudWatch는 언제 사용하나요?\nCloudWatch는 AWS 클라우드 위에서 동작하는 리소스와 애플리케이션의 성능(또는 퍼포먼스)을 모니터링할 수 있는 서비스입니다.\n우리가 CloudWatch를 사용하면…\n\nAWS 리소스로부터 데이터를 수집하기 때문에 리소스의 퍼포먼스를 시각화 가능하고,\n리소스 퍼포먼스의 변화에 알람을 보내거나 자동으로 대응하도록 설정 가능하며,\n운영 중에 문제가 없는지 통합적으로 확인 가능하다는 장점이 있는데요.\n\n\n(출처: AWS 공식 문서)\nCloudWatch는 기본적으로 리소스의 메트릭(시간이 지나면서 변화하는 데이터, 예: 리소스 사용률)이 저장되는 공간인데요.\nEC2(AWS의 컴퓨팅 서비스)와 같은 AWS 리소스가 CloudWatch에 메트릭을 저장하면, 사용자는 위와 같이 리소스의 퍼포먼스를 메트릭의 통계로 확인할 수 있는 것입니다. 참고로 사용자가 커스텀하게 정의한 메트릭도 CloudWatch에서 저장 및 통계가 가능합니다.\n\n(출처: AWS 공식 문서)\n그리고 CloudWatch의 알람 기능은 이렇게 저장된 메트릭이 사용자 정의된 특정 조건에 부합하면 미리 지정한 행동(리소스 중지, 시작, 제거 등)을 취하는 것을 말합니다.\n정리하자면, 메트릭을 저장하고 통계를 제공하는 CloudWatch는 아래와 같이 활용 가능합니다.\n\n애플리케이션 성능 모니터링: 성능 데이터를 시각화하고 알람을 설정해서 AWS 리소스의 성능 이슈를 파악하고 해결\n문제 원인 분석: 문제 발생 시 메트릭을 분석해서 원인 파악 및 해결\nAWS 리소스 사용량 최적화: 미리 정의한 특정 AWS 리소스의 메트릭(예: EC2의 CPU 또는 메모리 사용량)이 임계값에 도달 시 수행해야 할 조치를 설정해서 리소스 사용량을 자동으로 조정하고 비용 절약\n\nCloudTrail도 비슷한 거 아닌가요?\nCloudTrail의 이름에도 Cloud가 포함되기 때문에 CloudWatch와 유사한 서비스라고 생각하기 쉬운데요. 사실 두 서비스의 성격은 꽤 다릅니다.\nCloudWatch가 리소스의 메트릭을 지켜본다면(Watch), CloudTrail은 AWS 사용자가 어떤 행위를 했었는지 추적(Trail)하는 서비스이기 때문인데요.\n\n(AWS CloudTrail은 AWS 계정이 어떤 행위를 했는지 그 흔적을 추적합니다.)\nAWS 계정에 대해 규정 준수 여부를 확인하거나 감사(Audit)를 수행하기 위해 만들어진 서비스가 바로, CloudTrail입니다.\n사용자는 AWS 계정이 새로 생성되는 시점부터 CloudTrail에 접근 가능한데요. 특히 CloudTrail의 Event History 기능을 사용해서 지난 90일 동안 AWS 계정에서 발생한 이벤트(AWS 리소스 생성, 수정 제거 등)의 기록을 확인, 검색, 다운로드할 수 있게 됩니다.\nEvent History에서는 지난 90일 동안 발생한 AWS 계정의 이벤트만 조회 가능하기 때문에, 이런 이벤트를 더 오래 보관하려면 CloudTrail Lake라는 기능으로 이벤트 데이터 저장소를 생성해야 하는데요.\nCloudTrail Lake를 사용 시 추가 비용이 발생할 수 있지만, 해당 기능에서는 아래와 같이 대시보드를 통해 AWS 계정의 이벤트 트렌드를 시각화할 수 있습니다.\n\n(출처: AWS 공식 문서)\n그래서 CloudTrail은 아래와 같이 활용 가능합니다.\n\n규정 준수 및 감사: 규정 준수 여부 증명에 필요한 정보를 CloudTrail 로그로 활용\n보안: AWS 계정의 이용자 및 API 행위를 기록하여 보안성 제고\n운영: AWS 계정에서 발생한 이벤트를 검색하여 이슈 분석에 활용 가능. CloudTrail Lake 대시보드에서 AWS 계정 이벤트의 트렌드를 시각화\n\n이렇게 CloudTrail과 CloudWatch는 성격과 쓰임새가 서로 다릅니다.\n그럼 X-Ray는요?\nAWS X-Ray는 프로덕션 단계 또는 배포된 애플리케이션을 분석하고 디버깅 가능한 서비스인데요.\nX-Ray를 사용하면…\n\n애플리케이션에 대한 사용자의 요청(Request)을 쉽게 추척 가능하고,\n병목 현상과 높은 대기 시간(Latency)을 파악할 수 있어 애플리케이션의 성능 향상에 활용 가능하며,\n실시간으로 서버리스 애플리케이션을 디버깅함으로써 클라우드 사용 비용 절약과 성능 향상이 가능하다는 이점이 있습니다.\n\n애플리케이션 성능이라고 하니 위에서 살펴봤던 CloudWatch가 떠오르는데요.\nCloudWatch는 애플리케이션이나 AWS 리소스에 대한 메트릭을 수집하고 통계 및 조치가 가능하다고 했었죠.\n하지만 X-Ray는 해당 애플리케이션을 거치는 사용자의 요청을 기반으로 디버깅을 하거나, 성능 분석이 가능하다는 점에서 CloudWatch와 다릅니다.\nX-Ray의 동작 방식을 살펴보면 그 차이점을 좀 더 명확하게 알 수 있는데요.\n\n(출처: AWS 공식 문서)\n먼저 X-Ray를 사용하려는 애플리케이션에서 X-Ray SDK를 통해 X-Ray 데몬으로 세그먼트 정보를 전송합니다.\nX-Ray SDK(Software Development Kit)는 애플리케이션 내에서 자체 수정 없이도 메타 데이터를 쉽게 기록하고 AWS의 X-Ray로 전달할 수 있도록 고안된 소프트웨어 개발 도구 모음인데요. C#, Java, Go, Node.js, Python, Ruby로 개발된 애플리케이션에서 X-Ray SDK를 사용 가능합니다.\n참고로, AWS Lambda, AWS Elastic Beanstalk 등에서는 X-Ray와 바로 연동 가능한 설정이 존재합니다.\n세그먼트(Segment)란 X-Ray에서 사용되는 개념으로, 호스트 정보와 요청 및 응답, 작업 내용, 발생 이슈에 대한 정보가 담긴 메타 데이터를 말합니다.\nX-Ray 데몬은 전달받은 데이터를 X-Ray 콘솔로 다시 전송하는데요. 이후 사용자는 웹 브라우저 상에서 X-Ray 콘솔로 접근하여 애플리케이션 분석 및 디버깅이 가능하게 되는 것입니다.\nX-Ray 콘솔에서는 아래와 같이 애플리케이션이 요청을 받고 보내는 애플리케이션과 AWS 리소스를 그림으로 보여줄 뿐만 아니라, 각 요청과 응답에 걸리는 시간도 함께 보여주기 때문에 병목 현상이나 레이턴시도 확인 가능합니다.\n\n(출처: AWS 공식 문서)\n지금까지 살펴본 X-Ray의 쓰임새를 정리하면 아래와 같습니다.\n\n애플리케이션 분석 및 디버깅: 애플리케이션으로부터 생성되는 트레이스 정보를 가지고 분석 및 디버깅\n서비스 맵 생성: 위에서 살펴본 X-Ray 콘솔과 같이 AWS 리소스로부터 데이터를 가져와, 구현한 클라우드 아키텍처에 병목 현상은 없는지 그림으로 표시해주기 때문에 이를 가지고 애플리케이션 성능 확인 및 향상에 활용\n\n이렇게 AWS의 로깅 및 모니터링 서비스인 CloudWatch, CloudTrail, X-Ray에 대해 살펴봤는데요.\n저도 위와 같이 정리하고 비교하면서 각각의 성격과 차이점이 더욱 명확하게 와닿았고, 앞으로 더 적재적소에 활용 가능하겠다는 생각이 들었습니다.\n여러분들에게도 이번 글이 AWS에서 로깅 및 모니터링 서비스를 사용하기 시작할 때 도움이 되길 바랍니다.\nReferences\n\ndocs.aws.amazon.com/whitepapers/latest/aws-overview/introduction.html\naws.amazon.com/cloudwatch/\naws.amazon.com/cloudtrail/\ndocs.aws.amazon.com/xray/latest/devguide/aws-xray.html\njibinary.tistory.com/334\nmedium.com/@himanshkumar/a-brief-introduction-to-aws-x-ray-1437ac23f549\n"},"blog/Admission-Controller-소개":{"title":"Admission Controller 소개","links":[],"tags":["Kubernetes","Policy"],"content":"Admission Controller는 k8s 클러스터에서 관리자가 정의한 정책(Policy)을 수행(Admission Control)하는 플러그인입니다.\n\nAdmission Controller를 사용하는 이유\n\n보안성 측면: 인증 및 인가를 거친 요청에 대해 미리 정의한 정책을 적용하므로 보안성을 높일 수 있습니다.\n제어성 측면: 클러스터에 배포하는 App에 적절한 Label을 자동으로 기입하거나 App이 사용할 Memory 또는 CPU의 규모를 자동으로 제한하여, 클러스터 내 배포하는 리소스를 더욱 효과적으로 제어할 수도 있습니다.\n\nAdmission Controller 작동 방식\n위의 표처럼, k8s API 서버로 온 요청은 인증(Authentication)과 인가(Authorization) 과정을 먼저 거친 다음 해당 요청에 대한 Admission Control이 수행됩니다.\nAdmission Control은 아래와 같이 2단계로 나눠 진행됩니다.\n\n변경(Mutate) 단계: 들어온 요청과 매칭되는 정책이 해당 요청의 일부 값을 변경하는 Mutate 정책일 경우, 해당 요청에서 변경되어야 하는 부분을 알려주는 값을 k8s API 서버로 반환합니다.\n검증(Validate) 단계: 들어온 요청과 매칭되는 정책이 해당 요청을 허용/거부하는 Validate 정책일 경우, 해당 요청이 정책에 부합하는지에 대해 true/false 값을 k8s API 서버로 반환합니다.\n\nAdmission Controller를 사용하는 방법\nk8s 클러스터에는 Pod에 사용하는 Image나 인증서 등에 관한 다양한 Admission Controller가 기본적으로 준비되어 있습니다. (관련 링크) Admission Controller를 사용하려면 k8s API 서버의 config에 아래 내용을 추가합니다.\n--enable-admission-plugins={추가하려는 Plugin 이름}\n\nAdmission Controller를 직접 구현하는 방법도 있습니다. k8s API 서버로부터 오는 요청을 받아서 특정 정책에 따른 로직을 수행 후 Admission Control 결과 양식에 따라 결과값을 반환하는 서비스를 구현하는 방식인데요.\n이때 Admission Webhook이 사용되며, 이는 외부에서 작동하는 Admission Controller가 Admission Control 요청을 받고 이에 따른 특정 행위를 수행하도록 도와주는 역할을 합니다. 서드파티로 개발된 Kyverno와 같은 정책 엔진도 Admission Webhook을 사용합니다.\nReferences\n\nAdmission Controllers Reference\nA Guide to Kubernetes Admission Controllers\n"},"blog/CIS-Kubernetes-Benchmark":{"title":"CIS Kubernetes 벤치마크 알아보기","links":[],"tags":["Security"],"content":"\nCIS Benchmark란, 사이버 보안 향상을 위해 활동하는 국제 조직 Center of Internet Security에서 제안하는 IT 인프라 보안 관련 우수 사례 및 가이드라인을 의미합니다. 현재 다양한 종류의 CIS Benchmark가 존재하며, 이 중 Kubernetes와 관련된 것이 CIS Kubernetes Benchmark입니다.\nCIS Kubernetes Benchmark의 목적\n\nKubernetes에 적합한 보안 가이드라인을 기업과 조직에게 제공하여, 현재 알려져있는 Kubernetes 관련 취약점과 부적절한 설정(Misconfiguration)으로부터 안전한 Kubernetes 환경을 구축하는 것입니다.\n기업 내 엔지이너의 입장에선 일정한 툴을 사용하는 것만으로 Kubernetes 클러스터가 CIS Benchmark에서 권장하는 기준에 부합하는지 확인하고 적절한 조치를 취할 수 있게 됩니다.\n\nCIS Kubernetes Benchmark에서 다루는 범위\n\nKubernetes 클러스터 설정\nWorker Node 설정\nNetwork Policy\nRole-Based Access Control (RBAC)\nSecret 관리\nLogging 및 Monitoring\nPod 보안 정책\n컨테이너 보안\n\nCIS Kubernetes Benchmark의 구성 요소\nBenchmark는 다수의 권장 사항으로 구성되어 있습니다. 그리고 각 사항의 검토 결과는 아래 4가지 속성을 가지고 있습니다.\n\n\nScoring\n\nScored: 권장 사항이 Scored라면, 해당 권장 사항을 준수하지 못할 경우 최종 Benchmark 점수가 감소합니다.\nNot Scored: 권장 사항이 Not Scored라면, 해당 권장 사항을 준수하지 못해도 최종 Benchmark 점수는 감소하지 않습니다.\n\n\n\nLevels\n\nLevel 1: 명확한 보안 이점을 제공하거나, 기능성을 제한하지 않는 권장 사항이라는 뜻입니다.\nLevel 2: Level 1의 확장된 수준입니다. 심층적이거나 기능성에 부정적인 영향을 줄 수도 있는 권장 사항을 의미합니다.\n\n\n\nResults\n\nPass: 권장 사항을 준수했음을 의미합니다.\nFail: 권장 사항을 준수하지 못함을 의미합니다.\n\n\n\nResponsibility\n\n권장 사항을 준수해야 하는 주체를 의미합니다.\n주로 조직(Organization)으로 명시되지만, 특정 경우에는 서비스 제공자(vendor) 등이 명시되기도 합니다.\n\n\n\nCIS Kubernetes Benchmark 툴\n가장 보편적으로 쓰이는 툴은 kube-bench입니다.\n\nkube-bench는 해당 레파지토리에서 제공하는 yaml 파일을 받아온 뒤, 검토를 원하는 Kubernetes 클러스터 안에서 Job object로 실행하는 방식입니다.\n실행한 Job이 완료되면 해당 Pod의 로그로 Benchmark 검토 결과를 확인할 수 있습니다.\n\nReferences\n\nwww.armosec.io/glossary/cis-kubernetes-benchmark\nKubernetes CIS Benchmark: Why You Need It and Getting Started\n"},"blog/CKA-취득-후기":{"title":"CKA 취득 후기 공유","links":[],"tags":["Certificate"],"content":"\n그동안 저는 Linux 재단에서 주관하는 Kubernetes 자격증을 꾸준히 취득하고 있었습니다. 그러다 마침내 저번주 KCSA 자격증 취득을 마지막으로, Linux 재단의 Kubernetes 자격증 5종을 모두 취득하게 되었는데요.\n이를 기념하고자 각 Kubernetes 자격증에 대한 소개와 저의 취득 후기를 시리즈로 구성하여 공유해보겠습니다.😃\n그 시작은 제가 가장 처음에 취득한 CKA(Certified Kubernetes Administrator)입니다.\n\nCKA 시험은 Kubernetes 관련 자격증 중 가장 많이 알려져있고 대중적입니다. CKA 자격증에 대한 정보는 아래와 같이 정리할 수 있습니다.\n\n응시료: $395\n\n코드를 입력하면 20% 또는 30% 할인해주는 쿠폰을 자주 배포하므로, 응시료 결제 전에 cka discount coupon을 찾아보는 것이 응시료를 절약할 수 있는 방법입니다.\n\n\n기본 언어: 영어\n\nLinux 재단 주관 Kubernetes 시험은 모두 기본 언어가 영어이므로, 시험 지문 역시 모두 영어인 점을 참고해야 합니다.\n\n\n시험 유형: 온라인 감독 환경에서 실습형\n응시 가능 기간: 결제 후 1년\n자격증 유효 기간: 합격 후 2년\n불합격 시 1회 재응시 기회 제공\n\nCKA에 응시한 이유\nKubernetes에 관심이 있어 공부를 하고 싶었는데, 어떻게 공부하면 좋을지 고민이 됐었습니다. 그러던 중에 무언가 남는 것이 있으면 좋겠다는 생각에 Kubernetes 자격증을 응시하기로 한 것이죠.\nKubernetes 자격증에 대해 조사해보니 CKA가 가장 많이 알려져있었고, Kubernetes 관리자(Administrator)를 위한 시험이기도 하니 Kubernetes에 대해 포괄적으로 다룰 수 있을 것 같아 응시하게 되었습니다.\nCKA 준비 방법\n가장 큰 도움을 받은 것은 Udemy의 Certified Kubernetes Administrator (CKA) with Practice Tests 강의입니다.\n\nKubernetes의 전반적인 이론과 개념을 알기 쉽게 설명해주는 강의였습니다.\n실습 과정이 포함되어 있어 Kubernetes의 기본기를 익히기에 좋았습니다.\n참고로 해당 강의는 영어로 촬영된 강의입니다. (Udemy의 기계 번역 자막이 지원되었던 걸로 기억합니다.)\n\nCKA 취득으로 얻은 것들\n그렇다면 저는 CKA를 취득하면서 어떤 것을 얻었을까요?\n가장 먼저, Kubernetes 기본 지식을 학습하면서 Kubernetes에 대한 관심이 더 깊어졌습니다. 이는 저에게 큰 영향을 주었고, 이후 제가 클라우드 엔지니어링 쪽으로 커리어를 쌓고자 한 계기가 되었습니다.\n그리고 CKA 자격증 취득 후, 제가 Kubernetes에 관심이 있고 꾸준히 공부하고 있음을 쉽게 알릴 수 있었습니다. 그리고 이러한 점은 이직 당시에도 유리하게 작용했다고 생각합니다."},"blog/CKS-취득-후기":{"title":"CKS 취득 후기 공유","links":["📆-Project/Guide-to-DevOps/KO/CKA-취득-후기"],"tags":["Certificate"],"content":"\nCKS(Certified Kubernetes Security Specialist)는 클라우드 보안 지식을 중점적으로 확인하는 시험이며, 제가 두 번째로 취득한 Kubernetes 자격증입니다. CKS 자격증 취득을 계기로 저는 클라우드 보안과 클라우드 네이티브 프로젝트들에 대해 더 관심을 가지고 공부하게 되었죠.\n출제 범위를 제외한 CKS 자격증의 대한 응시 정보는 CKA와 거의 유사합니다.\n\n응시료: $395\n기본 언어: 영어\n시험 유형: 온라인 감독 환경에서 실습형\n응시 가능 기간: 결제 후 1년\n자격증 유효 기간: 합격 후 2년\n불합격 시 1회 재응시 기회 제공\n\nCKS에 응시한 이유\nCKS는 다른 Kubernetes 자격증과 달리 유일하게 응시 조건이 있었는데요. 바로 CKA 자격증을 먼저 취득해야 하는 것입니다. 그래서 CKS가 CKA의 상위 자격증이란 인식이 있었고, 실제로 난이도도 Kubernetes 자격증 중 가장 어렵다는 평이 있었기에, Kubernetes 역량을 더 키우고자 도전하게 되었습니다.\n또한 CKS의 출제 범위인 클라우드 보안에 대해서도 저는 거의 몰랐기 때문에 이 분야에 대해 공부하고 싶은 마음도 있었습니다.\nCKS 준비 방법\nUdemy 온라인 강의인 Kim Wüstkamp의 Kubernetes CKS Complete Course를 수강하면서 시험을 준비했었는데요. 앞서 말씀드렸던 것처럼, 저는 보안쪽 지식이 거의 없었기에 이 강의에 포함되었던 모든 이론 설명 영상과 실습 문제를 풀면서 개념을 익히고 시험을 준비했습니다.\n해당 강의를 모두 수강 후 시험에 합격할 수 있었으니, 클라우드 보안에 대한 개념을 잘 잡아주는 강의였다고 생각합니다.\n또한 강의 중간중간에 클라우드 보안과 관련된 좋은 블로그 글이나 컨퍼런스 발표 영상도 공유해줘서 관련 개념을 이해하는 데에 도움이 되었습니다.\n그리고 KodeKloud의 강의와 유사하게 Killer Shell이라는 플랫폼을 통해 클라우드 보안 관련 실습 문제를 풀 수 있었는데요. 역시 관련 개념을 잡기에 좋았습니다.\nCKS 취득으로 얻은 것들\nCKS를 취득하면서 클라우드 보안 분야에 대한 기본적인 지식을 얻었고, 해당 분야에 대해 더 깊이 관심을 가지게 되었습니다. 그동안 경험했던 DevOps 엔지니어링 분야와는 또다른 느낌이어서 신선하기도 했고, 보안도 소프트웨어 개발 및 운영에 있어 필수적인 것이라는 생각이 들어서요.\n그리고 CKS의 출제 범위에는 다양한 보안 관련 Third-party 툴들이 포함되는데요. 그 덕분에 이런 툴들을 대략적으로 경험할 수 있었고, 이로 인해 클라우드 네이티브 프로젝트들에 대해서도 관심을 가지게 되었습니다.\n마지막으로, 가장 난이도 높은 Kubernetes 자격증을 취득했다는 사실에 Kubernetes에 대한 자신감이 더 상승했습니다.😁\n물론 실무에서 활용할 수 있는 역량도 함께 길러야 하겠지만요.👍"},"blog/CNCF에서-졸업한-KubeEdge-프로젝트-소개":{"title":"최근 CNCF에서 졸업한 KubeEdge 프로젝트 소개","links":[],"tags":["Kubernetes"],"content":"🎓CNCF로부터 졸업한 KubeEdge\n\n2024년 10월, KubeEdge라는 엣지 컴퓨팅(Edge Computing) 오픈소스 프로젝트가 졸업(Graduated) 단계가 되었다고 CNCF가 밝혔습니다.\n클라우드 네이티브 소프트웨어 생태계 발전을 위해 설립된 CNCF(Cloud Native Computing Foundation)는 오픈소스 클라우드 네이티브 프로젝트를 육성하고 유지하기 위해 다양한 프로젝트를 관리하고 있는데요.\nCNCF에서 관리하는 프로젝트들은 성숙도에 따라 샌드박스(Sandbox), 인큐베이팅(Incubating), 졸업(Graduated) 단계를 부여받습니다.\n프로젝트가 졸업 단계에 돌입했다는 것은, 충분히 성숙하여 실제 운영환경에 해당 프로젝트를 안정적으로 적용할 수 있음을 의미하는데요.\nKubernetes, Argo CD, Helm, Prometheus 등 우리에게 익숙한 여러 클라우드 네이티브 프로젝트가 이미 CNCF의 졸업 단계를 거쳤답니다.\n☁️KubeEdge는 어떤 프로젝트?\n\n그렇다면 이번에 CNCF에서 졸업한 KubeEdge는 어떤 프로젝트일까요?\n위에서 KubeEdge는 오픈소스 엣지 컴퓨팅 프로젝트라고 잠깐 언급했었는데요. 엣지 컴퓨팅이란, 데이터를 생성한 위치와 가까운 곳(또는 가장자리라고 해서 Edge라는 단어가 붙습니다.)에서 처리, 분석 및 저장하는 기술을 말합니다.\n예를 들면, 자율 주행 차량에 설치된 다양한 IoT 센서가 실시간으로 수집하는 수많은 데이터를 즉각적으로 처리할 수 있도록 엣지 컴퓨팅 기술을 사용할 수 있는데요. 수집한 데이터를 처리하기 위해 중앙 데이터 센터로 전송하던 기존의 방식보다 대기 시간과 통신 비용이 대폭 줄어든다는 장점이 있습니다.\nKubeEdge는 이런 엣지 컴퓨팅 기술을 Kubernetes 기반에서 구현할 수 있도록 도와주는 프로젝트입니다. 컨테이너에서 동작하는 애플리케이션을 디바이스(Edge) 내에 배포해서 사용할 수 있는 것인데요. KubeEdge는 클라우드와 엣지 환경으로 구분되어, 관리자가 클라우드와 연결된 Kubernetes API Server를 이용하여 중앙 집중형 관리가 가능합니다.\n\nKubeEdge를 사용해서 얻을 수 있는 또 다른 이점은 아래와 같습니다.\n개발 간소화\n개발자는 쉽게 접할 수 있는 HTTP 또는 MQTT 기반으로 애플리케이션을 개발하고 이를 클라우드나 엣지 환경 중 원하는 곳에 동작시킬 수 있습니다.\n네이티브한 Kubernetes 지원\nKubeEdge를 사용하면 애플리케이션 배포, 노드(디바이스) 관리, 모니터링과 같은 활동을 기존의 Kubernetes 환경과 동일하게 수행 가능합니다.\n배포 간소화\n이미 Kubernetes 상에서 배포되어 동작하던 리소스는 엣지 환경에서도 쉽게 재배포가 가능합니다.\n❕KubeEdge CNCF 졸업의 시사점\nKubeEdge 프로젝트는 2018년 화웨이 클라우드에서 오픈소스로 공개되었고, 2019년에 CNCF에서 첫 번째 클라우드 네이티브 엣지 컴퓨팅 프로젝트로서 샌드박스 단계로 합류했는데요. 2020년 인큐베이팅 단계, 그리고 올해 졸업 단계를 거치면서 110여개의 조직의 컨트리뷰션을 받아왔습니다.\nCNCF는 KubeEdge가 스마트 에너지, 자동차, 물류, 블록체인, 위성 시스템 분야까지 폭넓게 적용되었다고 언급했는데요.\nKubernetes 기반 엣지 컴퓨팅 프로젝트인 KubeEdge가 CNCF의 공식 졸업 단계를 거침으로써, 앞으로 Kubernetes의 적용 범위는 더 큰 폭으로 확장될 것으로 예상됩니다.\nReferences\n\nwww.cncf.io/announcements/2024/10/15/cloud-native-computing-foundation-announces-kubeedge-graduation/\nkubeedge.io/docs/\nwww.redhat.com/ko/topics/edge-computing/what-is-edge-computing\nwww.cncf.io/projects/\n"},"blog/Clair-소개":{"title":"컨테이너 보안 스캐닝 툴 Clair 소개","links":["blog/컨테이너-보안-스캐닝"],"tags":["Security"],"content":"Clair는 컨테이너 이미지의 구성요소를 분석하여 취약점을 보고해주는 애플리케이션입니다. 이러한 활동을 컨테이너 보안 스캐닝이라고 하며, 더 자세한 설명은 여기서 확인하실 수 있습니다.\nClair의 동작 방식\nClair는 Indexer, Matcher, Notifier 컴포넌트로 구성되며, 취약점 분석은 Indexing과 Matching 순서로 진행됩니다.\n\n\n\nIndexing\n\n먼저 Clair의 Indexer가 스캔 대상인 컨테이너 이미지를 분석하여 이미지에 포함된 패키지, 이미지의 Base 이미지, 이미지에서 사용되는 레파지토리 등의 정보를 얻어냅니다.\n이렇게 얻어낸 컨테이너 이미지 관련 정보(Clair 공식 문서에선 Manifest라고 합니다.)는 DB에 저장되며, 해당 인덱싱에 대한 정보도 IndexReport라는 데이터 구조로 DB에 저장됩니다.\nIndexReport는 Matching 단계에서 취약점 분석에 쓰입니다.\n\n\n\nMatching\n\nClair의 Matcher가 DB에 저장된 IndexReport를 가져와 취약점 분석을 진행합니다.\n취약점 분석은 DB에 미리 저장되어있는 최신 취약점 데이터를 기반으로 진행됩니다.\nMatcher는 취약점 분석 후 VulnerabilityReport라는 취약점 보고서 객체를 생성합니다.\n취약점 보고서는 HTML 양식으로도 생성되어 웹 브라우저에서 조회 가능합니다.\n또한 Matcher는 주기적으로 최신 취약점 데이터를 DB에 업데이트하는 역할도 수행합니다.\n\n\n\nNotifications\n\nClair는 취약점 분석에 대한 알림 기능도 지원합니다.\nClair의 Notifier는 Matcher가 수행하는 DB의 최신 취약점 데이터 업데이트를 추적하고, 기존에 Indexer가 DB에 저장했던 컨테이너 이미지의 Manifest 중 취약점 데이터 업데이트에 영향을 받는 것이 있으면 알려주는 역할을 수행합니다.\nNotifier의 알림은 Webhook 등으로 받아볼 수 있습니다.\n\n\n\nClair의 활용 방안\n\nCI/CD에 Clair를 활용하면 서비스를 배포하기 전에 컨테이너 이미지를 스캔하여 보안 취약점을 사전에 최소화할 수 있습니다.\nClair의 Mathcer와 Notifier를 활용하여 최신 취약점 업데이트 및 알림을 팀내 전파하는 시스템을 구축하면, 개발자가 사전에 적절한 조치를 취할 수 있고 보안 이슈를 미리 예방할 수 있습니다.\n\nReferences\n\nwww.wiz.io/academy/container-security-scanning\nsnyk.io/learn/container-security/container-scanning\n"},"blog/Docker-Container-안에서-Docker-사용하기":{"title":"Docker Container 안에서 Docker 사용하는 방법","links":["blog/컨테이너-런타임-보안"],"tags":["Docker"],"content":"Docker는 애플리케이션을 Container라는 독립된 환경을 실행하고 관리해주는 툴인데요. 독립된 환경이라고 했지만, 호스트 안에 거의 새로운 Linux 환경을 구축한다는 느낌을 받기도 하죠.\n그래서 아래와 같은 질문이 나올 수도 있습니다.\n\nDocker Container 안에 Docker를 설치해서 Host의 Docker를 사용할 수도 있지 않을까?\n\n그렇게 나온 컨셉이 바로, ==Docker In Docker(DID)==입니다.\n우리가 Docker를 설치할 때 Docker daemon과 Docker CLI가 함께 설치됩니다. 터미널 환경에서 docker build, docker run 등의 명령어를 입력하면 Docker CLI가 명령어를 Docker daemon에게 전달해서  해당 동작을 수행하게 하는데요.\nDocker CLI가 Docker daemon에게 명령어를 전달하는 과정에서 통로 역할을 하는 파일이 하나 있으니, 바로 docker.sock입니다.\n\nDocker와 함께 설치되는 docker.sock 파일의 기본 위치는 /var/run/docker.sock인데요. Docker CLI에서도 해당 위치에 docker.sock 파일이 존재한다는 설정이 기본값으로 지정되어 있습니다.\n이렇게 DID라는 개념을 설명드리기 전에 docker.socket이라는 파일을 먼저 살펴본 것은, Docker In Docker(DID) 구현을 위해 docker.socket 파일을 활용할 예정이기 때문입니다.\n위에서 Docker CLI가 Docker daemon에 Docker 명령어를 전달할 때 docker.sock 파일을 이용한다고 했었죠. 이는 곧 외부 Docker CLI가 docker.socket에 접근할 수 있다면, 이와 연결된 Docker daemon에 명령어를 내릴 수 있다는 의미입니다.는 설정이 기본값으로 지정되어 있습니다.\n이렇게 DID라는 개념을 설명드리기 전에 docker.socket이라는 파일을 먼저 살펴본 것은, Docker In Docker(DID) 구현을 위해 docker.socket 파일을 활용할 예정이기 때문입니다.\n위에서 Docker CLI가 Docker daemon에 Docker 명령어를 전달할 때 docker.sock 파일을 이용한다고 했었죠. 이는 곧 외부 Docker CLI가 docker.socket에 접근할 수 있다면, 이와 연결된 Docker daemon에 명령어를 내릴 수 있다는 의미이기도 합니다.\nDID를 사용하는 이유\nCI/CD 파이프라인을 구축할 때 ==DID==를 활용하면 편한 경우가 있는데요. 쉬운 설명을 위해 Jenkins가 Docker Container로 동작하는 상황에서 Jenkins 파이프라인을 구성 중이라고 가정해볼게요.\n파이프라인이 동작할 때 특정 스테이지에서 Docker Image를 빌드해야 하는 경우가 있을 수 있습니다. 하지만 파이프라인을 실행하는 Jenkins가 Docker Container 상에서 동작 중인 상황이기 때문에 우리는 해당 Container 내부에서 Docker Build 작업을 수행해야 합니다.\n이때 위에서 설명한 ==DID 컨셉==을 이용한다면, Container 내부에 Docker CLI를 설치 후, 이미 호스트에 설치된 docker.sock에 접근하는 방식으로 호스트의 Docker daemon으로 Docker Build 작업을 수행할 수 있게 됩니다.\nDID를 사용하는 방법\nDID 컨셉을 활용하기 위한 가장 핵심은, Docker 명령어 실행이 필요한 컨테이너를 실행할 때 아래 옵션을 추가하는 것입니다.\ndocker run -v /var/run/docker.sock:/var/run/docker.sock ...\n\n우리가 지금까지 가정했던 상황의 경우, Jenkins가 실행되는 Docker Container가 여기에 해당됩니다.\n이 Container가 실행되는 기존 docker run 명령어에 위와 같이 호스트의 docker.sock 파일 경로를 Container 내부의 동일한 경로에 복사하는 -v 옵션을 추가하는 것인데요.\n여기에 추가로 해당 Container 내부에 Docker CLI를 설치하면, Container 내 터미널 환경에서 docker 명령어를 바로 수행할 수 있게 됩니다.\nDID 사용 시 생각해볼만한 점들\n위에서 살펴본 것처럼, ==DID==는 Container 내부의 Docker CLI가 호스트에 설치된 Docker daemon을 사용합니다. 즉, ==Container 안으로 접근만 할 수 있다면 호스트의 Docker daemon을 제한 없이 사용==할 수 있는 것이죠.\n만약 이를 외부에서 악용한다면 ==심각한 보안 이슈가 발생==할 수도 있는데요. 그렇기 때문에 DID를 사용할 때엔 Container 접근과 관련된 보안 설비가 충분히 구축되어야 안전하겠습니다.\n이와 관련해서 컨테이너 런타임 보안에 대해 지난 글에서 다룬 적이 있으니 참고해보세요 :)\nReferences\n\nmedium.com/dtevangelist/docker-in-docker-fb54252e3188\njaykos96.tistory.com/44\njpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/\n"},"blog/Dockerfile의-ADD-vs-COPY":{"title":"Dockerfile의 ADD vs COPY","links":[],"tags":["Docker"],"content":"ADD 명령어와 COPY 명령어의 공통점\n두 명령어 모두 호스트의 특정 경로(출발 경로)에 있는 파일 또는 디렉토리를 Docker 이미지 안의 특정 경로(도착 경로)로 복사할 수 있습니다.\nADD 명령어의 또다른 기능들\nCOPY 명령어와 달리, ADD 명령어에는 일반 복사 외에 추가로 지원하는 기능이 있습니다.\nURL을 활용하여 파일 다운로드\n\nADD 명령어의 출발 경로를 입력하는 부분에 호스트 경로 대신 URL을 입력할 수 있습니다.\nURL을 입력할 경우, 해당 원격지로부터 파일을 다운로드하여 Docker 이미지의 도작 경로에 추가됩니다.\n\n압축 자동 해제 및 추출\n\nADD 명령어의 출발 경로에 호스트 내에 압축(gz, bz2, xz)된 tar 아카이브 파일이 들어갈 수도 있습니다.\n압축 파일은 자동으로 해제되며, 이때 추출된 디렉토리가 Docker 이미지의 도착 경로에 저장됩니다.\n"},"blog/Dockerfile의-RUN,-CMD,-ENTRYPOINT-명령어":{"title":"Dockerfile의 RUN, CMD, ENTRYPOINT 명령어","links":[],"tags":["Docker"],"content":"Dockerfile을 통해 Docker 컨테이너를 실행하는 과정에서, 특정한 작업이나 애플리케이션의 프로세스를 실행시킬 때 RUN, CMD, ENTRYPOINT 명령어를 사용합니다. 각 명령어에 어떤 차이점이 있는지 알아보겠습니다.\nRUN 명령어\n먼저 RUN 명령어는 Dockerfile에서 이미지를 생성할 때 가장 많이 쓰이는 명령어로, 애플리케이션이나 패키지를 설치할 때 사용합니다. 기존 이미지 위에 새로운 레이어를 생성하는 명령어입니다.\nFROM ubuntu:18.04\nRUN apt-get update\nRUN apt-get install -y nginx\n위 Dockerfile 예제에서는 Ubuntu 18.04 환경 위에 apt-get update 명령어로 패키지 업데이트를 한 다음, nginx를 설치합니다. 다만 위와 같은 Docker 이미지는 패키지 업데이트 및 애플리케이션 설치만 될 뿐, 어떠한 프로세스가 실행되지 않기 때문에 컨테이너가 정상적으로 동작하지 않습니다.\n정상적으로 동작하는 Docker 이미지를 만들기 위해서는 컨테이너 내부에서 최종적으로 실행되는 프로세스를 CMD 또는 ENTRYPOINT 명령어로 정의해야 합니다.\nCMD 명령어\n컨테이너 내부에서 최종적으로 실행되는 프로세스의 명령어로, Docker CLI상에서 다른 명령어로 덮어씌울 수 있습니다.\nFROM ubuntu:18.04\nRUN apt-get update\nCMD [&quot;echo&quot;, &quot;Hello, World&quot;]\n위 Dockerfile 예제를 이용하여 Docker 이미지를 빌드하고, docker run [이미지 이름] 명령어로 실행 시 실행 결과는 아래와 같이 CMD 명령어로 정의한 프로세스가 실행됩니다.\nHello, World\n\n만약 이미지 빌드 후 docker run [이미지 이름] hostname 명령어로 실행할 경우, CMD 명령어로 정의한 프로세스가 아닌 hostname 프로세스만 실행되어 컨테이너의 호스트네임이 표시됩니다.\n4aa43esdd984\n\nENTRYPOINT 명령어\nCMD 명령어와 달리, Docker CLI상에서 다른 명령어로 덮어씌울 수 없습니다.\nFROM ubuntu:18.04\nRUN apt-get update\nENTRYPOINT [&quot;echo&quot;, &quot;Hello, World&quot;]\n위 Dockerfile 예제를 이용하여 Docker 이미지를 빌드하고, docker run [이미지 이름] 명령어로 실행 시 실행 결과는 아래와 같이 ENTRYPOINT 명령어로 정의한 프로세스가 실행됩니다.\nHello, World\n\n만약 이미지 빌드 후 docker run [이미지 이름] hostname 명령어로 실행할 경우, ENTRYPOINT 명령어로 정의한 echo Hello, World 뒤에 hostname 명령어가 따라 붙어 함께 실행됩니다.\nHello, World 4aa43esdd984\n"},"blog/Docker에서-사용하는-네트워크":{"title":"Docker에서 사용하는 네트워크","links":[],"tags":["Docker","Network"],"content":"Docker 네트워크의 종류\nDocker에서 사용하는 네트워크 중 대표적인 것은 아래 3가지입니다.\n\nBridge\nHost\nOverlay\n\nBridge 네트워크\n\n가상 인터페이스와 호스트의 인터페이스를 연결하여 도커 컨테이너가 외부와 연결 가능하게 해주는 네트워크입니다.\nDocker가 실행될 때 자동으로 Bridge 네트워크가 생성되며, Docker 컨테이너 생성 시 네트워크를 따로 지정하지 않으면 Docker 컨테이너가 미리 생성되어 있던 Bridge 네트워크와 연결됩니다.\n\nHost 네트워크\n\n호스트 네트워크 환경과 IP를 그대로 사용하며, Docker 컨테이너 내부 애플리케이션을 별도로 포트포워딩하지 않고 외부에서 접근 가능합니다.\n\nOverlay 네트워크\n\n다른 호스트 간에 네트워크를 공유하는 방식입니다. (호스트가 여러 개일 때 사용)\nOverlay 네트워크에 연결된 컨테이너들은 암호화 방식을 사용하여 서로 안전하게 통신할 수 있습니다.\n"},"blog/Docker에서-사용하는-파일-시스템":{"title":"Docker에서 사용하는 파일 시스템","links":[],"tags":["FileSystem","Docker"],"content":"UFS(Union File System)\n\nUFS는 여러 개의 파일 시스템을 하나의 파일 시스템에 마운트하는 방식입니다.\n현재 Docker에서는 UFS를 구현하기 위한 Storage Driver로 Overlay를 주류로 사용합니다.\nDocker 이미지에서 Layer는 각각의 파일 시스템을 겹쳐 놓은 형태와 유사합니다.\nLayer는 Container Layer와 Image Layer로 나뉩니다.\n\nContainer Layer:\n\n쓰기 작업이 가능한 Layer입니다.\n각 컨테이너의 최상단 Layer이며, 컨테이너 생성 후 모든 변경 작업이 이루어지는 Layer입니다.\nR/W 속도는 상대적으로 느립니다.\n\nImage Layer:\n\n읽기 작업만 가능한 Layer입니다.\n다른 컨테이너와 공유되는 Layer입니다.\n"},"blog/Elasticsearch-및-Kibana에-오픈소스-라이선스-적용-선언":{"title":"Elasticsearch 및 Kibana의 오픈소스 라이선스 적용 선언","links":[],"tags":["Monitoring"],"content":"✨Elastic의 오픈소스 라이선스 적용 선언\n\n2024년 8월 말, Elastic은 Elasticsearch 및 Kibana에 오픈소스 라이선스를 다시 적용하겠다고 발표했습니다. (원문 링크)\n현재 Elasticsearch 및 Kibana 최신 버전(v8.15.1) 이후에 릴리즈되는 버전부터 AGPL v3.0(GNU Affero General Public License v3.0) 라이선스가 적용될 수 있도록 수정할 예정이라는 게 이번 발표의 요지인데요.\nAGPL v3.0이 적용되는 소프트웨어를 수정 없이 상용 제품에 사용하는 경우, 제품의 코드까지 공개하지 않아도 된다는 점에서 사용자에게 유리합니다.\n하지만 AGPL v3.0이 적용되는 소프트웨어 자체를 수정한 경우에는 해당 소스코드를 공개해야 합니다. 게다가 아래 AGPL-3.0 파생 저작물 범위에 해당하는 제품의 소스 코드는 바이너리 형태로 재배포할 때뿐만 아니라 네트워크를 통해 사용자와 상호 작용을 하는 경우에도 공개해야 하는데요.\n\nAGPL이 적용되는 소프트웨어의 수정 코드\nAGPL이 적용되는 소프트웨어와 동일한 프로세스에서 동작하는 Module\nAGPL이 적용되는 소프트웨어와 링크로 연결한 Library\nAGPL이 적용되는 소프트웨어를 상속한 Class\n\nAGPL이 적용되는 소프트웨어와 제품이 어떻게 결합되느냐에 따라 원격 네트워크로 서비스하는 경우에도 제품 소스 코드를 공개해야 하는 리스크도 존재하기 때문에 세심한 주의가 필요합니다.\nElastic도 해당 발표에서 자신들도 이번 라이선스 적용으로 사용자들이 우려할 수 있음을 알고 있다고 밝혔는데요. ‘AGPL은 MongoDB와 Grafana 등의 프로젝트에도 적용된 라이선스이기 때문에 Elasticsearch와 Kibana 사용에 AGPL이 큰 영향을 미치진 않을 것’이라고 언급하면서, ‘AGPL 적용은 우리가 오픈소스 생태계로 다시 다가갈 수 있는 좋은 첫걸음이 될 거라 믿는다’고 했습니다.\n🧐기존 Elasticsearch와 Kibana의 라이선스는 어땠길래?\n그렇다면 Elasticsearch와 Kibana의 라이선스가 어떻게 변해왔길래 Elastic이 오픈소스로 다시 돌아왔다고 발표한 것일까요?\nElasticsearch를 포함한 ELK(Elasticsearch, Logstash, Kibana) 스택 툴들은 처음 릴리즈 당시 Apache-2.0 라이선스가 적용되고 있었습니다. Apache-2.0이 적용되는 소프트웨어는 수정하거나 상용 제품에서 활용되어도 제품의 소스코드를 공개할 의무가 없기 때문에 대표적인 오픈소스 라이선스라고 할 수 있는데요.\nELK 스택 툴들은 모두 7.10 버전대까지 Apache-2.0 라이선스가 적용되었고, 이 버전대의 ELK 툴들을 OSS(Open Source Software) 버전이라고도 합니다.\n이후 Elasticsearch와 Kibana는 2021년에 Elastic License와 SSPL(Server Side Public License) 중 하나를 선택해서 적용될 수 있도록 변경되었습니다.\nElastic License에는 ‘제품을 다른 사람에게 관리형 서비스(Managed Service)로 제공할 수 없다’는 내용이 포함되어 있는데요. 이는 외부 사용자가 Elasticsearch API를 직접 사용할 수 있거나 Kibana의 대시보드에 직접 접속할 수 있도록 제공하는 것 등을 제한하는 내용입니다. 만약 이런 제한을 풀고 싶다면, Elastic 담당자와 컨설팅 및 유료 버전을 구독해야 합니다.\nSSPL에는 ‘소프트웨어를 실행하는 데에 필요한 서비스 소스코드를 공개해야 한다’는 내용이 명시되어 있습니다. 라이선스가 적용되는 소프트웨어를 관리하기 위한 모니터링이나 백업용 프로그램의 이 ‘서비스 소스코드’에 포함되기 때문에, 원치 않는 소스코드를 공개해야 하는 리스크가 존재합니다.\n이렇게 2021년부터 Elasticsearch와 Kibana에 적용되었던 라이선스는 모두 오픈소스 라이선스라고 할 수 없었습니다. 그렇기 때문에 이번 AGPL v3.0 적용을 Elastic에선 오픈소스로 다시 돌아왔다고 표현하는 것이죠.\n\n하지만 AGPL v3.0 역시 위에서 살펴봤던 것처럼 일부 소스코드 공개 조건이 존재하기 때문에, 오픈소스와 Elastic 커뮤니티에선 관련해서 여러 논의가 이어질 것으로 보입니다.\nElastic에서 발표한 새로운 라이선스 정책은 Elasticsearch 및 Kibana를 사용하려던 분들이 참고할 만한 내용이라 생각해서 다뤄봤는데요.\n여담으로 제가 ELK 스택 모니터링 시스템과 Kubernetes를 주제로 한 온라인 강의를 제작 중이어서, 저에게도 이번 내용이 흥미로웠습니다.😄\n제작 중인 강의는 10월 중 릴리즈를 목표로 하고 있는데, 관련 소식은 추후 다시 공유드려보겠습니다.🌱\nReferences\n\nwww.elastic.co/kr/blog/elasticsearch-is-open-source-again\nsktelecom.github.io/guide/use/obligation/agpl-3.0\nbonohubby.com/entry/Elasticsearch-License\n"},"blog/Falco-소개":{"title":"컨테이너 런타임 보안 툴 Falco 소개","links":["blog/컨테이너-런타임-보안"],"tags":["Security"],"content":"\nFalco는 Linux 시스템을 대상으로 개발된 클라우드 네이티브 보안(Cloud-native Security) 툴입니다. Linux 커널 이벤트에 대한 규칙(Rule)을 지정하면 이에 따른 실시간 알림을 보내주는 역할을 수행합니다.\nFalco가 지원하는 이러한 행위를 컨테이너 런타임 보안 활동이라고 하는데요. 이와 관련된 자세한 설명은 여기서 확인하실 수 있습니다.\nFalco를 사용하는 이유\n현재 컨테이너 런타임 보안 활동을 지원하기 위해 다양한 툴이 개발 및 유지보수되고 있습니다. 그 중 Falco는 아래와 같은 장점을 지니고 있습니다.\n\nKubernetes, Docker Swarm과 같은 다양한 Container Orchestration Platform에서 사용 가능\nKubernetes의 Daemonset으로 배포 및 실행을 지원하므로 모든 노드에 대한 모니터링이 용이함\nKubernetes API Call을 활용할 수 있어 클러스터 내 Node 및 Pod의 상태를 모니터링 가능\nFalco 가 사용할 수 있는 Default Ruleset이 미리 정의되어 있어 컨테이너 런타임 보안 활동을 쉽고 빠르게 시작 가능\n특정 조건에 대한 이상 행위를 감지하는 Custom Ruleset을 쉽게 정의 가능\n\nFalco의 동작 방식\nFalco의 동작 프로세스는 크게 아래 3단계로 나눌 수 있습니다.\n\nLinux Kernel의 System Call 또는 Kunernetes API Call 포착\n사전 정의된 Ruleset (또는 Default Ruleset)에 따라 표시할 Output 처리\n사전 정의된 채널로 Output 전송\n\nFalco는 동작 중인 컨테이너에서 생성되는 여러 요청을 감시하기 위해 Ruleset을 사용하는데요. Falco의 Ruleset은 아래와 같이 정의됩니다.\n- rule: Detect bash in a container # Rule 이름\n  desc: You shouldn&#039;t have a shell run in a container # Rule 설명\n  condition: container.id != host and proc.name = bash # 프로세스의 Call 중 Falco가 포착하는 조건\n  output: Bash ran inside a container (user=%user.name command=%proc.cmdline %container.info) # 해당 Rule에 대해 Falco가 표시하는 Output 양식\n  priority: INFO # Falco가 표시하는 Output의 Type\nFalco는 다양한 채널을 통해 Output을 전송합니다. 그 중 대표적인 채널은 아래와 같습니다.\n\nStandard Output\nFile\nSyslog\nHTTP[s]\ngRPC\n\nFalco 설치 방식\n\n\nKubernetes 클러스터에 Helm Chart로 설치\n\nKubernetes 클러스터에 Falco를 가장 쉽고 빠르게 설치하는 방법은 Helm을 이용하는 것입니다.\nHelm으로 Falco를 설치하면, 배포된 Falco Pod의 로그를 확인함으로써 Falco Ouput 모니터링이 가능합니다.\nCustom Ruleset을 yaml 파일로 정의한 다음, Helm으로 배포되어있는 Falco에 적용하는 것도 가능합니다.\n\n\n\nLinux 호스트에 패키지로 설치\n\nFalco 공식 패키지 저장소에서 각 시스템 아키텍처에 맞는 버전을 직접 설치 가능합니다.\n이렇게 설치한 Falco의 Configuration 및 Ruleset 파일 역시 호스트에 저장 및 사용되므로 수정이 필요하다면 호스트 내 파일을 직접 수정해야 합니다.\n\n\n\nLinux 호스트에 컨테이너 이미지로 배포\n\nFalco의 공식 컨테이너 이미지 저장소에서 Falco 실행이 가능한 컨테이너 이미지를 받아와 호스트에 배포할 수도 있습니다.\n\n\n\nReferences\n\nfalco.org/\ngithub.com/falcosecurity/charts/tree/master/charts/falco\nsysdig.com/blog/intro-runtime-security-falco/\n"},"blog/Fluent-Bit에서-발견된-보안-취약점-(CVE-2024-4323)":{"title":"Fluent Bit에서 발견된 보안 취약점 (CVE-2024-4323)","links":[],"tags":["CVE","Monitoring"],"content":"\n다양한 클라우드 제공자와 IT 기업에서 널리 사용 중인 로그 데이터 수집 툴 Fluent Bit에서 보안 취약점(CVE-2024-4323)이 발견되었습니다.\n해당 취약점은 Fluent Bit의 내장 HTTP 서버에 존재하던 버퍼 오버플로(Buffer Overflow)가 원인이라고 합니다.\n해당 취약점이 위험한 이유\n이번 취약점에 대한 테스트 결과, Buffer Overflow 동작으로 인해 인접한 메모리에 저장되어있던 정보가 HTTP 응답으로 반환되는 경우가 있었다고 합니다. 이는 민감한 정보가 유출될 위험이 있음을 보여주는데요.\n그 외에도 서비스 거부(Denial of Service), 원격 악성 코드 실행 공격 등에 해당 취약점이 이용될 수도 있다고 합니다.\n다만 해당 취약점을 이용해서 원격 악성 코드 실행을 수행하는 것은 비교적 어렵기 때문에, 해당 공격 위험이 급박한 것은 아니라고 합니다.\n대처 방법\n해당 보안 취약점에 대처할 수 있는 가장 확실한 방법은, Fluent Bit를 v3.0.4 이상으로 업그레이드하는 것입니다.\n만약 버전을 바로 업그레이드하기 어렵다면, 인증된 사용자와 서비스만 Fluent Bit의 모니터링 관련 API에 접근할 수 있도록 설정하는 것이 권장된다고 합니다.\nReferences\n\nCritical Fluent Bit flaw affects major cloud platforms, tech companies’ offerings (CVE-2024-4323)\nwww.tenable.com/plugins/nessus/197568\n"},"blog/Infrastructure-as-Code-(IaC)-알아보기":{"title":"Infrastructure as Code (IaC) 알아보기","links":[],"tags":["Infrastructure"],"content":"⚙️Infrastructure as Code란\n\n우리가 애플리케이션을 배포하고 서비스를 운영하려면 서버, 스토리지, 네트워크 등과 같은 인프라 설정이 필요한데요.\n인프라 구축을 애플리케이션 배포 때마다 매번 직접 수동으로 한다면 시간도 오래 걸리고 지루한 작업이 될 것입니다.\n만약 이런 인프라 준비 작업을 코드 기반으로 관리하고 자동화할 수 있다면 어떨까요?\n이렇게 등장한 개념이 바로, **Infrastructure as Code(IaC)**입니다.\n✅IaC를 적용해야 할 이유\n\n앞서 살펴본 것처럼, IaC는 인프라 환경을 수동으로 준비하고 설정하는 대신에 코드를 기반으로 필요한 인프라 리소스를 프로비저닝하고 관리할 수 있는 개념입니다.\n이런 IaC는 DevOps와 CI/CD를 구현하는 데에도 중요한데요.\nDevOps 접근법을 기반으로 개발팀과 운영팀의 배포 환경을 직접 수동으로 맞춘다면 갖가지 오류나 불일치 문제가 발생할 수 있고, 극단적인 경우에는 애플리케이션을 수동으로 배포해야만 하는 상황이 생길 수도 있습니다.\n하지만 IaC를 도입하면 개발팀과 운영팀이 사용할 애플리케이션 배포 환경을 수월하게 맞출 수 있어서 애플리케이션 배포 자동화가 가능하고, 이는 DevOps 접근법에도 부합합니다.\nIaC를 적용하면 인프라 환경 역시 CI/CD 파이프라인을 통해 테스트와 버전 컨트롤이 가능하다는 장점이 있기 때문에 DevOps 관점에서도 IaC 도입이 권장됩니다.\nIaC의 개념에 대해 살펴봤으니 IaC를 수행할 수 있는 툴을 살펴볼 차례인데요.\n각 IaC 툴에 대해 알아보기 전에 IaC 툴을 사용하면서 얻을 수 있는 이점을 정리하면 아래와 같습니다.\n\n인프라 구축 프로세스를 자동화하여 인프라 프로비저닝 및 관리에 필요한 시간 단축\n인프라 리소스 구축에 대한 정의를 소스코드로 관리할 수 있기 때문에 휴먼 에러(Human error)를 줄이고, 일관성 있는 환경 구축 가능\n코드 기반으로 인프라를 자동으로 구축할 수 있기 때문에 인프라 구성을 보다 쉽게 확장할 수 있고, 상황에 맞게 인프라 환경을 더욱 신속히 변경 가능\n인프라 변경 이력이 관련 코드 수정으로 기록되기 때문에 이력 추적과 문서화에 용이\n\n이런 이점을 가진 대표적인 IaC 툴 2가지 Terraform과 Crossplane에 대해 간단히 알아보려 하는데요. 첫 번째 순서는 Terraform입니다.\n🛠️IaC 툴 알아보기\nTerraform\n\nHashiCorp에서 제공하는 Terraform은 퍼블릭 클라우드와 온프레미스 인프라 리소스를 읽기 쉬운 코드로 정의할 수 있도록 도와줍니다.\nHashiCorp와 Terraform 커뮤니티에서 개발한 수많은 Provider 덕분에 다양한 클라우드 플랫폼과 서비스의 리소스를 API를 이용해서 자체 개발한 문법의 코드로 인프라 구축이 가능한데요.\nTerraform의 워크플로우는 아래와 같이 3단계로 나눌 수 있습니다.\n\nWrite(작성): 필요한 인프라 리소스를 개발자가 코드 기반으로 정의\nPlan(계획): 정의한 코드를 기반으로 Terraform이 리소스 생성, 업데이트, 제거 준비\nApply(적용): 계획에 대해 승인이 될 경우, Terraform이 알맞은 순서에 맞게 정의된 리소스에 대한 작업 수행\n\nCrossplane\n\n이제 Crossplane에 대해 간단히 알아보겠습니다. Crossplane은 Kubernetes 커스텀 리소스를 이용해서 인프라 관련 리소스를 선언적으로 관리할 수 있는 오픈소스 Kubernetes 애드온인데요.\nKubernetes 클러스터에 설치된 Crossplane을 통해, 사용자는 Kubernetes와 상호작용하는 것만으로 AWS, Azure, GCP 같은 외부 리소스를 관리할 수 있게 됩니다.\nTerraform과 Crossplane의 차이\n두 IaC 툴 모두 인프라 관련 리소스를 쉽고 효율적으로 구축하고 관리할 수 있도록 도와준다는 것을 알 수 있는데요. 그렇다면 Terraform과 Crossplane의 차이점은 무엇일까요?\n네이티브 k8s와의 호환성\nCrossplane은 Kubernetes 기반으로 설계된 반면, Terraform은 독자적으로 사용 가능하고 Kubernetes와의 연동은 별도로 제공된 Provider를 이용합니다.\n리소스 선언 문법\n두 가지 툴 모두 선언적 문법을 사용하는데요. 다만 Crossplane은 Kubernetes에서도 사용하는 YAML 문법을 사용하고, Terraform은 독자적인 HCL 문법을 사용합니다.\n프로비저닝\nCrossplane은 Kubernetes Controller를 통해 리소스를 프로비저닝하지만, Terraform은 별도로 개발된 API/SDK를 통해 직접 프로비저닝합니다.\n생태계\nTerraform은 Provider에 대한 생태계가 성숙한 상태이지만, Crossplane의 생태계는 계속 성장 중에 있습니다.\nIaC 툴과 Helm chart의 차이\nKubernetes를 관리하고 운영하다보면 프로젝트 단위로 리소스를 쉽게 관리하기 위해 Helm chart를 사용하는 경우가 많습니다.\n그래서 위에서 소개한 IaC 툴들과 Helm chart가 동일한 성격의 툴로 보이기도 하는데요.\n하지만 IaC 툴과 Helm chart에도 명확한 차이점은 존재합니다.\nIaC 툴은 Kubernetes뿐만아니라 AWS, GCP, Azure와 같은 퍼블릭 클라우드의 인프라 리소스도 구성이 가능합니다. 하지만 Helm chart는 Kubernetes 클러스터 내의 리소스만 배포하고 관리할 수 있도록 도와주죠.\n또한 IaC 툴은 새로운 Kubernetes 클러스터 생성이 가능하지만, Helm chart는 기존 Kubernetes 클러스터 내의 리소스만 생성할 수 있습니다.\nReferences\n\nwww.redhat.com/en/topics/automation/what-is-infrastructure-as-code-iac\nspacelift.io/blog/infrastructure-as-code-tools\ndeveloper.hashicorp.com/terraform/intro\ndocs.crossplane.io/latest/getting-started/introduction/\nmedium.com/@bijit211987/crossplane-vs-terraform-which-infrastructure-as-code-tool-is-best-a5f41f65e8a1\nThe Difference between Terraform and Helm Charts - Coralogix\n"},"blog/Istio-소개":{"title":"Service Mesh 툴 Istio 소개","links":["blog/Service-Mesh-소개"],"tags":["Network"],"content":"\nIstio는 클라우드 환경에서 Service Mesh를 구현할 수 있는 오픈소스 솔루션입니다. Service Mesh가 무엇이고 왜 사용되는지 궁금하시다면, 여기를 확인해보세요.\nIstio를 사용하는 이유\nIstio는 Service Mesh의 주요 기능들을 모두 지원하고 있는데요. Istio를 사용함으로써 얻을 수 있는 이점을 각 측면 별로 정리하면 아래와 같습니다.\n\n가시성\n\nService Mesh에서 발생하는 모든 Service 통신 관련 로그와 메트릭을 쉽게 모을 수 있음\n\n\n트래픽 관리\n\nService 사이의 트래픽과 API 요청을 별도의 레이어에서 제어 가능\nCircuit Breaker, Timeout, Retry와 같은 Service에 대한 속성을 별도 영역에서 쉽게 설정 가능\n\n\n보안성\n\n트래픽 허용 관련 Policy나 TLS 암호화, 인증 등의 기능 제공\n\n\n도입 용이성\n\nIstio 생태계의 규모가 큰 덕분에 다양한 기술 문서나 적용 사례를 참고 가능\n\n\n\nIstio의 동작 방식\nIstio는 Service Mesh와 마찬가지로 데이터 플레인(Data Plane)과 컨트롤 플레인(Control Plane)으로 구성됩니다.\n\n데이터 플레인\n\nKubernetes의 Service 간의 통신을 담당\nEnvoy라는 오픈소스 프록시를 각 Pod 내에 Sidecar로 배포\nEnvoy 프록시가 Service에 대해 발생하는 네트워크 트래픽을 가져와서 Istio의 각 기능을 수행\nEnvoy 프록시는 컨트롤 플레인에서 Configuration한 설정이 일괄 적용됨\n\n\n컨트롤 플레인\n\nService에 추가되는 프록시에 대한 Configuration을 담당하는 영역\n\n\n\nReferences\n\nistio.io/latest/about/service-mesh\n"},"blog/Jenkins와-Pipeline-(Declarative-vs-Scripted)":{"title":"Jenkins와 Pipeline (Declarative vs Scripted)","links":[],"tags":["Jenkins"],"content":"Jenkins와 Pipeline\nJenkins는 워크 플로우를 자동화할 수 있는 오픈 소스 툴입니다. 소프트웨어의 지속적 통합(Continuous Integration, CI)과 지속적 배포(Continuous Delivery, CD)를 간단하게 구현할 수 있도록 도와주죠. 특히 Jenkins의 Pipeline을 사용하면 배포 과정을 더욱 쉽고 빠르게 구축할 수 있기 때문에, CI/CD 프로세스를 구현할 때 Jenkins를 사용하는 경우가 많습니다.\n지속적 배포(CD) 파이프라인을 예로 들어 Jenkins의 Pipeline에 대해 더 살펴보겠습니다. CD 파이프라인은 소프트웨어를 버전 관리 시스템(예: GitHub)에서부터 사용자 및 클라이언트에게로까지 바로 전달하는 일련의 프로세스라고 할 수 있습니다. 소프트웨어의 새로운 버전을 릴리즈하는 데에 필요한 모든 워크 프로우, 활동, 자동화 과정이 여기에 포함되며, 이러한 과정을 Jenkins의 Pipeline 파일의 코드로 구현할 수 있는 것입니다.\nJenkins Pipeline 코드를 작성하는 방식에 따라 Declarative Pipeline과 Scripted Pipeline으로 나뉩니다.\nDeclarative Pipeline\nDeclarative 방식은 Scripted 방식보다 최근에 Jenkins에 추가되었으며, Pipeline 작성에 특화된 문법을 따르기 때문에 코드 작성과 관리가 더욱 용이해졌습니다.\n코드 예제\npipeline {\n  agent any\n  stages {\n    stage(&#039;Hello World) {\n      steps {\n        sh &#039;echo Hello World&#039;\n      }\n    }\n  }\n}\n장점\n\n보다 구조화된 Pipeline 코드 작성이 가능하여 코드 관리 용이성과 가독성을 높일 수 있습니다.\nBlue Ocean 인터페이스와 연동하기 쉽습니다.\n\n단점\n\nScripted 방식보다 코드 작성의 자유도가 떨어지고, 복잡한 로직의 Pipeline 코드를 작성하기 어려울 수도 있습니다.\nDeclarative 방식을 지원하지 않는 플러그인도 존재합니다.\n\nScripted Pipeline\nScripted 방식은 Jenkins가 기존에 지원하던 Pipeline 작성 방식입니다. Groovy 언어가 제공하는 대부분의 기능을 사용할 수 있어 Pipeline 코드를 유연하게 작성 가능합니다.\n코드 예제\nnode {\n  stage(&#039;Hello World) {\n    sh &#039;echo Hello World&#039;\n  }\n}\n장점\n\nPipeline 코드 작성이 비교적 자유롭습니다.\n복잡한 로직의 Pipeline 코드 작성도 가능합니다.\n\n단점\n\n구조화된 양식이 없기 때문에 불안정한 Pipeline 코드를 작성할 위험이 있으며, 코드를 이해하거나 관리하기 어려워질 수도 있습니다.\n"},"blog/KCNA와-KCSA-자격증-취득-후기":{"title":"KCNA와 KCSA 자격증 취득 후기 및 핵심 키워드","links":["🗃️-Infinity-Drawer/CKA-취득-후기","🗃️-Infinity-Drawer/CKS-취득-후기"],"tags":["Kubernetes"],"content":"📜Kubernetes와 클라우드 네이티브 지식을 검증하는 KCNA / KCSA 자격증 시험\n\n지난 글에서 Kubernetes의 대표적인 자격증인 CKA와 CKS의 취득 후기를 공유드렸습니다.\n저는 그 이후에 CKAD와 KCNA, KCSA 자격증을 취득해서 CNCF 재단의 Kubernetes 자격증을 모두 모았고, CNCF의 Kubestronaut 자격을 얻게 되었습니다. Kubestronaut에 대해 궁금하시다면, 제가 정리한 뉴스레터 글을 참고해주세요.\nCKA와 CKAD, CKS는 모두 실습형 시험입니다. 저는 실무에서 Kubernetes를 다루고 있기 때문에, 주어진 요구사항에 맞춰 Kubernetes 환경에서 각 리소스를 다루고 환경을 설정하는 실습형 시험이 낯설지는 않았습니다. 그래서 시험을 준비할 때에도 어떻게 공부해야 할지 명확했던 기억이 있습니다.\n하지만 KCNA와 KCSA는 객관식 시험입니다. 저에겐 오히려 이런 방식이 시험을 준비할 때 더 까다로웠습니다. 출제 범위는 넓은데, 그 내용을 영어로 풀어쓴 문항과 보기를 잘 이해하고 그 중 정답을 골라야 했기 때문이죠.\n그래서 이번 글에선 KCNA와 KCSA의 출제 범위와, 각 주제의 핵심 키워드를 소개해드리고자 합니다. 소개해드린 키워드와 연결 링크의 영문 문서를 참고하시면 시험 준비에 도움이 될 것입니다.\n최근 Kubernetes 자격증에 대한 관심이 많아지고 Kubestronaut를 목표하시는 분들도 많아지고 있는데, KCNA와 KCSA 시험 준비가 막막하신 분들에게 도움이 되었으면 하는 마음입니다.\n🔰KCNA의 출제 범위 및 핵심 키워드\n\nKCNA(Kubernetes and Cloud Native Associate) 시험은 Kubernetes와 클라우드 네이티브 관련 지식을 가지고 있는지를 확인합니다. 출제 범위는 위 그림과 같고, 각 출제 비중 역시 퍼센티지로 확인하실 수 있습니다.\nKubernetes Fundamentals\nKubernetes의 기본 구성요소에 대한 지식을 물어보는 문제 비중이 가장 큰데요. 아래 키워드에 대해서는 링크로 첨부한 영문 공식 문서를 참고해보시는 것을 추천드립니다. 각 구성요소가 어떤 역할을 하고 어떻게 동작하는지 이해하는 것이 중요합니다.\n\nkube-apiserver\nkube-scheduler\nkube-controller-manager\nkubelet\nkube-proxy\n\n그리고 Kubernetes의 주요 Workload 리소스에 대해서도 공식 문서 내용을 참고하는 걸 추천드립니다.\n\nDeployment\nStatefulset\nDaemonSet\nJob\nCronJob\n\nContainer Orchestration\n컨테이너 오케스트레이션을 구현하는 데에 필요한 인터페이스도 자주 출제되는 주제입니다. 관련 키워드는 아래와 같습니다.\n\nContainer Networking Interface (CNI)\nContainer Storage Interface (CSI)\nContainer Runtime Interface (CRI)\n\nCloud Native Architecture\n클라우드 네이티브의 대표적인 기술에 대해 물어보는 경우도 많은데요. 관련 주요 키워드는 아래와 같습니다.\n\n오토 스케일링(Autoscaling)\n서버리스(Serverless)\n\nCloud Native Observability\n클라우드 네이티브의 옵저빌리티 및 모니터링에 대해서는 아래 클라우드 네이티브 애플리케이션이 주요 키워드입니다.\n\nPrometheus\nGrafana\n\nCloud Native Application Delivery\n애플리케이션 배포 관련 문제도 출제될 수 있는데요. 아래 키워드에 대해 살펴보는 것을 추천드립니다.\n\nGitOps\n\nArgoCD\n\n\nCI/CD\n\nBlue-green deployment\nCanary deployment\n\n\n\n🛡️KCSA의 출제 범위 및 핵심 키워드\n\nKCSA(Kubernetes and Cloud Native Security Associate) 시험은 Kubernetes와 클라우드 네이티브의 보안 관련 지식을 가지고 있는지를 확인합니다. 이 역시 위 그림에서 출제 범위와 출제 비중을 확인하실 수 있습니다.\nOverview of Cloud Native Security\n클라우드 네이티브 관점에서 보안이 무엇이고, 관련 기술에는 어떤 것이 있는지 알고 있는 것이 중요합니다. 주요 키워드는 아래와 같습니다.\n\nCloud Native Security Layer - 4C\nCIS Benchmark\n\nkube-bench\n\n\nMITRE ATT&amp;CK\nNetwork Policies\nRBAC (Role-Based Access Control)\n\nKubernetes Cluster Component Security\nKubernetes 클러스터 구성요소의 보안은 문제로 자주 나오는 주제입니다. 주요 키워드는 아래와 같습니다.\n\nkube-apiserver 접근 제어\nkubelet 접근 제어\netcd 접근 제어\n\nKubernetes Security Fundamentals\nKubernetes에서 보안을 위해 사용되는 기능 역시 출제 비중이 높은 주제입니다. 주요 키워드는 아래와 같습니다.\n\nPod Security Standards\nPod Security Admission\nService Account\nSecret Management\nControl Plane Isolation\nData Plane Isolation\nAuditing\n\nKubernetes Threat Model\nKubernetes의 보안 취약점이 될 수 있는 요소에 대해 물어보는 경우도 있습니다. 관련된 주요 키워드는 아래와 같습니다.\n\nTrust Boundary\nDenial of Service(DoS)\nPrivilege Escalation\n\nPlatform Security\n플랫폼 보안에 대한 주요 키워드는 다음과 같습니다.\n\nSupply Chain Security\nImage Repository\nObservability\nService Mesh\nAdmission Control\n\nCompliance and Security Frameworks\n보안 준수 관련 표준에 대한 문제도 출제 가능성이 있습니다. 주요 키워드는 아래와 같습니다.\n\nNVD(National Vulnerability Database)\nSTRIDE\nCNCF Supply Chain Security\nKubescape\n\nReferences\n\nfaun.pub/how-to-ace-kcna-kubernetes-and-cloud-native-associate-exam-ac7bceace1eb\nmedium.com/@wattsdave/kubernetes-cloud-native-security-associate-kcsa-study-notes-and-exam-prep-f4c8f84d1c4f\n"},"blog/Kubernetes-v1.31-릴리즈!---주요-업데이트-소개":{"title":"Kubernetes 1.31 릴리즈 - 주요 업데이트 소개","links":[],"tags":["Kubernetes"],"content":"지난 8월에 Kubernetes v1.31이 릴리즈되었습니다. 프로젝트 네임은 Elli인데요.\nKubernetes 10주년 이후 처음 릴리즈되는 이번 버전에서도 다양한 기능이 안정화되고 새로운 기능도 추가되었습니다.\n\n그중 아래 주요 업데이트에 대해 소개해드리겠습니다.\n\nAppArmor 지원 기능 Stable 단계로 상향\nPersistentVolume의 lastTransitionTime Stable 단계로 상향\n이미지 볼륨 지원 기능 추가 (Alpha 단계)\n\n🛡️AppArmor 지원 기능 Stable 단계로 상향\nAppArmor는 리눅스 서버를 보호하기 위해 특정 애플리케이션이 네트워크나 파일 등에 접근하는 것을 제한하는 보안 모듈입니다. Ubuntu나 SUSE 등 많은 리눅스 배포판에서 AppArmor가 기본으로 적용되어 있는데요.\n특정 애플리케이션에 누군가 악의적으로 침투해서 동작 중인 리눅스 호스트로 접근하는 것을 막기 위해 AppArmor가 사용되는 거죠.\n\nAppArmor의 보호 대상으로서 접근이 제한되는 리소스를 정의한 것을 Profile이라고 하는데요. 리눅스 호스트에는 여러 개의 Profile을 정의해서 관리할 수도 있습니다.\n이전부터 AppArmor의 Profile을 Kubernetes 상에서 배포된 애플리케이션에도 적용할 수 있도록 securityContext의 appArmorProfile 필드를 지원하고 있었는데요.\n그동안 Beta 단계였던 AppArmor 지원 기능이 이번 Kubernetes 1.31 버전 릴리즈부터 Stable 단계로 상향되었고, 이제 Kubernetes에서 AppArmor를 안정적으로 지원한다고 볼 수 있습니다.\n만약 Kubernetes 클러스터에 배포하는 애플리케이션의 호스트 리소스 접근을 제한하고 싶다면 AppArmor를 고려해보는 건 어떠신가요?\n\n\n                  \n                  Info\n                  \n                \n\n참고로 Kubernetes v1.30 이전까진 Pod의 securityContext 필드가 아닌 annotations 필드에서 AppArmor Profile을 사용했습니다.\nKubernetes v1.30 이후부터 securityContext 필드에서 AppArmor Profile을 정의하도록 지원하기 시작했답니다.\n\n\n⏱️PersistentVolume의 lastTransitionTime 기능 Stable 단계로 상향\nPersistent Volume(PV)은 Pod에서 로컬이나 외부 스토리지를 부분적으로 사용할 수 있도록 만들어진 Kubernetes 리소스입니다. PV는 아래 3가지 중 하나의 상태를 가집니다.\n\nPending: PV와 Pod가 연동되기 위해 준비하는 상태\nBound: PV와 Pod가 연동된 상태\nReleased: Pod가 종료되거나 기타 이유로 PV를 사용하지 않아 Pod와 PV간의 연동이 끊어진 상태\n\n\nKubernetes v1.28부터 이런 PV의 상태가 변경되는 시점을 기록하는 lastTransitionTime 필드가 도입되었는데요. lastTransitionTime 기능을 사용하면 PV가 상태를 변경할 때마다 업데이트되어 PV의 최근 상태 변경 시점을 알 수 있게 됩니다.\n그렇기 때문에 아래와 같이 활용 가능합니다.\n\nRetention(데이터 보존 및 삭제) 정책 구현\n\n예: 특정 기간 동안 계속 Released 상태인 PV 삭제\n\n\n스토리지의 Health 모니터링\n\n예: 비정상적으로 긴 시간 동안 Pending 상태인 PV 감지 및 조치\n\n\n\n만약 PV 관리와 관련된 정책이나 모니터링이 필요하다면 PV의 lastTransitionTime 필드가 도움이 될 것입니다.\n💿이미지 볼륨 지원 기능 추가 (Alpha 단계)\nv1.31 버전부터 Alpha 단계로 새로 추가된 기능도 다양한데요. 그 중 이번에 소개드릴 기능은 이미지 볼륨 지원 기능입니다.\n이미지 볼륨 지원 기능이란, Pod가 PV로 일반적인 스토리지를 사용하는 것처럼 Container Image를 볼륨으로 사용할 수 있도록 지원함을 의미합니다.\n\n이미지 볼륨을 사용하는 Pod의 Yaml의 예시는 아래와 같습니다.\nkind: Pod\nspec:\n  containers:\n    - …\n      volumeMounts:\n        - name: my-volume\n          mountPath: /path/to/directory\n  volumes:\n    - name: my-volume\n      image:\n        reference: my-image:tag\n이 기능은 특히 인공지능 개발 분야에서 활용될 것으로 기대된다고 하는데요. 인공지능 모델 서버 컨테이너에 모델 Weight가 담긴 이미지를 볼륨 마운트한다면, 인공지능 모델을 서비스하는 이미지와 모델이 저장된 이미지를 분리해서 운용할 수 있기 때문에 효율적인 배포가 가능할 것이라고 합니다.\n그 외에도 여러 컨테이너에 공용으로 필요한 데이터가 담긴 이미지를 볼륨으로 마운트해서 보안 사고 가능성과 전체적인 이미지 크기를 함께 줄이는 등, 다양하게 활용할 수 있을 것으로 보입니다.\nReferences\n\nkubernetes.io/blog/2024/08/13/kubernetes-v1-31-release/\nkubernetes.io/docs/concepts/security/linux-kernel-security-constraints/#apparmor\nkubernetes.io/blog/2023/10/23/persistent-volume-last-phase-transition-time/\nkubernetes.io/blog/2024/08/16/kubernetes-1-31-image-volume-source/\n"},"blog/Kubernetes-v1.32---Memory-Manager-GA-진입과-주요-업데이트":{"title":"Kubernetes v1.32: Memory Manager GA 단계 진입","links":[],"tags":["Kubernetes"],"content":"Kubernetes는 매 릴리즈마다 새로운 기능과 개선 사항을 통해 발전하고 있습니다.\n최근 v1.32 버전은 다양한 업데이트와 함께 Memory Manager 기능이 GA(General Availability) 단계로 진입된 것이 가장 주목받는 변화 중 하나인데요.\n이번 뉴스레터에서는 Kubernetes v1.32의 주요 내용과 Memory Manager의 의미, 그리고 앞으로의 영향을 살펴보겠습니다.\n📊 Kubernetes v1.32 버전 업데이트의 주요 내용\n\n(출처: CNCF 재단)\nKubernetes v1.32는 컴포넌트 성능 최적화와 다양한 옵션 도입이 포함된 릴리즈인데요. 주요 업데이트를 요약하면 아래와 같습니다.\n\nMemory Manager GA 단계 진입\n\n워크로드의 메모리 사용을 더 효율적으로 관리하여 성능 개선 및 안정성 제고\n\n\nCPU Manager 기능 강화\n\n워크로드가 사용할 CPU를 더욱 엄격히 할당하여 성능 향상을 이끌어내는 strict-cpu-reservation 옵션 도입 시작\n\n\nCSI(컨테이너 스토리지 인터페이스) 관련 기능 강화\n\n여러 볼륨으로 이루어진 그룹의 복사본 저장이 가능한 Volume Group Snapshot 기능 Beta 단계 진입\n\n\nKubernetes API Server 성능 향상\n\n기존 메모리 사용이 컸던 List 요청을 Watch List 요청으로 대체하는 WatchListClient 옵션 도입 시작\n\n\n\n🔍 이번 업데이트로 GA 상태에 진입한 Memory Manager 기능이란?\n\n(출처: Kubernetes 공식 블로그)\nMemory Manager는 컨테이너 애플리케이션에 메모리 자원을 더 효율적으로 할당할 수 있는 기능입니다.\nMemory Manager의 역할을 이해하려면, NUMA(Non-Uniform Memory Access)라는 디자인 개념을 알아야 하는데요. 필요한 부분만 빠르게 훑어보겠습니다.\n\n(출처: live.boost.org/doc/libs/1_66_0/libs/fiber/doc/html/fiber/numa.html)\nNUMA는 멀티프로세서 구조에 쓰이기 위해 등장했습니다. 멀티프로세서 구조에선 메모리의 위치에 따라 프로세서가 메모리에 접근하는 속도가 달라지는데요.\n프로세서가 자신의 지역(Local)에 있는 메모리에 더 빠르게 접근할 수 있게 하여 성능을 향상 시킨 것이 바로 NUMA 설계입니다.\n하지만 반대로 프로세서가 다른 지역에 있는 메모리에 접근할 땐 시간이 더 걸리게 되죠.\n이런 특징을 고려해 Pod를 배포할 때 같은 지역에 있는 CPU와 메모리를 사용하도록 할당해서 성능 향상을 꾀하는 것이 바로, CPU Manager와 Memory Manager의 역할입니다.\n그래서 Memory Manager을 사용하면…\n\n실시간 처리가 요구되는 애플리케이션의 메모리 대역폭을 보장하여 실시간 애플리케이션 지원이 가능하고,\n메모리 조각화를 줄이고, 메모리 접근 속도를 최적화하여 자원 효율성이 향상되며,\n워크로드가 예측 가능한 방식으로 메모리를 사용할 수 있도록 보장하여 컨테이너화된 워크로드의 안정성을 강화할 수 있습니다.\n\nMemory Manager는 이전에 알파와 베타 단계에서 제공되었다가 사용자들의 피드백과 실질적인 검증을 거쳐 이번 v1.32에서 GA 상태로 전환되었는데요. GA로의 전환은 안정성과 신뢰성을 보장한다는 의미를 담고 있습니다.\n📊 Memory Manager의 GA 진입이 앞으로 미칠 영향은?\nMemory Manager의 GA 진입은 쿠버네티스를 사용하는 기업과 개발자들에게 다음과 같은 긍정적인 영향을 미칠 것으로 기대됩니다.\n\n성능 민감한 애플리케이션의 도입 확대\n\n금융이나 의료 등 실시간 처리가 중요한 산업에서 쿠버네티스의 활용도가 높아질 것입니다.\n\n\n클라우드 네이티브 워크로드의 안정성 증가\n\n메모리 관련 장애를 줄이고 클러스터 안정성을 개선함으로써 DevOps 팀의 운영 부담을 줄여줍니다.\n\n\n오픈소스 커뮤니티의 생태계 성장\n\nMemory Manager를 활용한 다양한 툴과 확장 기능이 개발될 가능성이 높아집니다.\n\n\n\n🙏 마치며\n쿠버네티스 v1.32는 Memory Manager 기능의 GA 진입을 비롯해 많은 발전을 보여주었습니다.\nMemory Manager는 실시간 처리와 성능 최적화가 중요한 워크로드 배포 측면에서 쿠버네티스의 가치를 더욱 높여줄 것으로 보이는데요.\n앞으로 이 기능이 실제 환경에서 어떻게 활용될지 기대됩니다.\nReferences\n\nkubernetes.io/blog/2024/12/11/kubernetes-v1-32-release/\nen.wikipedia.org/wiki/Non-uniform_memory_access\nkubernetes.io/blog/2021/08/11/kubernetes-1-22-feature-memory-manager-moves-to-beta/\n"},"blog/Kubernetes-내부-인증서-관리-모듈-cert-manager-소개":{"title":"Kubernetes 내부에서 TLS 인증서를 관리해주는 cert-manager 소개","links":[],"tags":["Kubernetes/Security"],"content":"안녕하세요. 이번 글에서는 최근 CNCF에서 졸업한 cert-manager라는 Kubernetes 내부 TLS 인증서 관리 프로젝트에 대해 살펴보면서, TLS 인증서에 대해서도 알아보겠습니다.\n📜TLS 인증서가 무엇인가요?\nKubernetes 내부에서 배포된 여러 서비스가 서로 통신할 때, 암호화되지 않은 HTTP보다 보안 측면에서 우수한 HTTPS를 사용하는 경우가 많은데요. 이건 실생활에서 사용되는 대부분의 웹 서비스도 마찬가지입니다.\n\n(우리가 사용하는 웹 브라우저에서도 HTTPS를 사용하는 안전한 페이지는 위 사진과 같이 확인할 수 있습니다.)\nHTTPS는 기존 HTTP에 TLS라는 프로토콜을 사용해서 암호화한 것으로, 이 TLS 덕분에 HTTPS는…\n\n외부의 제3자가 전송되는 데이터를 보지 못하게 숨기고, (암호화)\n정보를 교환하는 당사자가 실제로 요청된 당사자임을 보장하며, (인증)\n전송하는 데이터가 위조되거나 변조되지 않았음을 확인 가능하게 됩니다. (무결성)\n\n이렇게 TLS가 보안 관점에서 뛰어난 이유는 인증서라는 개념을 사용하기 때문인데요. 인증서는 아래와 같은 역할을 합니다.\n\n클라이언트(요청을 하는 쪽)에서 접근하려는 서버(요청을 받는 쪽)가 신뢰할 수 있는 서버임을 보증\n통신 중 암호화할 때 사용하는 정보를 클라이언트에게 전달\n\n인증서에는 해당 서버에 대한 도메인 정보와 통신 중 암호화할 때 사용할 정보(정확히는 서버의 공개 키입니다.)가 담겨있는데요. 이 인증서는 서버가 가지고 있다가, 클라이언트가 해당 서버에게 요청을 하면 서버가 클라이언트에게 전달합니다.\n클라이언트는 서버로부터 받은 인증서를 이용해서 해당 서버가 신뢰할 수 있는지 확인하고, 이후 해당 서버와 통신할 때 인증서에 포함된 서버의 공개 키를 이용하여 암호화 통신을 할 수도 있는 거죠.\n이 인증서를 TLS 프로토콜을 사용한 인증서라고 해서 TLS 인증서라고 불립니다.\n❔신뢰할 수 있는 서버인 것을 어떻게 TLS 인증서로 알 수 있나요?\nTLS 인증서는 서버가 아닌 별도의 주체가 발급하는데요. 일반적으로 CA(Certificate Authority)라고 하는 공인된 기관에서 TLS 인증서를 발급합니다.\n서버가 자신의 정보와 공개 키를 CA로 보내 인증서 생성을 요청하면, CA는 CA의 비공개 키를 이용해서 서버가 보낸 도메인 정보와 서버의 공개 키를 암호화함으로써 인증서를 생성한 다음에 서버로 전달합니다.\n\n클라이언트가 해당 서버에 연결을 시도할 때, 서버로부터 TLS 인증서를 받는다고 했었죠. 이 TLS 인증서는 CA의 비공개 키로 암호화되어 있는데요.\nCA의 공개 키는 누구에게나 공개되어 있기 때문에 클라이언트는 TLS 인증서를 CA의 공개 키로 복호화 시도를 할 것이고, 만약 TLS 인증서 복호화에 성공하면 해당 인증서는 CA가 발급한 진짜 인증서라는 뜻이겠죠.\n이 과정으로 해당 서버가 신뢰할 수 있음을 TLS 인증서를 통해 알 수 있는 것입니다.\n\nTLS 인증서에 대해 살펴보다보니… 은근슬쩍 공개 키와 비공개 키라는 개념이 새로 등장했는데요. TLS 인증서에서 공개 키와 비공개 키, 또는 비대칭 암호화 방식이라고 하는 개념은 뗄 수 없는 관계이다보니 가볍게 짚고 가겠습니다.\n주고 받는 데이터를 암호화하는 방식 중 하나인 비대칭 암호화 방식은, 한 쌍의 공개 키와 비공개 키를 가지고 데이터를 암호화할 수 있습니다. 이 방식은…\n\n공개 키로 암호화한 데이터는 같은 쌍의 비공개 키로 풀 수 있고,\n비공개 키로 암호화한 데이터는 같은 쌍의 공개 키로 풀 수 있는 건데요.\n\n이렇게 데이터 암호화와 복호화에 두 가지 키를 사용하기 때문에 더 안전하다는 장점과 더불어 특징이 한 가지 더 있습니다. 암호화를 공개 키로 하느냐, 비공개 키로 하느냐에 따라 사용되는 분야가 달라진다는 점인데요.\n위에서 살펴봤던 TLS 인증서 사용 과정을 다시 보면, 비대칭 암호화를 사용하는 방식이 두 가지로 나뉩니다.\n\nCA가 자신의 비공개 키로 암호화한 인증서를 클라이언트가 CA의 공개 키로 복호화하여 CA가 발급한 인증서임을 확인\n클라이언트가 인증서에 포함된 서버의 공개 키로 통신을 암호화하여 서버로 전송 (이후 서버가 자신의 비공개 키로 복호화)\n\n즉, 비공개 키로 암호화한 경우에는 인증에 중점을 둔 것이고, 공개 키로 암호화한 경우에는 데이터 보안에 중점을 둔 것이라고 할 수 있습니다.\n지금까지 알아본 TLS 인증서를 Kubernetes 클러스터 내에서 쉽게 발급하고 서비스가 사용할 수 있도록 관리 가능한 것이 바로, 맨 위에서 이야기했던 cert-manager입니다.\n🔎cert-manager가 k8s 내부에서 TLS 인증서를 어떻게 관리하나요?\n\ncert-manager는 두 가지 CRD(Custom Resource Definition, k8s에서 커스텀으로 제작한 리소스 정의)를 사용하는데요. 바로, Issuer와 Certificate입니다.\nIssuer는 Kubernetes 클러스터 내에서 사용할 TLS 인증서를 발급해주는 리소스입니다. 위에서 살펴봤던 CA와 같은 역할을 수행하는데요.\n외부에 이미 존재하는 CA와 연동된 Issuer Type들이 이미 개발되어 있기 때문에 이걸 사용할 수도 있고, 클러스터 내부에서 직접 서명하는 Issuer Type을 사용해서 Issuer 리소스를 배포할 수 있습니다.\n다음은 Certificate입니다. Issuer가 발급할 인증서에 대해 정의하는 리소스인데요. 인증서 대상 도메인과 유효 기간, 갱신 시점 등을 정의한 Certificate 리소스를 배포하면, 미리 배포되어 있던 Issuer 리소스가 이를 감지하여 서명한 인증서와 키 정보 등을 Kubernetes Secret 리소스로 저장합니다.\n이렇게 cert-manager는 Kubernetes 내에서 사용할 인증서를 쉽게 발급 및 관리할 수 있다는 특징 덕분에 사용자도 점점 더 많아지고 있는데요. 최근 CNCF에서 졸업을 달성했기 때문에 이후 더 많은 적용 사례가 기대되는 프로젝트입니다.\nReferences\n\nvelog.io/@bambookim/HTTPS%EC%99%80-SSLTLS\nwww.cloudflare.com/ko-kr/learning/ssl/transport-layer-security-tls/\nbrunch.co.kr/@artiveloper/24\ncert-manager.io/docs/\n"},"blog/Kubernetes-컨테이너-디자인-패턴":{"title":"Kubernetes 컨테이너 디자인 패턴 소개","links":[],"tags":["Kubernetes"],"content":"☁️Kubernetes 컨테이너 디자인 패턴이란?\n디자인 패턴은 소프트웨어를 디자인하는 과정에서 자주 발생하는 문제들에 대한 일반적인 해결책을 의미합니다. 소프트웨어 개발 중에 자주 마주치는 문제를 어떻게 해결할 수 있을지 유형별로 정리된 디자인 패턴은, 특히 객체 지향 프로그래밍을 위해 고안된 것이 일반적인데요.\n전통적인 소프트웨어 개발뿐만 아니라, 컨테이너 형태로 애플리케이션을 배포할 때 사용할 수 있는 디자인 패턴도 있다는 사실, 알고 계셨나요?\n그래서 오늘은, Kubernetes에서 컨테이너를 배포할 때 활용 가능한 대표적인 컨테이너 디자인 패턴 4가지를 알아보겠습니다.\n🔎컨테이너 디자인 패턴 알아보기\n사이드카(Sidecar) 패턴\n\n사이드카 패턴은 기존에 존재하던 컨테이너 애플리케이션의 코드를 수정하지 않으면서도, 해당 애플리케이션의 로그를 기록하거나 모니터링과 같은 기능을 별도로 추가하고 싶을 때 사용하는데요.\nKubernetes의 Pod 내부에 기존 컨테이너 외로 필요한 기능을 수행하는 컨테이너를 추가하는 방식으로 사이드카 패턴을 구현할 수 있습니다.\n예를 들어, 웹서버 애플리케이션 컨테이너가 동작 중인 Pod 내에 로그 수집기 fluentd를 별도 컨테이너로 추가한 다음, 기존에 동작하던 웹서버의 로그를 fluentd 컨테이너에서 수집하는 식이죠.\n앰배서더(Ambassador) 패턴\n\n앰배서더 패턴은 Kubernetes에 배포된 컨테이너의 네트워크 연결을 전담하는 별도의 컨테이너를 두는 패턴인데요. 이 패턴을 적용하면, 복잡한 네트워크 통신과 관련된 기능은 별도의 컨테이너로 빼내고, 기존 컨테이너는 애플리케이션이 본래 수행해야 하는 핵심 기능에만 집중할 수 있다는 장점이 있습니다.\n그래서 앰배서더 패턴으로 추가한 컨테이너를 프록시(Proxy) 컨테이너라고도 하는데요. 외부 환경과 내부 서버 사이에서 통신이 오고갈 수 있는 가교 역할을 하는 프록시 서버와 비슷하게 동작해서 그렇습니다.\nKubernetes 환경에서 앰배서더 패턴 구현이 가능한 이유는, 동일한 Pod 내에서 동작하는 컨테이너는 서로 Localhost로 통신할 수 있기 때문입니다.\nHTTP 웹서버로 자주 사용되는 Nginx는 앰배서더 패턴으로 추가하하기 좋은 애플리케이션 중 하나입니다.\n어댑터(Adapter) 패턴\n\n외부와의 연결을 별도의 컨테이너에서 전담하여 간소화시키는 앰배서더 패턴과는 달리, 어댑터 패턴은 기존 컨테이너가 외부로 보내는 결과물을 통일화하는 데에 활용됩니다.\n특히 어댑터 패턴은 모니터링을 위해 외부 서비스와 연동할 때 외부 서비스에서 요구하는 양식이나 인터페이스를 맞추기 위해 사용되는데요. 기존의 컨테이너는 그대로 두면서, 컨테이너에서 나오는 결과물을 외부 규격에 맞추는 별도의 컨테이너를 동일한 Pod 내에 두는 방식으로 어댑터 패턴을 구현할 수 있습니다.\n만약 컨테이너로 동작 중인 로그 데이터 저장소 Elasticsearch의 상태와 관련된 로그를 외부 모니터링 툴 Prometheus 서비스에 저장해야 한다면, 동일한 Pod에 별도의 Prometheus Exporter 컨테이너를 추가하여 Prometheus 규격에 맞는 Elasticsearch의 상태 로그를 내보낼 수 있는 것이죠.\n초기화 컨테이너(Init Container) 패턴\n\n초기화 컨테이너 패턴은 기존 컨테이너가 실행되기 전에 필요한 작업을 별도의 컨테이너에서 수행하는 디자인 패턴입니다.\nKubernetes에선 Pod의 스펙을 정의하는 Manifest에서 initContainers라는 필드로 초기화 컨테이너 패턴을 정식으로 지원하고 있는데요.\n예를 들어 기존 컨테이너의 Elasticsearch 애플리케이션이 동작하려면 /usr/share 내 폴더의 소유권을 다른 사용자로 바꿔야 하는 경우, 별도의 초기화 컨테이너에서 해당 작업을 수행한 다음 Elasticsearch 컨테이너를 실행할 수 있는 겁니다.\nReferences\n\nmedium.com/@bijit211987/container-design-patterns-for-kubernetes-3742fca51b19\nitnext.io/4-container-design-patterns-for-kubernetes-a8593028b4cd\nstatic.googleusercontent.com/media/research.google.com//en/pubs/archive/45406.pdf\n"},"blog/Kubernetes의-경량화-버전-비교---minikube-vs-k3s-vs-k0s":{"title":"Kubernetes 클러스터를 쉽고 가볍게 구축할 수 있는 경량화 버전: minikube, k3s, k0s","links":[],"tags":["Kubernetes"],"content":"Kubernetes 경량화 버전이란?\n\nKubernetes는 컨테이너를 배포 및 관리할 수 있는 강력한 플랫폼입니다. 하지만 처음부터 Kubernetes 클러스터를 구축하려면 설치 과정이 복잡하고, 클러스터 구축에 많은 리소스가 요구된다는 단점이 있습니다.\n이런 어려움을 해결하기 위해, 클러스터를 가볍고 편리하게 구축할 수 있도록 도와주는 Kubernetes의 경량화 버전들이 많이 나오고 있는데요.\n오늘은 이러한 Kubernetes 경량화 버전인 minikube, K3s, k0s에 대해 살펴보겠습니다.\n✨다양한 운영체제에서 Kubernetes 테스트 환경을 쉽게 구축 가능한 minikube\n\nminikube는 macOS, Linux, Windows 로컬 환경에 Kubernetes 클러스터를 쉽게 구축하도록 도와주는 툴입니다.\nminikube는 기본적으로 Docker와 같은 컨테이너 런타임을 이용해서 클러스터 구성요소를 로컬에 배포 후 작동시키는데요.\n최신 Kubernetes 릴리즈 버전을 꾸준히 지원하기 때문에 새로운 Kubernetes 기능을 테스트하고 싶을 때 유용하게 활용할 수 있습니다.\nminikube 설치를 완료하면 CLI 환경에서 간단한 명령어로 Kubernetes 클러스터를 시작할 수 있고 직관적인 아이콘들로 구축 상황을 알려주는 등, 사용자가 편리하도록 신경을 많이 썼다는 느낌을 받을 수 있는데요.\nminikube는 macOS, Linux, Windows 운영체제를 모두 지원하는 Cross-platform인 것도 장점입니다.\n\n다만, 이번에 살펴볼 경량화 버전 중에선 가장 많은 리소스(2 CPU / 2GB RAM)가 필요하고 Docker나 Hyper-V와 같은 컨테이너 툴 혹은 가상 머신이 먼저 준비되어 있어야 한다는 단점도 존재합니다.\nminikube는 Kubernetes를 학습하시는 분들에게 실습 환경으로 추천해드릴 수 있는 툴인데요. 물론 Kubernetes 관련 기능이나 애플리케이션을 테스트하는 환경으로 사용하기에도 좋습니다.\nminikube 설치 안내 페이지: 링크\n💫더 가벼운 Kubernetes 클러스터를 구축해주는 K3S와 K0S\n\n다음은 Kubernetes의 경량화를 지향하는 K3S와 K0S입니다.\nKubernetes를 줄여서 일컫는 k8s와 비교해보면, 이 프로젝트들의 이름(K3S, K0S)에서 얼마나 가벼움을 지향하고 있는지 느낄 수 있는데요.\n이 두 프로젝트들은 모두 Kubernetes 클러스터를 구축하는 데에 필수적인 구성요소들을 간추려서 단일 파일로 설치 및 실행하기 때문에 리소스 경량화와 클러스터 구축 간소화를 모두 얻을 수 있었습니다.\n기존 Kubernetes는 클러스터의 데이터 저장소로 etcd를 사용하는데요. K3S와 K0S는 경량화를 위해 etcd보다 더 가벼운 sqlite3를 기본 데이터 저장소로 사용합니다. 물론 etcd나 MySQL 같은 다른 저장소도 지원하기 때문에, 필요한 경우라면 사용할 수도 있습니다.\n이렇게 다양한 방법으로 경량화를 꾀한 K3S와 K0S의 최소 요구사항은 아래와 같습니다.\n\nK3S: 1 CPU / 512MB RAM\nK0S: 1 CPU / 1GB RAM\n\n두 프로젝트 모두 minikube의 최소 요구사항의 절반 수준이죠.\n그래서 K3S와 K0S 모두 테스트 및 배포 환경에도 적합하지만, 적은 리소스 사용량 덕에 IoT 환경에도 사용하기 적합하다고 하는데요.\n다만 두 프로젝트 모두 단일 파일로 클러스터 구성요소를 작동시키는 방식이기 때문에, 현재 Linux 계열 운영체제만 공식 지원하고 있습니다.\nK3S와 K0S는 Linux 환경에 익숙하시면서 Kubernetes 개발 또는 테스트 환경이 필요하신 분들에게 추천해드릴 수 있는 경량화 버전입니다.\nK3S 설치 안내 페이지: 링크\nK0S 설치 안내 페이지: 링크\nKubernetes를 개인적으로 실습하거나 테스트하기 위해 클러스터를 직접 구축하기엔 준비해야 할 것들이 많아서 쉽지 않을 수 있는데요.\n그럴 땐 오늘 소개해드린 경량화 버전을 한번 고려해보시는 것도 좋을 듯합니다.\nReferences\n\nalperenbayramoglu2.medium.com/simple-comparison-of-lightweight-k8s-implementations-7c07c4e6e95f\nminikube.sigs.k8s.io/docs\ndocs.k3s.io/\ndocs.k0sproject.io/head/\nmedium.com/@thakur.ajay/kubernets-vs-k3s-vs-k0s-32f1da81a306\n"},"blog/LLMOps를-위한-오픈소스-플랫폼-Dify":{"title":"LLMOps를 위한 오픈소스 플랫폼 Dify 알아보기","links":["blog/LLMOps에-대해-알아보기"],"tags":["LLMOps"],"content":"🛠️LLMOps와 오픈소스 툴 Dify\nLLMOps는 우리가 지난 글에서 알아본 것처럼, 대규모 언어 모델(LLM)을 효율적으로 개발하고 운영하기 위한 방법론과 프로세스를 의미하는데요.\n이런 LLMOps를 위한 오픈소스 LLM 애플리케이션 개발 플랫폼 Dify에 대해 소개해드리고자 합니다.\n\nDify는 AI 워크플로우, RAG 파이프라인, LLM 관리, 모니터링 등이 가능해서 LLM 애플리케이션을 빠르게 프로덕션 환경에 배포할 수 있도록 도와준다고 합니다.\n✨Dify의 장점은?\n그렇다면 다양한 LLM 개발 플랫폼 중 우리가 Dify를 고려해야 할 이유는 무엇일까요?\n셀프 호스팅\nDify는 다른 클라우드 기반 LLM 개발 플랫폼과는 달리 오픈소스로 개발되고있기 때문에 셀프 호스팅이 가능합니다. 즉, 직접 제어할 수 있는 서버에 배포하여 웹으로 접근 가능하기 때문에 보안에 민감한 데이터를 기반으로도 더욱 안전한 환경에서 LLM 애플리케이션을 개발하고 배포할 수 있는 것이죠.\n확장성\n또한 Dify는 다양한 모델 제공자를 지원하고 있습니다.\n이미 대중적인 OpenAI부터 Azure나 Google Cloud뿐만 아니라, Ollama 같은 로컬 LLM 제공자도 지원하고 있는데요.\n이외에도 수십가지의 모델 제공자를 지원하고 있으며, 자세한 정보는 이 페이지에서 확인하실 수 있습니다.\nWeb UI 기반\nDify는 보기 쉬운 Web UI로 LLM 애플리케이션 개발과 관리가 가능하다는 장점 또한 가지고 있습니다.\n아래에서도 직접 확인해보겠지만, 이런 직관적인 UI 덕분에 LLM 애플리케이션 개발 및 관리가 한 층 더 쉬워진다는 장점이 있습니다.\n💻Dify를 로컬에 배포하고 접속해보기\nDify에 대해 알아봤으니 이제 Dify를 직접 사용해봐야겠죠?\n공식 Github 레파지토리에 업로드된 Docker Compose 파일을 이용하면 Dify 구동에 필요한 모든 구성요소를 아래와 같이 로컬 환경에 쉽게 배포할 수 있습니다.\nDify 소스 코드 clone\n먼저 아래 명령어로 로컬에 Dify 소스코드를 가져옵니다.\ngit clone github.com/langgenius/dify.git\nDify 실행\n아래 명령어로 Dify 구동에 필요한 환경변수 파일을 생성하고 Docker compose로 실행합니다.\ncd dify/docker\ncp .env.example .env\ndocker compose up -d\nDify를 Docker compose로 배포한 결과는 아래와 같습니다.\n\nDocker compose 명령어로 Dify 관련 컨테이너를 확인해보면 아래와 같죠.\n\n이렇게 배포된 컨테이너 중 주요 서비스 3개는 다음과 같습니다.\n\napi\nworker\nweb\n\n그 외 디펜던시 구성요소는 아래와 같습니다.\n\nweaviate\ndb\nredis\nnginx\nssrf_proxy\nsandbox\n\n이렇게 배포된 Dify의 구성요소 중 nginx 컨테이너가 배포되었기 때문에, 웹 브라우저에 localhost 경로로 Dify Web UI에 접속이 가능합니다.\n\n관리자 계정 설정 후 로그인을 완료하면 아래처럼 로컬 환경에서 Dify 플랫폼을 이용할 수 있게 되는 거죠.\n\n✅Dify 활용하기\n그렇다면 Dify를 어떻게 활용할 수 있을까요? 아래와 같이 Dify의 활용 방안을 간략히 정리해봤습니다.\n로컬 LLM을 연동하여 로컬 LLM 애플리케이션 개발\n아래처럼 Dify는 다양한 모델 제공자와 연동할 수 있는데, 그 중 Ollama를 선택하면 로컬에서 구동 중인 LLM과 연동하여 애플리케이션 개발이 가능합니다.\n조직 특성상 보안에 민감하거나 규모가 크지 않은 팀에서 활용하기 적절한 방식입니다.\n\nDify에서 기본 제공하는 템플릿 활용하여 LLM 애플리케이션 개발\nDify는 아래 사진처럼 다양한 LLM 애플리케이션 개발용 템플릿을 제공하는데요.\n\n이 중 원하는 템플릿을 선택하면 아래 이미지와 같이 해당 애플리케이션 개발을 위한 스튜디오 화면과 테스트 창이 표시되어 LLM 애플리케이션을 빠르게 개발 및 테스트가 가능합니다.\n\nLLM 애플리케이션 개발을 위한 컨텍스트 생성\n개발하려는 LLM 애플리케이션에 주입할 텍스트 데이터 등의 컨텍스트를 아래와 같이 Web UI에서 생성할 수 있습니다.\n\n컨텍스트로 저장 가능한 파일 포맷으로는 TXT, markdown, PDF, XLSX, CSV 등이 있습니다.\n\nReferences\n\ngithub.com/langgenius/dify\n"},"blog/LLMOps에-대해-알아보기":{"title":"LLMOps에 대해 알아보기","links":[],"tags":["LLMOps"],"content":"요즘 ChatGPT의 인기 덕분에 LLM(Large Language Model)이 큰 관심을 받고 있습니다.\n혹시 LLMOps라는 컨셉에 대해 들어보셨나요?\n이미 많이 알고계신 DevOps(Development + Operations)와 같은 원리로 LLM과 Operations가 합쳐진 용어인데요.\nLLMOps는 LLM의 전체 라이프 사이클 주기 동안 모델을 효율적으로 개발, 배포, 관리할 수 있도록 특화된 방법론 및 프로세스를 의미합니다.\nLLMOps가 필요한 이유\n그렇다면 LLMOps가 왜 필요할까요? 가장 큰 이유는 보안 때문입니다.\n어떤 기업의 직원이 외부의 LLM 제공업체의 서비스를 이용한다고 가정해보겠습니다. 이 직원은 자신의 업무 수행에 도움을 받기 위해 외부 LLM 서비스에 이것저것 질문을 할 텐데요.\n이때 회사 내부 정보도 질문에 포함될 가능성은 충분히 있겠죠. 게다가 이런 질문들은 LLM 제공업체의 서버로 전송되기 때문에, 기업 입장에선 외부 LLM 서비스를 이용하기 어렵습니다.\n이런 이유로 기업 내부에서 자체 LLM 서비스를 운영 및 관리하는 경우가 생기기 시작합니다. 그리고 효율적이면서 고도화된 LLM 학습, 운영, 관리를 위해 LLMOps가 필요하게 되는 것이죠.\nLLMOps의 동작 방식\nLLMOps 주기는 아래와 같이 나눌 수 있습니다.\n\nFM(Foundation Model) 선정\nUse Case에 맞춰 적용\n평가(Evaluation)\n배포\n모니터링\n\n그리고 LLMOps의 Workflow를 도식화하면 아래 그림처럼 표현할 수 있죠.\n\nMLOps와 LLMOps의 차이점\nLLMOps가 나오기 전에 이미 MLOps라는 용어가 있었습니다. 머신러닝 모델의 학습, 개발, 관리를 위해 생겨난 개념인데요.\n두 개념 모두 AI 모델의 전체 개발 주기를 대상으로 한다는 공통점이 있지만, 차이점도 있습니다. LLMOps를 좀 더 잘 이해할 수 있도록 MLOps와의 차이점을 살펴보겠습니다.\n먼저 LLMOps에서는 FM(Foundation Model)을 사용합니다. 대규모 학습이 필요한 LLM의 특성상, AI 모델을 처음부터 새로 만들고 학습시키려면 엄청난 규모의 자원이 소모되기 때문입니다.\n\nFM은 기반 모델이란 뜻으로, 다양한 도메인의 대규모 데이터셋으로 미리 학습된 모델을 의미합니다.\n참고로 이렇게 FM을 기반으로 AI 모델을 학습/운영하는 접근법은 FMOps라고 하며, 현재 FMOps와 LLMOps라는 용어는 엄격히 구분하지 않고 혼용됩니다.\n\n또한 LLMOps 관점에선 사용자의 피드백이 성능 개선에 중요하므로, 피드백 데이터가 모델에 반영될 수 있는 파이프라인 설계가 추가로 필요하다는 차이점도 있습니다.\nReferences\n\nLLMOps가 주목받고 있는 이유\nwww.ibm.com/topics/llmops\nubiops.com/llmops-vs-mlops\nFMOps and LLMOps: Operationalize Generative AI at Scale\n"},"blog/Liveness,-Readiness,-Startup-Probe-소개-및-비교":{"title":"Liveness, Readiness, Startup Probe 비교","links":[],"tags":["Kubernetes"],"content":"🔎Kubernetes의 Container Probe란?\n\nKubernetes에서 Pod를 배포하면 Pod에서 정의한 컨테이너가 실행되는데요. 컨테이너 내부에서 실행되어야 하는 프로세스가 정상 작동할 때 비로소 Pod를 통해 원하는 서비스를 이용할 수 있게 됩니다.\nPod를 배포하고 운영하다보면 동작 중이던 컨테이너의 상태가 정상인지 주기적으로 확인이 필요할 때가 있습니다. 혹은 해당 컨테이너가 외부 트래픽을 받을 준비가 되었는지 알아야 할 때도 있죠.\n이렇게 컨테이너의 상태를 주기적으로 진단할 때 사용하는 것이 바로, Container Probe입니다.\n🩺Container Probe의 진단 유형과 Probe의 종류\nKubernetes의 Container Probe는 Manifest의 컨테이너 레벨에서 정의되는데요. 정의된 진단 설정에 따라 kubelet이 해당 컨테이너 내부에서 주기적으로 진단을 수행합니다.\nProbe는 아래와 같이 4가지 방법 중 하나로 컨테이너의 상태를 진단할 수 있습니다.\n\nexec\n\n컨테이너 내부에서 실행할 명령어 지정\n명령어 실행 후 상태 코드가 0이면 성공으로 진단\nexec는 수행될 때마다 컨테이너 내에 새로운 프로세스가 생성되기 때문에 Node의 CPU 사용량이 증가할 수 있으므로 주의가 필요\n\n\ngrpc\n\ngRPC 요청으로 진단하며, gRPC Health Check이 미리 구현되어 있어야 함\n응답 status가 SERVING이면 성공으로 진단\n\n\nhttpGet\n\n특정 port 및 path로 Pod의 IP에 대해 HTTP GET 요청으로 진단\n응답 status code가 200 이상 400 미만이면 성공으로 진단\n\n\ntcpSocket\n\n특정 port로 Pod의 IP에 대해 TCP 요청으로 진단\n해당 port가 열려 있으면 성공으로 진단\n\n\n\n이렇게 수행한 진단 결과는 아래 3가지 중 하나로 나옵니다.\n\nSuccess: 진단 성공\nFailure: 진단 실패\nUnknown: 진단 실패, Faliure와 달리 kubelet이 추가 진단 실행\n\nkubelet이 수행하는 Probe는 아래와 같이 3가지 유형으로 나뉘는데요.\n\nReadiness Probe\nLiveness Probe\nStartup Probe\n\n각 Probe에 대해 아래에서 더 자세히 알아보겠습니다.\n✨Readiness, Liveness, Startup Probe\nReadiness Probe\nReadiness Probe는 트래픽과 관련이 있는데요. Readiness Probe를 사용하면 해당 컨테이너로 트래픽이 들어오는 시점을 제어할 수 있기 때문입니다.\n즉, 컨테이너의 상태 진단 결과가 성공인 시점부터 해당 컨테이너에 트래픽이 들어오는 것을 허용하고 싶을 때 Readiness Probe를 사용하는 것이죠.\n각 Probe는 컨테이너 레벨에서 정의한다고 했는데요. 아래 Pod Manifest 예제로 Probe를 어떻게 정의하는지 알아보겠습니다.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: goproxy\n  labels:\n    app: goproxy\nspec:\n  containers:\n  - name: goproxy\n    image: registry.k8s.io/goproxy:0.1\n    ports:\n    - containerPort: 8080\n    readinessProbe:  # Readiness Probe를 정의하는 부분입니다.\n      httpGet:\n\t    path: /healthz\n        port: 8080\n      initialDelaySeconds: 15\n      periodSeconds: 10\n위 예제에서 Readiness Probe를 정의하는 부분만 살펴보면 해당 Probe의 작동 방식은 아래와 같습니다.\n\n위 Readiness Probe는 HTTP GET 요청으로 진단 수행 (httpGet)\nHTTP GET 요청을 보내는 Port와 경로는 8080 및  /healthz\nkubelet이 첫 Probe 진단을 수행되는 시점은 컨테이너 실행 후 15초 뒤 (initialDelaySeconds)\nkubelet은 10초 간격으로 Probe 진단 수행 (periodSeconds)\n\n위 예제에선 컨테이너에 readinessProbe 하나만 정의되었지만, 필요에 따라 컨테이너에 대해 Probe를 종류별로 모두 정의할 수도 있습니다.\nLiveness Probe\nLiveness Probe는 동작 중인 컨테이너의 상태를 주기적으로 진단할 때 사용됩니다.\nKubernetes에 배포된 컨테이너의 프로세스에 이슈가 생기거나 프로세스의 상태가 Unhealthy인 경우, kubelet이 이를 감지하여 자동으로 Pod의 restartPolicy(재시작 정책)에 따라 조치를 취하기 때문에 Liveness Probe가 반드시 필요한 것은 아닙니다.\n하지만 Probe의 컨테이너 상태 진단 결과에 따라 종료 또는 재시작이 필요한 경우라면 Liveness Probe를 사용할 수 있습니다.\n위 예제 코드에서 봤던 것처럼, 모든 Probe는 spec.containers 레벨에서 정의할 수 있는데요. 위 코드에 exec를 수행하는 Liveness Probe를 추가로 정의한다면 아래와 같겠습니다.\nspec:\n  containers:\n  - name: goproxy\n    image: registry.k8s.io/goproxy:0.1\n    ports:\n    - containerPort: 8080\n    readinessProbe:  # Readiness Probe를 정의하는 부분입니다.\n      httpGet:\n\t    path: /healthz\n        port: 8080\n      initialDelaySeconds: 15\n      periodSeconds: 10\n    livenessProbe:  # 동일한 컨테이너에 대해 Liveness Probe를 추가로 정의했습니다.\n      exec:\n        command:\n\t\t- cat\n\t\t- /tmp/healthy\n      initialDelaySeconds: 5\n      periodSeconds: 5\n위 예제처럼 Liveness Probe와 Readiness Probe를 동일한 컨테이너에 대해 함께 사용할 경우, 아래와 같이 각 Probe의 역할 수행을 기대할 수 있습니다.\n\nReadiness Probe로 해당 컨테이너가 준비될 때까지 들어오는 트래픽 제어\nLiveness Probe로 해당 컨테이너 동작 중에 진단 실패 시 자동으로 재시작\n\nStartup Probe\n마지막으로 Startup Probe는, 그 이름처럼 컨테이너가 정상적으로 시작했는지 여부를 진단합니다.\n가동하는 데에 시간이 오래 걸리는 컨테이너의 상태를 진단해야 하는 경우라면, Liveness Probe의 periodSeconds 값을 길게 설정해서 진단 주기를 넓히기보다는 Startup Probe를 정의해서 컨테이너의 시작 성공 여부를 파악하는 것이 좋은데요.\n그래야 Liveness Probe로는 동작 중인 컨테이너의 상태를 적절한 주기로 진단할 수 있기 때문입니다.\nStartup Probe는 아래와 같이 정의할 수 있습니다.\nstartupProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  failureThreshold: 30\n  periodSeconds: 10\n위 예제를 보면 failureThreshold(최대 실패 허용 횟수) 값이 설정되어 있는데요.\n해당 Startup Probe는 최대 300초(30 * 10) 동안 주기적으로 컨테이너가 정상적으로 실행했는지를 진단하고, 300초가 지나도록 진단에 실패하면 Pod의 restartPolicy(재시작 정책)에 따라 컨테이너를 재시작하거나 종료하게 됩니다.\n한 가지 참고할 점은, Startup Probe의 진단이 성공하기 전까진 Readiness Probe와 Liveness Probe가 실행되지 않다는 것입니다.\n이렇게 각 Probe는 수행 시점과 역할이 조금씩 다르기 때문에, 컨테이너의 성격이나 운영 정책에 따라 적절한 조합으로 Probe를 사용한다면 더욱 효과적으로 Pod를 배포하고 운영할 수 있습니다.\nReferences\n\n# Configure Liveness, Readiness and Startup Probes\n# Pod Lifecycle\n"},"blog/Naver가-만든-리눅스-배포판-'NAVIX'":{"title":"Naver가 만든 리눅스 배포판 'NAVIX': 리눅스 생태계에 어떤 파장을 불러올까?","links":[],"tags":["DevOps"],"content":"최근 Naver가 무료로 제공하는 자체 리눅스 배포판 NAVIX를 발표하며 기술 업계의 관심을 끌고 있습니다.\nNAVIX는 기존 리눅스 배포판과 어떤 차별점을 갖고 있으며, Naver가 이를 통해 이루고자 하는 비전은 무엇일까요?\n이번 글에서는 NAVIX의 출발점과 미래 가능성을 살펴보겠습니다.\nNAVIX 한눈에 보기\n\nNAVIX에 대해 자세히 살펴보기 전에, 주요 특징을 가볍게 정리해보면 아래와 같습니다.\n\nNaver Cloud가 직접 제작하고 관리하여 지속적인 사용 가능\n누구나 소스코드에 접근하고 수정 가능하여 투명하게 공개된 오픈소스\n최신 보안 표준 준수로 시스템을 안전하게 보호\n\n이제 NAVIX의 탄생 배경과 다른 리눅스 배포판과의 차이점, 그리고 미래 활용 가능성을 차례로 알아보겠습니다.\n\nNaver가 리눅스 배포판을 만든 이유는?\nNAVIX의 개발 배경에는 급변하는 리눅스 시장에서 안정적인 리눅스 배포판 배포와 독립적이고 투명한 기술 생태계 구축이라는 목표가 있습니다.\nNaver는 대규모 서비스 운영과 클라우드 환경에서 보다 안정적이고 효율적인 운영체제를 필요로 했습니다.\n하지만 항상 변화하는 환경에 놓인 기존의 리눅스 배포판들은 이런 요구를 완벽히 충족하지 못했고, 그래서 Naver는 자체 배포판 개발을 결심하게 된 것이죠.\n또한, NAVIX는 Naver Cloud 플랫폼과의 통합성을 높이기 위해 설계될 것으로 보이는데요.\n이를 통해 자체 기술 생태계를 더욱 강화하고, Naver Cloud 플랫폼에서의 성능 최적화도 실현할 수 있기 때문입니다. AWS가 직접 리눅스 배포판을 개발해서 AWS 환경 내 성능 최적화를 한 것처럼 말이죠.\n그렇다면 NAVIX는 기존 리눅스 배포판과 어떤 차이점이 있을까요?\n이에 대해 알아보기 전에, 기존 리눅스 배포판에도 여러 가지가 있으니 우리가 많이 사용하는 Ubuntu와 CentOS의 차이를 먼저 짚어보고 가겠습니다.\n\nUbuntu와 CentOS는 어떻게 다를까?\n\n리눅스 생태계에서 가장 널리 사용되는 두 가지 배포판인 Ubuntu와 CentOS는 각기 다른 특징과 강점을 가지고 있는데요.\n\nUbuntu는 사용자 친화적인 인터페이스와 폭넓은 커뮤니티 지원을 자랑합니다. 데스크톱과 서버 환경에서 모두 쉽게 사용할 수 있어 개인 사용자에게도 적합하며, 최신 기술 도입에 적극적이죠.\nCentOS는 안정성을 최우선으로 합니다. 그래서 서버 환경에서 장기간 지원(LTS)이 필요한 경우 선호되죠. 보수적인 업데이트 주기를 통해 엔터프라이즈급 운영에 적합한 배포판입니다. CentOS가 Community Enterprise Operating System의 줄임말인 만큼, 그 성격이 이름에 바로 나와있음을 알 수 있죠.\n\n두 배포판의 그 외 다른 특징까지 함께 표로 정리하면 아래와 같습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUbuntuCentOSSystem CoreDebian 기반Red Hat 기반업데이트 주기자주가끔플랫폼의 주요 타겟개인 개발 서버상용화 서버패키지 관리apt-get, aptyum\n이 두 배포판은 각각의 용도에 따라 다르게 선택되는 상황인데, NAVIX는 이들 배포판의 장단점을 참고하여 새로운 방향성을 제시하고자 하려는 것 같습니다.\n이제, NAVIX와 기존 배포판이 어떻게 다른지 살펴보겠습니다.\n\nNAVIX는 기존 배포판과 무엇이 다를까?\nNAVIX는 기존의 Ubuntu나 CentOS를 기반으로 만들어졌지만, 여러 면에서 차별화를 보여주고 있는데요. 이를 정리하면 아래와 같습니다.\n\n지속 사용 가능한 안정성: Naver Cloud에서 직접 제작하고 관리하므로 장기적인 지원과 업데이트가 보장\n일관된 성능: 실행 중인 하드웨어나 워크로드에 관계없이 추적, 분석 툴을 통해 시스템 최적화 가능\n보안 강화: 자체 보안 패치를 추가 적용하여 내부 데이터 및 서비스 보호 강화\n\n이런 차별점은 NAVIX가 단순히 새로운 리눅스 배포판에 그치지 않고, 실질적인 서비스 운영과 개발자 경험을 개선하는 데 초점을 맞추고 있음을 보여줍니다.\n\nNAVIX는 어떤 미래를 그리고 있을까? 클라우드와 리눅스 생태계의 새로운 가능성\nNAVIX는 현재 Naver 내부에서 사용하고 있지만, 앞으로 외부 개발자 및 기업들에게도 주요 배포판으로 자리매김할 가능성이 있는데요. 몇 가지 시나리오를 살펴보면 아래와 같습니다.\n\nNaver Cloud 생태계 강화: Naver Cloud와 긴밀히 통합될 수 있기 때문에, 이를 사용하는 기업과 개발자들에게 최적화된 개발 및 운영 환경 제공 가능\n오픈소스 커뮤니티 기여: 오픈소스로 제공됨에 따라, 외부 커뮤니티가 참여하여 배포판을 발전시키고 확장될 수 있음\n\n그렇기 때문에 NAVIX는 단순한 리눅스 배포판이 아닌, 클라우드와 리눅스 생태계의 핵심 구성 요소로 발전할 잠재력도 가지고 있다고 볼 수 있겠습니다.\n\nNaver의 NAVIX 발표는 Naver가 자체 기술을 바탕으로 클라우드와 리눅스 생태계에서의 입지를 강화하려는 의지를 보여주는데요.\n앞으로 NAVIX가 기술 업계에 어떤 영향을 미칠지 지켜보는 것도 흥미로울 것 같습니다.\n\n참조\n\nwww.itmaya.co.kr/wboard/view.php\nm.blog.naver.com/PostView.naver\nnavix.navercorp.com/news/2024-09-01-navix/\n"},"blog/OpenSSH의-최근-보안-취약점-소개-(CVE-2024-6387)":{"title":"최근 발견된 OpenSSH 보안 취약점 CVE-2024-6387","links":[],"tags":["OpenSSH","CVE"],"content":"이번 글에선 최근 OpenSSH 패키지에서 발견된 보안 취약점 CVE-2024-6387에 대한 소개와 대처 방법을 공유드리겠습니다.\n🐡OpenSSH란?\n\nOpenSSH는 리눅스 PC의 SSH 연결을 위해 가장 널리 사용되는 무료 패키지입니다. (윈도우 PC나 MAC에서도 OpenSSH를 사용할 수 있습니다.) SSH는 Secure Shell의 줄임말로, 네트워크를 통해 다른 컴퓨터에 접근하거나 그 컴퓨터에서 명령 실행을 할 수 있도록 해주는 프로토콜을 의미합니다.\n또한 SSH는 원격 접근할 때 컴퓨터 간에 주고받는 데이터를 암호화하기 때문에 더욱 안전하다는 장점도 있는데요.\n즉, ==OpenSSH는 한 컴퓨터에서 다른 리눅스 서버로 암호화된 원격 접속을 할  수 있도록 도와주는 패키지==라고 할 수 있습니다.\nOpenSSH 패키지의 대표적인 툴은 아래와 같은데요.\n\nsshd: 다른 컴퓨터에서 원격 접속을 할 수 있도록 만들어주는 OpenSSH 서버\nssh: 다른 컴퓨터로 원격 접속할 때 사용\nscp: 다른 컴퓨터로 파일 전송할 때 사용\n아마 리눅스 서버를 다루셨다면 한 번쯤 접해봤을 가능성이 높은 툴들이죠.\n이 중에 ==OpenSSH 서버인 sshd에서 권한 제한 없이 원격에서 코드 수행이 가능할 수 있는 치명적인 보안 취약점이 발견==된 것입니다.\n\n❗이번 보안 취약점이 조금 특별한 이유?\nCVE-2024-6387을 최초로 발견한 IT 보안 업체 Qualys의 발표에 따르면, 이번 취약점은 ==2006년에 이미 발견되어 수정된 취약점 CVE-2006-5051이 최근 있었던 패치로 인해 다시 발생된 것==이라고 합니다.\n기존에는 잘 동작하던 소프트웨어가 패치 이후 버그나 문제가 생기는 것을 리그레션(Regression, 회귀)이라고 하는데요. 이번 보안 취약점도 리그레션으로 인해 발생한 OpenSSH의 취약점이라고 해서  regreSSHion이란 별칭을 얻게 되었습니다.\n참고로 이런 리그레션을 방지하기 위해 소프트웨어의 패치를 올릴 때 기존 기능이 정상 동작하는지 확인하는 테스트를 리그레션 테스트(Regression Test, 회귀 테스트)라고 합니다.\n\n✅CVE-2024-6387 대처 방법\nOpenSSH 서버(sshd)를 사용 중이라면 CVE-2024-6387에 영향을 받을 수 있는데요. 이번 보안 취약점에 어떻게 대처할 수 있을까요?\n대처하기 전에 ==가장 먼저 확인해야 할 것은 현재 사용 중인 OpenSSH의 버전입니다.\n위에서 설명한 것처럼 해당 취약점은 이전에 이미 수정된 것이 최신 패치로 인해 다시 발생했기 때문에, 아래와 같이 이전에 동일한 취약점이 수정된 버전 ~ 문제가 되기 이전 버전까지는 이번 보안 취약점에 대해 안전==하다고 볼 수 있습니다.\n\n ==4.4p1 ~ 8.5p1==\n\nOpenSSH 개발팀에서 이 이번 보안 취약점을 수정한 최신 9.8 버전을 릴리즈했기 때문에, 만약 사용 중인 버전이 안전한 범위 밖에 있다면 최신 버전으로 업그레이드하는 방법도 있습니다. (관련 문서)\n&lt;!주의!&gt;\nOpenSSH를 업그레이드할 경우, 기존에 사용 중이던 OpenSSH 관련 작업이 중지되거나 기존 설정이 변경될 수 있으므로 세심한 주의가 필요합니다.\n또한 ==OpenSSH 서버 관련 설정 파일인 /etc/ssh/sshd_config을 업그레이드 전 미리 백업==하시는 것을 추천합니다.\n현재 OpenSSH 공식 GitHub에서 안내하고 있는 OpenSSH 최신 버전 설치 방법은 아래와 같이 확인하실 수 있습니다.\ngithub.com/openssh/openssh-portable/tree/master#building-from-git\n\nReferences\n\nwww.qualys.com/regresshion-cve-2024-6387\nubuntu.com/security/CVE-2024-6387\nwww.openssh.com/security.html\nchancoding.tistory.com/165\n"},"blog/Service-Mesh-소개":{"title":"Service Mesh 소개","links":["📆-Project/Guide-to-DevOps/KO/Istio-소개"],"tags":["Network"],"content":"\n클라우드 개발 환경에서 종종 Service Mesh란 키워드를 들으신 적이 있을 수 있습니다. Service Mesh라고 하면 네트워크와 관련된 것 같은 느낌이 있는데요.\n이름에 담긴 느낌처럼, Service Mesh의 간략한 정의는 ‘애플리케이션의 서비스 간 모든 통신을 처리하는 소프트웨어 계층’이라고 할 수 있습니다.\nService Mesh를 도입하는 이유\n그렇다면 Service Mesh를 도입해서 서비스 간 모든 통신을 처리하고 관리해야 할 이유는 무엇일까요? 이는 아래와 같이 정리할 수 있습니다.\n\n가시성\n\nMSA 환경에서 서비스 간에 주고받는 트래픽에 대한 로그를 얻을 수 있으므로, 애플리케이션 외부에서 일어나는 이벤트의 가시성을 확보 가능\n\n\n트래픽 관리\n\n서비스 간에 주고받는 트래픽을 독립된 레이어에서 제어 가능\n\n\n보안성\n\n상호 TLS(mTLS) 설정으로 서비스간 통신에서 검증이 가능\n별도의 Policy를 적용하여 서비스간 통신 가능 여부를 제어할 수 있고, 이를 코드로 관리 가능\n\n\n\nService Mesh가 동작하는 방식\n그렇다면 Service Mesh는 어떻게 동작할까요? Service Mesh의 두 구성 요소인 데이터 플레인(Data Plane)과 컨트롤 플레인(Control Plane)으로 나눠서 알아보겠습니다.\n\n데이터 플레인\n\nService Mesh의 데이터 처리 역할 수행\n각 서비스에 Service Mesh를 위한 별도의 네트워크 프록시가 Sidecar로 추가되며, 해당 프록시는 서비스로 들어오고 나가는 모든 트래픽이 거치는 중간 게이트 역할\n서비스 간의 통신이 발생할 때 동작 과정\n\nSidecar로 추가되었던 네트워크 프록시가 요청을 받음\n받은 요청을 별도의 네트워크 연결로 캡슐화\n출발 프록시와 도착 프록시 간의 안전하고 암호화된 채널 설정\n\n\n이외에도 로드 밸런싱, Circuit Breaker, 트래픽 라우팅과 같은 역할도 수행\n\n\n컨트롤 플레인\n\nService Mesh의 중앙 관리 및 구성 계층 역할 수행\n서비스 간의 라우팅 규칙, 로드 밸런싱 정책, 보안 설정 등이 가능한 영역\n이외에도 Mesh 내 모든 서비스를 추적하는 레지스트리, Telemetry 데이터의 수집 및 집계 역할도 수행\n\n\n\nService Mesh를 구현하는 방법\n지금까지 알아본 Service Mesh는 어떻게 적용할 수 있을까요? 대표적인 Service Mesh 솔루션으로 Istio가 있습니다.\nIstio는 Kubernetes와 함께 작동하도록 설계되어 있어 호환성이 높으며, 이때 Istio의 네트워크 프록시가 Kubernetes의 Pod 내 Sidecar 컨테이너로 추가되는 방식으로 Service Mesh를 구현합니다.\nIstio와 관련된 더욱 자세한 글은 여기서 확인해보세요.\nReferences\n\nIstio는 무엇이고 왜 중요할까?\n서비스 메시란 무엇인가요?\n실무자를 위한 서비스 메시 - 지금 서비스 메시가 의미 있는 이유\n"},"blog/k8s-환경에서의-안정성을-위한-카오스-엔지니어링":{"title":"Kubernetes 환경에서의 안정성 테스트를 위한 카오스 엔지니어링","links":[],"tags":["Cloud","Test"],"content":"🧪카오스 엔지니어링이란?\n\n카오스 엔지니어링이란, 개발 중인 시스템의 안정성을 테스트하기 위해 임의적으로 장애 상황을 조성하는 기법을 말합니다.\n그래서 혼란이란 뜻의 카오스(Chaos)가 이름에 붙은 것인데요.\n카오스 엔지니어링을 통해 시스템상에서의 취약점은 없는지 알 수 있기 때문에 시스템을 더욱 안정적으로 만들 수 있죠.\n☁️클라우드 네이티브 환경에서 카오스 엔지니어링이 필요한 이유\n\n클라우드 네이티브 환경은 다양한 컴포넌트가 유기적으로 연결되어 있어 그 자체로 복잡하기 때문에, 테스트하기 쉽지 않은데요.\n서비스의 부하 등을 측정하는 일반적인 테스트 툴로는 이런 분산 시스템 환경에서 효율적으로 테스트하기 어려우므로, 카오스 엔지니어링이라는 다른 접근법이 필요한 것입니다.\n카오스 엔지니어링은 시스템에 임의적인 장애 상황을 일으킨 다음, 시스템이 어떻게 반응하는지 확인하여 잠재적인 이슈를 잡아내고 사고를 미연에 방지할 수 있습니다.\n그렇다면 실제 카오스 엔지니어링은 어떻게 수행할 수 있을까요?\nCNCF 재단의 Incubating 프로젝트로 등록된 Chaos Mesh를 소개하며 알아보도록 하겠습니다.\n⛓️Kubernetes 카오스 엔지니어링에 특화된 Chaos Mesh\n\nChaos Mesh는 클라우드 네이티브 환경을 위한 오픈소스 카오스 엔지니어링 플랫폼입니다.\n다양한 종류의 장애 상황 시뮬레이션이 준비되어있기 때문에, 구성할 수 있는 장애 시나리오의 폭도 매우 넓다는 장점이 있습니다.\nChaos Mesh는 동작에 필요한 각 컴포넌트(contoller-manager, daemon, dashboard)가 Kubernetes 클러스터상에 CRD(Custom Resource Definition)로 배포되는 방식으로 설치하는데요.\n그래서 Helm을 사용하거나 Chaos Mesh가 자체로 작성한 쉘 스크립트를 실행해서 설치할 수 있습니다. 자세한 설치 정보는 공식 페이지에서 확인하실 수 있습니다.\nChaos Mesh 설치가 완료되면 Experiment라고 하는 테스트 객체를 생성해야 합니다. Chaos Mesh에는 기본적으로 다양한 장애 상황을 발생시킬 수 있는 리소스가 준비되어 있고, 이런 리소스를 Experiment로 사용하는데요.\nChaos Mesh에서 기본적으로 생성 가능한 대표적인 장애 발생 리소스는 아래와 같습니다.\n\nPodChaos: Pod 상태를 의도적으로 불안정하게 만들거나 Pod 내 특정 Container가 동작에 실패하는 상황 조성\nNetworkChaos: 네트워크 지연, 패킷 손실 등의 상황 조성\nStressChaos: CPU 또는 메모리에 대한 부하 조성\n\nExperiment는 k8s 리소스처럼 yaml 파일로 정의 후 kubectl로 실행할 수 있는데요.\n아래는 NetworkChaos로 만든 Experiment의 예시입니다. 이 Experiment를 실행하면 default 네임스페이스 내 &quot;app&quot;: &quot;test&quot; 라벨을 가진 Pod에 대해 12초 동안 10ms의 네트워크 지연을 유발시키는 테스트가 진행됩니다.\napiVersion: chaos-mesh.org/v1alpha1  \nkind: NetworkChaos  \nmetadata:  \n\tname: network-delay  \nspec:  \n\taction: delay\n\tmode: one\n\t\tnamespaces:  \n\t\t\t- default  \n\t\tlabelSelectors:  \n\t\t\t&#039;app&#039;: &#039;test&#039; \n\tdelay:  \n\t\tlatency: &#039;10ms&#039;  \n\t\tduration: &#039;12s&#039;\n또한 여러 개의 Experiment는 하나의 Workflow로 구성해서 관리할 수 있습니다. 즉 Workflow를 구성함으로써 전체적인 카오스 엔지니어링 프로세스를 정의하고 수행할 수 있는 거죠.\n그리고 Chaos Mesh의 기본 컴포넌트 중에는 Dashboard도 존재하는데요. Chaos Experiment를 직관적으로 관리하고 모니터링할 수 있어 테스트 시나리오를 관리하는 측면에서 유용할 것으로 보입니다.\n\nReferences\n\nchaos-mesh.org/docs/\nmedium.com/@seifeddinerajhi/chaos-engineering-on-kubernetes-a-beginners-guide-revel-in-chaos-0974ae9bee8b\n"},"blog/kube-bench-소개":{"title":"kube-bench 소개","links":["blog/CIS-Kubernetes-Benchmark"],"tags":["Kubernetes","Security"],"content":"kube-branch는 CIS Kubernetes Benchmark를 기반으로 k8s 환경이 보안적으로 안전한지 검토해주는 툴입니다.\nCIS Kubernetes Benchmark에 대해 궁금하시다면 여기서 확인해보세요.\nkube-bench를 사용하는 이유\nkube-bench는 공식 레파지토리에서 배포하는 Job yaml 파일 또는 Docker 이미지를 실행하는 것만으로도 CIS Kubernetes Benchmark 검토를 쉽게 수행할 수 있고, Benchmark 결과도 바로 확인할 수 있기 때문에 많이 사용됩니다.\nkube-bench 실행 방법\n컨테이너 이미지로 실행\n\nk8s 환경에서 아래 명령어로 kube-bench의 Docker 이미지를 실행합니다.\n\ndocker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t docker.io/aquasec/kube-bench:latest --version 1.28\n이때 --version은 현재 검토하려는 k8s 클러스터의 버전을 명시합니다.\n\n\n명령어를 실행하면 kube-bench의 공식 Docker 이미지를 통해 CIS Benchmark 검토가 진행되며, 완료 시 검토 결과를 바로 확인할 수 있습니다.\n\nk8s Job Object로 실행\n\nk8s 환경에서 kube-bench 공식 레파지토리에서 제공하는 Job yaml 파일을 가져옵니다. (파일 링크)\n가져온 yaml 파일을 이용해 kube-bench를 실행하는 k8s Job을 아래 명령어로 클러스터에 배포합니다.\n\nkubectl apply -f {가져온 yaml 파일 이름}.yaml\n\n\n배포한 Job 실행이 완료되었다면 아래 명령어로 Benchmark 검토 결과를 확인할 수 있습니다.\n\nkubectl logs {배포한 Job의 Pod 이름}\n\n\n\nkube-bench의 실행 결과\n\nkube-bench를 원하는 방법으로 실행하고나면, 위 이미지와 같은 검토 결과를 얻을 수 있습니다.\n각 카테고리 별로 CIS 보안 사항에 대한 검토 결과를 확인할 수 있으며, 현재 보안 사항이 준수되고 있는지의 여부(PASS/FAIL)와 보안 사항의 세부 설명에 대해서도 확인 가능합니다.\nReferences\n\ngithub.com/aquasecurity/kube-bench\n"},"blog/kubectl-플러그인-매니저-krew-사용해보기":{"title":"kubectl 플러그인 매니저 krew 사용해보기","links":[],"tags":["Kubernetes"],"content":"🔎krew란?\n\nKubernetes를 운영 및 관리할 때 가장 많이 사용하는 툴이라고 하면 역시 kubectl일 텐데요. kubectl 덕분에 우린 커맨드 창에서 Kubernetes의 각 Object를 배포, 관리, 테스트할 수 있습니다.\n이런 kubectl을 더욱 편리하고 효율적으로 사용할 수 있도록 도와주는 플러그인이 다양한 사람들에 의해 개발되고 있다는 사실, 알고 계셨나요?\nkrew는 이렇게 개발되는 kubectl 플러그인을 저장하고 배포하는 플러그인 매니저입니다.\n2024년 7월 기준 현재 264개의 kubectl 플러그인이 krew에 등록되어 있는데요.\nkrew를 사용하면 CLI상에서 kubectl 플러그인을 편리하게 검색하거나 로컬에 설치할 수 있습니다.\nmacOS, Linux, Windows 등 대부분의 운영체제에서도 지원되는 krew를 직접 사용해보고, 추천 플러그인도 소개해보겠습니다.\n🛠️krew 사용해보기\n로컬에서 krew를 사용하려면 git이 먼저 설치되어 있어야 합니다. 각 운영체제별 krew 설치 방법은 공식 링크에서 확인할 수 있습니다.\nkrew를 설치했다면 이제 직접 사용해볼 차례입니다. 먼저 아래 명령어로 최신 플러그인 리스트를 받아옵니다.\n$ kubectl krew update\n그리고 아래 명령어로 사용 가능한 플러그인을 탐색할 수 있습니다.\n$ kubectl krew search\n \nNAME                            DESCRIPTION                                         INSTALLED\naccess-matrix                   Show an RBAC access matrix for server resources     no\nadvise-psp                      Suggests PodSecurityPolicies for cluster.           no\nauth-proxy                      Authentication proxy to a pod or service            no\n[...]\n아래 명령어로 원하는 플러그인을 다운로드 가능합니다.\n$ kubectl krew install ctx # 예제에선 kubectx라는 플러그인을 다운로드합니다.\n설치한 플러그인의 최신 업데이트를 아래 명령어로 한 번에 설치할 수도 있습니다.\n$ kubectl krew upgrade\n더 이상 사용하지 않는 플러그인은 아래 명령어로 삭제할 수 있습니다.\n$ kubectl krew uninstall ctx # 예제에선 설치된 kubectx 플러그인을 삭제합니다.\nkrew는 kubectl 플러그인 매니저 툴이기 때문에 설치/사용 방법이 어렵진 않은데요. 이제 실제로 사용하기 좋은 kubectel 플러그인 3가지를 추천드리겠습니다.\n✨추천 kubectl 플러그인 소개\nkubectx\n가장 먼저 소개해드릴 추천 플러그인은 kubectx입니다. kubectx는 Kubernetes의 context object를 쉽게 변경할 수 있게 도와주는 플러그인인데요.\nKubernetes의 context란, 여러 Kubernetes 클러스터에 접근할 수 있도록 관련 config 값을 모아둔 Object를 말합니다.\n이런 context를 변경해서 다른 Kubernetes 클러스터에 접근하려면 기존에는 아래와 같은 과정을 거쳐야 했습니다.\n\nkubectl config get-contexts 명령어로 변경 가능한 context 이름 확인\nkubectl config use-context {context 이름} 명령어로 context 변경\n\n하지만 kubectx 플러그인을 사용하면 context를 간단한 CLI 명령어만으로 쉽게 변경 가능합니다.\nkubectx는 아래와 같이 설치할 수 있습니다.\n$ kubectl krew install ctx\n설치 완료 후 아래와 같은 명령어로 현재 context 확인 및 context 변경이 가능합니다.\n$ kubectl ctx\ncoffee\nminikube\ntest\n \n$ kubectl ctx coffee\nSwitched to context &quot;coffee&quot;\n \nneat\n다음으로 소개해드릴 kubectl 플러그인은 neat입니다.\nKubernetes 클러스터에 배포된 object의 Manifest를 참고하거나 가져오기 위해 kubectl get ... -o yaml 형태의 명령어를 사용할 때가 많은데요.\n하지만 이렇게 가져온 Manifest에는 아래 이미지와 같이 object 배포 당시 메타데이터나 현재 object의 상태와 관련된 데이터 등도 포함하고 있어서 가독성이 떨어집니다.\n\n이 때 neat 플러그인을 사용하면, 아래와 같이 object 배포에 필요없는 정보가 제외된 Manifest를 얻을 수 있습니다.\n\nneat는 아래와 같이 설치할 수 있습니다.\n$ kubectl krew install neat\nneat를 사용하는 가장 직관적인 방법은 파이프라인(|)을 사용하는 것입니다. 아래와 같이 기존 kubectl get 명령어에 kubectl neat 명령어를 연결하면 가독성이 더욱 좋은 Manifest를 얻을 수 있습니다.\n$ kubectl get ... -o yaml | kubectl neat\nkail\n마지막으로 소개해드릴 플러그인은 kail입니다. Kubernetes와 tail을 합친 이름을 지닌 kail은, Kubernetes의 다양한 Object에서 발생하는 로그를 실시간으로 보여주는 플러그인입니다.\nKubernetes 상에서 배포된 애플리케이션에 대한 로그를 확인하고 싶을 때 우린 kubectl logs {Object 종류}/{Object 이름} 명령어를 주로 사용합니다.\n하지만 이런 방식은 단일 Object에 대한 로그만 확인이 가능하며, 여러 Object의 로그를 한 번에 확인하는 등 좀 더 복잡한 로그 조회는 어렵죠.\nkail은 이런 문제를 해결해주는 플러그인입니다.\nkail은 아래와 같이 설치할 수 있습니다.\n$ kubectl krew install tail\nkail을 이용하면 Service, ReplicaSet, Deployment 등을 Argument로 필터링한 다음, 매칭되는 Pod의 로그를 한 번에 확인할 수 있습니다.\n예를 들어, frontend라는 이름의 Service와 webapp이라는 이름의 Deployment에 포함된 Pod의 로그를 확인하고 싶다면 아래와 같은 kail 명령어를 사용하면 되는 것이죠.\nk tail --svc frontend --deploy webapp\nkail을 이용하면 특정 Namespace와 Node에서 동작 중인 Pod의 로그도 한 번에 확인할 수 있습니다.\n예제를 위해 stress란 이름의 Namespace에 일정 주기로 테스트용 로그를 생성하는 nginx 컨테이너 및 cache 컨테이너가 포함된 Deployment가 배포했습니다.\n그리고 해당 Object들은 playground라는 이름의 Node 위에서 동작하고 있죠.\n이때 kail로 stress Namespace와 playground Node에서 동작 중인 모든 Pod의 로그를 조회하면 아래와 같이 표시됩니다.\n&lt;stress Namespce에서 동작하는 모든 Pod 로그 조회&gt;\n\n&lt;playground Node에서 동작하는 모든 Pod 로그 조회&gt;\n\nReferences\n\nkrew.sigs.k8s.io\ngithub.com/ahmetb/kubectx\ngithub.com/itaysk/kubectl-neat\ngithub.com/boz/kail\n"},"blog/ollama와-Open-WebUI-로컬-배포":{"title":"ollama와 Open-WebUI 로컬 배포","links":[],"tags":["Docker","Ollama"],"content":"ollama와 Open-WebUI\nLLM을 활용한 서비스가 다양하게 출시되는 요즘, 로컬에서 LLM을 사용할 수 있도록 도와주는 ollama이라는 툴에 관심이 생겼습니다.\n오픈 LLM 모델의 GGUF 파일이 있다면 ollama를 이용해 로컬 환경에서 LLM과 상호작용이 가능한데요.\n로컬 LLM은 개인 정보 유출 위험이 적고 비용 발생도 없다는 장점이 있습니다.\n그래서 제 노트북의 로컬 환경에 직접 ollama를 실행시킨 다음, 공개된 LLM 모델을 가져와 테스트를 진행해봤습니다.\n테스트를 진행한 노트북 사양은 아래와 같습니다.\n\nCPU: AMD Ryzen 7 4800H with Radeon Graphics 2.90 GHz\nRAM: 32 GB\n\nCLI 환경에서 동작하는 ollama를 보다 쉽게 사용하기 위해, Open-WebUI라는 툴을 함께 사용했는데요.\nOpen-WebUI는 Chat GPT와 유사한 UI를 가지고 있고, 호스트에 실행 중인 ollama와 연동되어 웹 브라우저상에서 LLM에 질문을 하거나 다양한 LLM 관련 설정도 가능합니다.\nDocker Compose를 활용하여 로컬 배포\nollama와 Open-WebUI 로컬 배포에 대해 조사해보니 모두 로컬에 직접 설치하는 글이 대부분이었지만, 각 툴이 호스트 환경으로부터 독립되어야 일관된 기능이 보장될 수 있으므로 우리는 Container 환경에서 실행해보도록 하겠습니다.\n다행히 ollama와 Open-WebUI 모두 공식 Container Image가 공개되어 있어서 Docker로 실행하는 데엔 어려움이 없겠는데요.\n하지만 로컬 LLM을 사용하고 종료할 때마다 이 툴들의 Container Image를 실행하고 다시 종료하려면 손이 많이 갈 것 같습니다.\n그래서 여러 Container를 한 번에 배포할 수 있는 Docker Compose를 활용하도록 하겠습니다.\nDocker Compose는 한 개 이상의 Container를 항상 동일한 옵션과 조건으로 한 번에 실행할 수 있도록 도와주는 기능입니다. Container 실행에 필요한 각종 정보를 compose.yaml이라는 파일에 정의해두었다가, docker compose 명령어를 실행하면 yaml 파일에 정의된 Container들이 실행되는 방식입니다.\n배포 과정\n먼저 아래와 같이 Docker Compose 파일을 정의합니다.\nservices:\n  openWebUI:\n    image: ghcr.io/open-webui/open-webui:main\n    restart: always\n    ports:\n      - &quot;3000:8080&quot;\n    extra_hosts:\n      - &quot;host.docker.internal:host-gateway&quot;\n    volumes:\n      - open-webui-local:/app/backend/data\n \n  ollama:\n    image: ollama/ollama:0.1.34\n    ports:\n      - &quot;11434:11434&quot;\n    volumes:\n      - ollama-local:/root/.ollama\n \nvolumes:\n  ollama-local:\n    external: true\n  open-webui-local:\n    external: true\n다음은 Docker Volume 생성입니다. Volume은 Container 동작 중에 생성/수정되는 데이터를 저장하는 공간인데요.\n위 compose.yaml에서 정의한 바와 같이, ollama-local(ollama의 데이터 저장)와 open-webui-local(Open-WehUI의 데이터 저장)라는 이름의 Docker volume을 생성하기 위해 터미널에서 아래 명령어를 실행합니다.\n\ndocker volume create ollama-local\ndocker volume create open-webui-local\n\n이제 docker compose 명령어로 두 개의 Container를 로컬에 배포해볼 건데요. 그 전에 compose.yaml 파일이 정상적으로 실행되는지 확인하기 위해 아래 명령어로 dry run을 해보겠습니다. (dry-run은 어떤 명령어가 예상대로 동작하는지 모의 실행하는 것을 말합니다. 해당 명령어가 실제로 실행되는 것은 아닙니다.)\n\ndocker compose --dry-run up -d (compose.yaml 파일이 존재하는 경로에서 실행)\n\n\ndry run이 잘 실행되는 것을 확인했으니 이제 아래 명령어를 실행하여 실제로 로컬 배포를 진행해보겠습니다.\n\ndocker compose up -d (compose.yaml 파일이 존재하는 경로에서 실행)\n\n\n각 Container가 정상 실행되었다는 메시지를 확인 후, compose.yaml에서 정의한 Open-WebUI의 Port 번호를 참고하여 웹 브라우저에서 localhost로 접속합니다. (본 예제에서 Open-WebUI 경로: http://localhost:3000)\n\n웹 브라우저로 접속한 Open-WebUI 창에서 Sign up 버튼을 눌러 계정을 새로 만들고 접속합니다. (이렇게 만든 계정은 우리가 이전에 생성한 Open-WebUI의 Docker Volume에 저장되므로 Sign up은 최초 한 번만 필요하며, 이후엔 계정으로 로그인하면 됩니다.)\n\n아직 ollama에서 사용할 LLM 모델이 없으므로, Open-WebUI의 오른쪽 상단의 톱니바퀴 버튼을 누른 뒤 models 메뉴 내 Pull a model from Ollama.com 옵션 입력창에 원하는 LLM 모델의 태그를 입력합니다. (본 예제에서는 llama3:8b를 가져왔습니다. ollama에서 제공하는 LLM 목록은 여기서 확인 가능합니다.)\n\nLLM 모델 다운로드가 완료되면 홈 화면의 왼쪽 상단에서 다운로드한 모델 선택이 가능하고, 이후 Chat을 진행할 수 있습니다.\n\n\n로컬 배포한 Container 관리\n만약 Docker Compose로 로컬 배포한 ollama와 Open-WebUI Container를 종료하고 싶다면 아래 명령어를 실행합니다.\n\ndocker compose down (compose.yaml 파일이 존재하는 경로에서 실행)\n\n추후에 용량 관리를 위해 ollama와 Open-WebUI가 사용하던 Volume을 삭제하고 싶다면 아래 명령어를 실행합니다. (Backup하지 않은 Volume은 삭제 후 복구할 수 없습니다.)\n\ndocker volume rm {대상 Volume 이름}\n\nReferences\n\ngithub.com/ollama/ollama\ngithub.com/open-webui/open-webui\n"},"blog/ollama와-crewAI로-로컬-환경에-블로그-포스팅-시스템-구축":{"title":"ollama와 crewAI로 로컬 환경에 블로그 포스팅 시스템 구축","links":["blog/ollama와-Open-WebUI-로컬-배포"],"tags":["Ollama","CrewAI"],"content":"🦙🧑‍🤝‍🧑Ollama와 CrewAI\nOllama는 로컬 환경에서 LLM을 실행하는 오픈소스 툴입니다. 지난 글에서 Docker로 Ollama와 Open-WebUI라는 툴을 실행해서  웹 브라우저로 로컬 LLM에게 질문을 해보는 튜토리얼을 진행한 적이 있었죠. (관련 블로그 글)\n이번엔 Ollama와 CrewAI를 활용해서 로컬 LLM 기반으로 블로그 글을 작성해주는 시스템을 구축해보려합니다.\nOllama만으로도 충분히 블로그 글을 자동으로 작성할 수 있지 않냐고요? 물론 Ollama로 실행한 LLM에게 부탁해도 글을 써줍니다. 하지만 CrewAI라는 툴을 사용하면, 각자의 역할과 목표를 가지고 있는 여러 LLM 기반 작업자(에이전트)가 일련의 프로세스를 거쳐 더욱 체계적으로 글을 써줄 수 있거든요.\n\n방금 이야기한 블로그 글 작성 시스템을 예로 들면서 알아보겠습니다.\nDevOps 관련 블로그 글을 쓸 때는 보통 아래와 같은 프로세스로 진행이 될 텐데요.\n\n인터넷 자료 조사\n조사한 내용을 토대로 글쓰기\n작성한 글에 오탈자는 없는지 검수하기\n\n이러한 각 과정을 수행하는 에이전트들을 둬서 서로 상호작용하며 작업을 수행하도록 시스템을 만드는 것이 CrewAI의 역할입니다.\n게다가 CrewAI는 파이썬 기반으로 개발되었고 직관적인 명령어들을 사용하기 때문에, 쉽고 빠르게 여러 에이전트로 구성된(Multi-Agent) 작업 수행 시스템을 구축할 수 있다는 장점도 있습니다.\nCrewAI의 에이전트는 역할, 목표, 배경으로 정의하는데요. 각 에이전트가 작업을 수행할 때 자신은 어떤 배경을 가지고 있고, 어떤 목표를 수행하는지 등을 미리 알려주는 거죠.\n\n위의 블로그 글 작성 시스템으로 다시 돌아와서, 위에서 언급한 프로세스의 3가지 작업을 담당하는 CrewAI 에이전트들을 아래처럼 정의해보겠습니다.\n\n인터넷 자료 조사\n\n역할: Researcher\n목표: 최신 DevOps 관련 토픽 조사\n배경: IT 대기업에서 근무 중인 세계적인 Researcher\n\n\n글쓰기\n\n역할: Writer\n목표: DevOps 관련 블로그 글 작성\n배경: IT 관련 글 작성에 특화된 최고의 Technical Writer\n\n\n검수하기\n\n역할: Proofreader\n목표: 기술 블로그 글 검수\n배경: IT 분야에 특화된 유명 Proofreader\n\n\n\n그리고 각 수행되어야 하는 작업도 아래와 같이 정의할 수 있습니다.\n\n최신 DevOps 관련 뉴스 조사\n\n담당 에이전트: Researcher\n출력물 설명: 약 3문단 분량의 최신 DevOps 관련 리포트\n\n\n조사 리포트를 기반으로 DevOps 관련 블로그 글 한 편 작성\n\n담당 에이전트: Writer\n출력물 설명: 약 4문단 분량의 Markdown 형식 DevOps 관련 블로그 글\n\n\n제공된 블로그 글을 보다 자연스럽게 검수\n\n담당 에이전트: Proofreader\n출력물 설명: 약 4문단 분량의 Markdown 형식 DevOps 관련 블로그 글\n\n\n\n\n웹 접근 관련 유의사항\n로컬 LLM을 사용하는 상황에서 웹 접근이 필요한 Researcher 같은 경우엔 Google search API 서비스 등을 별도로 이용해야 합니다.\n그래서 이번 실습에선 카드 등록 없이 이메일 등록으로 최대 2,500회 Google Search 쿼리가 가능한 Serper 서비스를 이용했습니다.\n\n🖥️Ollama와 CrewAI로 블로그 글 작성 시스템 구축하기\n이제 로컬에서 직접 Ollama와 CrewAI를 실행해서 블로그 글 작성 시스템을 구축해보도록 하겠습니다. 각 툴은 Docker로 로컬에 배포합니다.\n이번 실습에선 Ollama의 llama3(8b) 모델을 활용할 예정인데요. 그럴려면 먼저 Ollama를 이용해서 llama3 모델을 로컬에 가져와야겠죠.\nOllama의 모델이 저장될 공간인 Docker volume을 먼저 아래 명령어로 생성합니다.\ndocker volume create ollama-local\nDocker volume을 생성했다면 이제 compose.yaml라는 이름의  Docker compose 파일을 생성하겠습니다. Docker compose는 여러 Docker 컨테이너의 배포 설정을 쉽게 관리하고 실행할 수 있도록 정의하는 파일인데요. 지금은 우선 Ollama에 대해서만 정의해보겠습니다.\ncompose.yamlservices:\n  ollama:\n    image: ollama/ollama:0.1.34\n    container_name: ollama\n    ports:\n      - &quot;11434:11434&quot;\n    volumes:\n      - ollama-local:/root/.ollama #LLM이 저장될 Volume 지정\nvolumes:\n  ollama-local:\n    external: true\ncompose.yaml 작성이 끝나면 해당 파일이 있는 경로의 터미널에서 아래 명령어로 Docker compose를 실행하겠습니다. 지금은 Docker compose 파일에 정의되어 있는 Ollama만 실행되겠죠?\ndocker compose up -d\nOllama가 정상 실행되었다면 터미널에 아래처럼 표시가 될 겁니다.\n\n이제 실행된 Ollama 컨테이너에 접속해서 우리가 사용할 llama3 LLM을 가져오겠습니다.\n먼저 아래 터미널 명령어로 Ollama 컨테이너에 접속합니다.\ndocker exec -it ollama bash\n접속한 터미널은 아래와 유사한 모습일 겁니다.\n\n이 상태에서 아래 Ollama 명령어를 입력해서 공개된 원격 저장소에서 LLM을 가져옵니다.\nollama pull llama3:8b\n위 명령어를 입력하면 아래처럼 LLM을 가져오는데요. 가져온 LLM은 처음에 생성했던 Docker volume ollama-local에 저장됩니다.\n\ncompose.yaml 파일에서 Ollama 컨테이너와 ollama-local volume 연동 설정을 넣어두었기 때문에, compose.yaml 파일로 실행하는 Ollama는 llama3:8b LLM을 계속 사용할 수 있게 됩니다.\nOllama로 LLM 설치는 완료되었으니, exit 명령어로 컨테이너에서 나옵니다.\n이제 CrewAI를 Docker로 실행해볼 건데요. CrewAI는 파이썬 패키지이므로, 우리가 작성해야 할 파일은 아래와 같이 총 3가지입니다.\n\nCrewAI를 Docker에서 실행하기 위한 Dockerfile(crewai.Dockerfile)\nCrewAI 관련 설정과 정의 후 실행하는 Python 스크립트(main-crewai.py)\nmain-crewai.py 실행에 필요한 패키지를 정의한 requirements.txt\n\ncrewai.DockerfileFROM python:3.12.4\n \nWORKDIR /app\nCOPY requirements.txt ./requirements.txt\nRUN pip install -r requirements.txt\n \nCOPY main-crewai.py ./\n \nCMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;main-crewai.py&quot; ]\nmain-crewai.pyimport os\nfrom crewai import Agent, Task, Crew, Process\nfrom crewai_tools import SerperDevTool\n \n# 로컬에 실행 중인 ollama에서 원하는 LLM 가져옴. 예제에선 llama3 (파라미터 사이즈 8b) 사용\nfrom langchain.llms import Ollama\nollama_model = Ollama(\n    base_url=&#039;http://ollama:11434&#039;,\n    model=&quot;llama3:8b&quot;)\n \nos.environ[&quot;OTEL_SDK_DISABLED&quot;] = &quot;true&quot;\n \n# researcher Agent가 웹에 접근해서 최신 IT 정보를 찾을 수 있도록 server.dev API 서비스 이용\nos.environ[&quot;SERPER_API_KEY&quot;] = &quot;{자신의 serper API key를 넣어주세요}&quot;  # serper.dev API key\nsearch_tool = SerperDevTool()\n \n# crewai 패키지로 원하는 Agent의 역할(role)과 목표(goal) 설정\n# 최신 DevOps 관련 토픽을 조사하는 Agent 정의\nresearcher = Agent(\n    role=&#039;Researcher&#039;,\n    goal=&#039;Discover a newest and attracting topic about DevOps&#039;,\n    backstory=&quot;You&#039;re world class researcher working on a big IT company&quot;,\n    verbose=True,\n    allow_delegation=False,\n    llm=ollama_model,\n    tools=[search_tool]\n)\n \n# 블로그 글을 작성하는 Agent 정의\nwriter = Agent(\n    role=&#039;Writer&#039;,\n    goal=&#039;Create DevOps blog post&#039;,\n    backstory=&quot;You&#039;re a best technical writer who is specialized on writing IT content&quot;,\n    verbose=True,\n    allow_delegation=False,\n    llm=ollama_model\n)\n \n# 작성된 글을 검수하는 Agent 정의\nproofreader = Agent(\n    role=&#039;Proofreader&#039;,\n    goal=&#039;Edit and proofread technical article&#039;,\n    backstory=&quot;You&#039;re a famous proofreader who is specialized on IT domain&quot;,\n    verbose=True,\n    allow_delegation=False,\n    llm=ollama_model\n)\n \n# 정의한 Agent들로 수행할 작업(Task) 정의\nresearch_task = Task(\n    description=&#039;Investigate the latest DevOps news&#039;,\n    agent=researcher,\n    expected_output = &#039;A comprehensive 3 paragraphs long report on the latest and famous DevOps.&#039;\n)\n \nwriting_task = Task(\n    description=&#039;Write a blog post about DevOps with one topic provided from the researcher&#039;,\n    agent=writer,\n    expected_output=&#039;A 4 paragraph article about DevOps formatted as markdown.&#039;,    \n)\n \nproofreading_task = Task(\n    description=&#039;Proofread the provided blog post to make more natural article&#039;,\n    agent=proofreader,\n    expected_output=&#039;A 4 paragraph article about DevOps formatted as markdown.&#039;,    \n)\n \n# 위 Agent와 Task, 작업 프로세스를 정의\ncrew = Crew(\n  agents=[researcher, writer, proofreader],\n  tasks=[research_task, writing_task, proofreading_task],\n  llm=ollama_model,\n  verbose=2, # crew 작업 중에 발생하는 로그의 자세한 정도를 설정 가능.\n  process=Process.sequential # Task가 순차적으로 실행될 수 있도록 sequential로 정의.\n)\n \n# 정의한 crew 실행 및 작업 과정에서 발생하는 로그 출력\nresult = crew.kickoff()\nprint(result)\nrequirements.txtcrewai==0.32.0\ncrewai-tools==0.2.6\nlangchain==0.1.20\nCrewAI 관련 파일 준비가 끝났다면, 아래 Docker 명령어로 CrewAI가 실행될 Docker 이미지를 생성합니다.\ndocker build -t my-crewai -f crewai.Dockerfile .\nCrewAI Docker 이미지 빌드가 끝났다면, 위에서 정의했던 compose.yaml 파일을 아래와 같이 최신화합니다.\ncompose.yamlservices:\n  ollama:\n    image: ollama/ollama:0.1.34\n    container_name: ollama\n    ports:\n      - &quot;11434:11434&quot;\n    volumes:\n      - ollama-local:/root/.ollama\n  crewai:\n    image: my-crewai\n    container_name: crewai\n    depends_on:\n      - ollama\n    extra_hosts:\n      - &quot;telemetry.crewai.com:127.0.0.1&quot; # To avoid &#039;Connection to telemetry.crewai.com timed out&#039; error when using local LLM\nvolumes:\n  ollama-local:\n    external: true\n이제 터미널에서 아래 Docker compose 명령어를 입력하면, CrewAI의 각 에이전트가 작업을 수행하는 과정과 최종 결과물을 터미널에서 확인할 수 있습니다. (명령어 마지막에 docker compose down을 연결한 것은 CrewAI 작업이 모두 완료되면 Ollama와 CrewAI 컨테이너 모두 정상 종료시키기 위함입니다.)\ndocker compose up -d &amp;&amp; docker compose logs crewai -f &amp;&amp; docker compose down\n🗂️CrewAI의 에이전트들의 작업 과정과 최종 생성 결과물\n이렇게 실행한 CrewAI의 로그를 살펴보면, 각 에이전트가 일하는 과정을 로그로 확인할 수 있습니다.\n\n\n또한 에이전트가 내놓은 최종 결과물도 확인할 수 있죠.\n\nCrewAI의 Researcher 에이전트가 작성한 리포트를 토대로 Writer 에이전트가 블로그 글을 써주는 등, 각 에이전트가 미리 정의된 프로세스대로 상호작용하는 것을 보니 정말 흥미로웠는데요.😄\n블로그 글 작성 외에도 CrewAI를 활용해서 어떤 작업 프로세스를 수행할 수 있을지 궁금해지네요.😊\nReferences\n\ndocs.crewai.com/\nfossengineer.com/ai-agents-crewai/#building-the-crewai-container\n# Create a Blog Writer Multi-Agent System using Crewai and Ollama\n"},"blog/더-안전한-CICD-파이프라인-만들기---SLSA-표준":{"title":"CI/CD 파이프라인을 더 안전하게 만드는 방법: SLSA 표준","links":[],"tags":["CICD"],"content":"🛡️SLSA의 보안 수준 4단계\n\n오늘날 수많은 기업과 조직에서 효율적인 개발 프로세스를 위해 CI/CD를 도입하는 경우가 많습니다. CI/CD 파이프라인으로 개발 이후의 코드 통합과 배포를 자동화하면 애자일한 개발이 가능하지만, 이런 파이프라인 내부에 보안 취약점이 존재한다면 심각한 보안 사고로 이어질 수도 있겠죠.\n그래서 2023년 4월, Google에서 소프트웨어 공급 프로세스의 보안 기준을 제시했는데요. 바로 SLSA(Supply-chain Levels for Software Artifacts)입니다.\nSLSA는 소프트웨어 산출물(Software Artifacts)을 공급하는 프로세스(Supply Chain)의 보안 수준(Levels)을 4단계로 정의한 표준인데요. SLSA의 단계는 보안 준수 정도에 따라 Build L0부터 Build L3까지 나눠집니다. SLSA의 특정 단계를 준수하고 있다는 것은 그만큼의 보안 조치를 취하고 있음을 알리는 것이죠.\nSLSA에서 정의한 보안 수준 4단계를 간략히 소개하면 아래와 같습니다.\n\nBuild L0:\n\nSLSA 조건에 부합하지 않은 상태\n개발과 테스트 빌드가 동일한 머신에서 수행되고, SLSA 표준 고려가 없는 상태를 의미\n\n\nBuild L1:\n\n패키지가 어디서 어떻게 빌드되었는지 명세된 문서(Provenance)가 있는 상태\n빌드 프로세스 변경 없이 SLSA 보안 지침을 일정 부분 준수하려는 개발 조직 대상\n\n\nBuild L2:\n\n호스팅되는 플랫폼 상에서 CI/CD 프로세스의 빌드가 이뤄지고, 빌드 후Provenance가 생성되는 상태\n호스팅되는 빌드 플랫폼으로 전환하여 SLSA 보안 지침을 적정 수준으로 준수하려는 개발 조직 대상\nBuild L1을 먼저 준수해야 함\n\n\nBuild L3:\n\n보안에 강한 빌드 플랫폼을 사용하고 있는 상태\nBuild L2를 먼저 준수해야 함\n\n\n\n🔎SLSA에서 정의한 CI/CD 파이프라인 대상 위협과 개선방안\n\nSLSA에서는 CI/CD 프로세스가 받을 수 있는 보안 위협도 위 그림과 같이 8가지로 정의했습니다. 그림에 표시된 각 위협과 개선방안을 살펴보면 아래와 같습니다.\n\nA: 소스코드 무단 변경\n\n개선방안: 소스코드 커밋에 대해 두 명 이상이 리뷰\n\n\nB: 소스코드 레파지토리 무단 침입\n\n개선방안: 보안이 강화된 소스코드 플랫폼 사용\n\n\nC: 무단으로 수정된 소스코드로부터 빌드\n\n개선방안: SLSA를 준수하는 빌드 서버 사용하여 빌드 후 생성되는 Provenance 확인\n\n\nD: 무단으로 수정된 디펜던시(의존성) 사용\n\n개선방안: Provenance를 확인 가능한 환경 조성을 위해 SLSA 준수\n\n\nE: 빌드 프로세스 무단 침입\n\n개선방안: 더 높은 SLSA 단계를 준수하여 안전한 빌드 환경 조성\n\n\nF: 기존 빌드 프로세스를 거친 것이 아닌, 무단으로 수정된 패키지 업로드\n\n개선방안: 패키지의 Provenance를 조회할 수 있도록 하여 정상적인 레파지토리에서 정상적인 과정으로 빌드되었는지 확인\n\n\nG: 패키지 저장소 무단 침입\n\n개선방안: F의 개선방안과 유사하게, 패키지의 Provenance를 조회할 수 있도록 환경 조성\n\n\nH: 무단으로 수정된 패키지 사용\n\n해당 위협에 대해 SLSA에서 개선방안을 직접 제안하지는 않지만, 패키지의 Provenance를 활용 가능할 것\n\n\n\n지금까지 SLSA에서 정의한  CI/CD 프로세스 대상 위협과 개선방안에 대해 살펴봤는데요. 이런 소프트웨어 공급 프로세스에 대한 위협은 프로세스를 통해 만들어진 패키지를 공격자의 의도대로 악용할 수 있도록 무단 수정하는 방식이 대부분이었습니다.\n그렇기 때문에 SLSA 표준에서는 패키지가 어디서 어떻게 빌드되었는지 그 기원을 기록한 Provenance를 중요하게 여기고 있는 것으로 보이는데요.\n만약 여러분이 개발하고 있거나 운영 중인 CI/CD 파이프라인의 보안을 개선하고 싶으셨다면, SLSA의 가이드를 참고해봐도 좋을 듯합니다.\nReferences\n\nslsa.dev/\ndeveloper.cyberark.com/blog/what-is-slsa-supply-chain-levels-for-software-artifacts/\n"},"blog/무중단-배포의-종류와-설명":{"title":"무중단 배포의 종류와 설명","links":[],"tags":["Deployment"],"content":"무중단 배포의 종류\n무중단 배포는 아래와 같이 크게 3가지 방식으로 나뉩니다.\nRolling 방식\n동작 중인 인스턴스를 점진적으로 업데이트하는 방식입니다.\n\n\n장점:\n\n인스턴스를 추가로 늘리지 않아도 괜찮습니다.\n인스턴스마다 차례로 버전이 전환되기 때문에 상황에 따라 롤백이 가능합니다.\n\n\n\n단점:\n\n신버전을 배포하고 구버전의 인스턴스 수가 감소하면서 사용 중인 인스턴스에 트래픽이 몰릴 수 있습니다.\n배포 과정에서 구버전과 신버전이 동시에 존재하는 시점이 생기고, 이때 사용자들이 통일되고 균일한 서비스를 받지 못하게 된다.\n\n\n\nBlue / Green 방식\n동작 중인 인스턴스 환경과 동일한 환경에서 새로운 버전을 배포한 뒤, 로드밸런서를 통해 모든 트래픽을 새로운 버전의 인스턴스 환경으로 한 번에 전환하는 방식입니다.\n\n\n장점:\n\n구버전의 인스턴스가 그대로 남아있기 때문에 롤백하기 쉽습니다.\n새 버전의 테스트가 용이합니다.\n\n\n\n단점:\n\n인스턴스 가동에 필요한 시스템 자원이 두 배로 필요합니다.\n인스턴스를 새로 가동하는 환경에 대한 테스트를 사전에 완료해야 합니다.\n\n\n\nCanary 방식\n동작 중인 인스턴스 환경과 동일한 환경에서 새로운 버전을 배포한 뒤, 소수의 사용자 트래픽을 새로운 버전으로 보내 문제가 없음을 확인합니다.\n문제가 없다면 점점 더 많은 사용자 트래픽을 새로운 버전으로 전달하는 방식입니다.\n참고로 Canary(카나리)라는 이름은, 광부들이 유독 가스에 민감한 ‘카나리아’라는 새를 자신들의 작업 환경에 미리 풀어 가스 누출 여부를 감지했던 것에서 유래되었습니다.\n\n\n장점:\n\nA/B 테스트로 활용가능합니다.\n\n\n\n단점:\n\n네트워크 트래픽 제어 작업이 추가로 필요합니다.\n\n\n"},"blog/보안에-강한-Dockerfile-작성-팁-5가지":{"title":"보안에 강한 Dockerfile 작성 팁 5가지","links":[],"tags":["Docker","Security"],"content":"\n들어가기\nDockerfile은 Docker 컨테이너 이미지를 빌드할 때 사용되는 각종 설정과 명령어를 선언한 파일입니다. 우리가 개발 환경을 구축하거나 서비스를 배포할 때 누군가 미리 만들어놓은 Docker 이미지를 사용하기도 하지만, 직접 Dockerfile을 작성해서 Docker 이미지로 빌드하는 경우도 많은데요.\n더욱 안전한 Docker 이미지를 제작할 수 있는 Dockerfile 작성 팁이 있다는 사실, 알고 계셨나요?\nDockerfile을 작성할 때 보안을 신경써야 하는 이유\n오늘날 대부분의 웹 서비스는 마이크로서비스 아키텍처를 따르고 있습니다. 하나의 커다란 서비스를 배포하는 것이 아닌, 기능이나 성격에 따라 나눠진 작은 서비스 여러 개를 배포 및 운영하는 방식을 마이크로서비스 아키텍처라고 하는데요.\n이때 마이크로서비스를 각각의 Docker 컨테이너 내부에서 동작하도록 구성하는 것이 일반적입니다. 하지만 이렇게 실제로 배포된 컨테이너 이미지가 외부 공격에 취약하다면… 생각만 해도 아찔한 보안 사고로 이어지겠죠.\n그래서 우리는 Dockerfile을 작성할 때부터 보안에 신경써야 합니다.\n보안을 위한 Dockerfile 작성 팁 5가지\n그렇다면 Dockerfile을 어떻게 작성하면 좋을까요? 여기 보안에 강한 Dockerfile 작성 팁팁 5가지를 소개해드립니다.\nMulti-Stage 방식으로 빌드\nDockerfile 내에서 애플리케이션 빌드 명령어 실행 후 나오는 최종 산출물을 또다른 Base Image로 복사하고, 해당 Base Image를 최종 Docker Image로 빌드하는 기법을 Multi-Stage라고 하는데요.\nMulti-Stage 방식으로 빌드된 Docker 이미지 내에는 애플리케이션 구동에 필요한 최소한의 요소만 담겨있으므로, 자연스럽게 잠재된 보안 취약점도 더 적어집니다.\nMulti-Stage를 사용해서 간단한 Golang 애플리케이션을 빌드하는 Dockerfile 예제를 같이 살펴보겠습니다.\n# /src/main.go 파일을 빌드하는 build 스테이지입니다.\nFROM golang:1.21 as build \nWORKDIR /src\nCOPY &lt;&lt;EOF /src/main.go\npackage main\n \nimport &quot;fmt&quot;\n \nfunc main() {\n  fmt.Println(&quot;hello, world&quot;)\n}\nEOF\nRUN go build -o /bin/hello ./main.go\n \n# build 스테이지로부터 빌드된 산출물들만 가져와 실행하는 최종 스테이지입니다.\nFROM scratch\nCOPY --from=build /bin/hello /bin/hello\nCMD [&quot;/bin/hello&quot;]\n위 Dockerfile을 실행하면 먼저 build라는 이름이 붙여진 스테이지가 먼저 실행되는데요. golang:1.21 이미지 위에 go build 명령어로 애플리케이션이 빌드되는 구간입니다.\n이후 또다른 스테이지가 실행되고, build 스테이지에서 빌드된 최종 산출물을 복사한 뒤 실행하는 로직이 수행되는데요. 위 Dockerfile로 빌드되는 이미지는 이 최종 스테이지의 내용만 담게 되는 것입니다.\n필요없는 패키지 제거\nDocker 컨테이너 내 애플리케이션 구동에 필요없는 패키지를 제거하는 것 역시 잠재 보안 취약점을 줄여주기 때문에 권장됩니다.\nRoot가 아닌 별도의 사용자를 생성 및 사용\nContainer 내에 Root 계정이 사용될 경우, 침입 사고 발생 시 피해가 커질 수 있습니다. 그래서 아래 예시와 같이 Root 계정이 아닌 별도의 그룹 및 사용자를 생성하고 사용하는 것이 안전합니다.\nRUN groupadd -r for-example &amp;&amp; useradd -r -g for-example for-example\nUSER for-example\nContainer 내 민감한 파일들은 Read Only로\n배포 이후 Docker Container 내에서 추후 수정이 필요하지 않는 파일들을 Read Only로 설정하면 보안성을 높일 수 있습니다.\nchmod -R a-w {폴더명} 또는 chmod a-w {파일명} 명령어를 사용하면, 모든 사용자는 해당 폴더 또는 파일에 대한 쓰기 권한을 잃게 됩니다.\nShell Access 제거\n컨테이너 안에서 실행 가능한 sh나 bash 등의 Shell은 동작 중인 컨테이너 내 애플리케이션을 디버깅할 때 사용되는 경우가 있지만, 이런 통로를 제거하면 컨테이너 내부 침입이 어려워져 보안성이 높아집니다.\nShell 접근을 제거할 경우, 아래 주의사항에 대해 고려해야 합니다.\n\n컨테이너 이미지에서 동작하는 애플리케이션이 Shell을 사용하고 있지 않은지 확인해야 합니다.\n해당 이미지를 배포한 뒤에 디버깅할 수 있는 다른 대안을 미리 마련해야 합니다.\n\n컨테이너 보안을 지킬 수 있는 또다른 방법\nDocker 이미지를 직접 제작할 땐 위와 같은 방법으로 안전한 이미지를 만들어 사용할 수 있지만, 만약 이미 누군가가 제작한 Docker 이미지를 사용할 때엔 그 이미지가 안전한지 어떻게 알 수 있을까요?\n이럴 때 사용할 수 있는 것이 바로, 컨테이너 보안 스캐닝 툴입니다.\n대표적으로 Trivy가 있는데요. 컨테이너 보안 스캐닝 툴에 대해서는 추후 다른 글에서 소개해보겠습니다.\nReferences\n\n# Best practices for writing Dockerfiles\n"},"blog/블록체인-개발에-적용-가능한-DevOps":{"title":"블록체인 개발에 적용 가능한 DevOps","links":["blog/Infrastructure-as-Code-(IaC)-알아보기"],"tags":["DevOps","블록체인"],"content":"⛓️블록체인 개발에 DevOps를 도입한다면\n블록체인은 최근 큰 관심을 이끈 기술입니다. 분산 거래 기록 시스템이라고도 불리는데요.\n지금도 블록체인은 다양한 곳에서 활용되고 있고, 수많은 블록체인 프로젝트도 개발 및 유지보수되고 있습니다.\n\n블록체인은 네트워크를 구성하고 있는 노드들이 네트워크에서 발생하는 데이터를 함께 검증해서 무결성을 증명해내는 기술이기 때문에, 시간이 흘러 블록체인 네트워크가 성숙해질수록 효율적인 네트워크 관리와 보안 사고 예방이 매우 중요해집니다.\n이렇게 블록체인 프로젝트를 개발하면서 가시화되는 문제를 정리하면 아래와 같을 것입니다.\n\n복잡성과 규모\n\n블록체인 네트워크는 수많은 노드와 방대한 양의 데이터로 점점 더 복잡해지고 있습니다.\n그로 인해 효율적인 노드 관리가 더욱 필요해지고 있습니다.\n\n\n보안\n\n트랜잭션과 데이터의 보안은 블록체인 기술에서 가장 중요한 요소입니다.\n그렇기 때문에 블록체인 네트워크에 대한 악의적인 공격을 실시간으로 감지하고 대응하는 것이 필요합니다.\n또한 스마트 컨트랙트나 블록체인 애플리케이션 코드에 취약점이 있는지 미리 검사하여 보안 사고를 예방하는 것 역시 중요해졌습니다.\n\n\n급속한 트렌드 변화\n\n블록체인 도메인은 빠르게 성하고 있기 때문에 이런 흐름에 맞추려면 애플리케이션이나 서비스를 짧은 주기로 자주 업데이트하고 적응하는 것이 필요합니다.\n\n\n\n위 내용은 DevOps를 통해 해결할 수 있는 문제와 많이 닮아있는데요.\n그래서 블록체인 개발 프로젝트에 DevOps를 도입하여 이런 문제들을 해결하는 경우가 많습니다.\n🧑‍💻블록체인 개발에 적용 가능한 DevOps\n블록체인 프로젝트 개발에 적용할 수 있는 DevOps 요소는 아래와 같이 크게 4가지로 나눠서 정리할 수 있습니다.\n\nCI/CD\n\n테스트 자동화\n\n스마트 컨트랙트 및 블록체인 애플리케이션 테스트 자동화\n\n\nCI/CD 파이프라인 구축\n\n블록체인 애플리케이션 빌드, 테스트, 배포를 자동화하는 CI/CD 파이프라인 구축\n\n\n\nIaC\n\n인프라 프로비저닝 자동화\n\nTerraform 등의 툴로 블록체인 네트워크 구축에 필요한 인프라 리소스 프로비저닝 자동화\n우리는 이미 IaC에 대해 알아본 적이 있는데요. 지난 글에서 더 자세한 내용을 확인해보세요.\n\n\n일관성 및 확장성\n\n필요 시 블록체인 네트워크가 일관성있게 재구축 가능하며, 네트워크의 규모도 일관성 있게 변경 가능\n\n\n\n보안\n\n보안 취약점 검사\n\n스마트 컨트랙트나 블록체인 애플리케이션을 배포하기 전 코드에 대한 보안 취약점을 검사하여 보안 사고를 사전에 예방\n\n\n네트워크 공격 감지\n\n블록체인 네트워크를 실시간으로 모니터링하고 의심되는 행위 발견 시 관리자 또는 개발자에게 알림을 보내는 시스템 구축\n\n\n\n모니터링 및 로깅\n\n네트워크 모니터링\n\n블록체인 네트워크의 상태와 퍼포먼스를 모니터링할 수 있는 시스템 구축\n\n\n트랜잭션 모니터링\n\n트랙잭션 로그를 수집하고 추적 가능한 시스템 구축\n\n\n\n✅블록체인 개발에 DevOps를 적용 사례들\n이미 여러 조직에서 블록체인 개발 프로세스에 DevOps를 적용하고 있는데요. 아래와 같이 사례를 간단히 정리해봤습니다.\n\nIBM 블록체인 플랫폼\nIBM은 블록체인 플랫폼을 Kubernetes 클러스터에서 이용할 수 있도록 별도의 Kubernetes Operator를 제공하고 있습니다.\nKubernets 클러스터 위에 블록체인 네트워크 운영에 필요한 컴포넌트를 배포하고 관리할 수 있기 때문에 확장성과 회복성을 제고할 수 있죠.\n\nEthereum Development\nEthereum의 패키지 중 Kurtosis는 Docker나 Kubernetes를 활용해서 프라이빗 Ethereum 테스트넷을 구성할 수 있습니다.\n블록체인 애플리케이션 테스트를 위한 테스트넷을 Kubernetes로 구성함으로써, 필요할 때마다 동일한 테스트 환경을 재구축하거나 설정 변경도 용이합니다.\nReferences\n\ndexoc.com/blog/how-devops-enhances-blockchain-development\nwww.ibm.com/docs/en/blockchain-platform/2.5.4\ngithub.com/ethpandaops/ethereum-package\n"},"blog/실시간-데이터-스트리밍의-강자,-Amazon-Kinesis-서비스-4종-비교":{"title":"실시간 데이터 스트리밍의 강자, Amazon Kinesis 서비스 4종 비교","links":[],"tags":["AWS"],"content":"Amazon Kinesis 제품군은 실시간 데이터 스트리밍 서비스의 대표 주자로, 대규모 데이터 흐름을 실시간으로 처리하고 분석할 수 있는 강력한 서비스들의 집합입니다.\n이번 글에서는 Amazon Kinesis 제품군 서비스 4종의 특징과 사용 사례를 살펴보고 비교해보겠습니다.\n왜 Amazon Kinesis인가?\n실시간 데이터 처리는 점점 더 많은 기업에서 필수 요건이 되고 있습니다. 애플리케이션 로그, 금융 거래, 영상 스트리밍 등 대규모 데이터 흐름을 관리하고 분석하기 위해서는 강력한 데이터 스트리밍 솔루션이 필요하죠.\nAmazon Kinesis 제품군은 실시간 데이터 스트리밍의 핵심 솔루션으로 자리 잡고 있는데요. 그 이유는…\n\n확장성: 데이터 처리량에 따라 수평적으로 확장 가능하며, 다양한 크기의 워크로드를 지원하고,\n다양한 통합 옵션: AWS 생태계 내의 다른 서비스와 매끄럽게 연동되어 엔드투엔드 데이터 파이프라인 구축이 용이하며,\n실시간 분석: 데이터를 수집하는 즉시 처리 및 분석할 수 있어 빠른 의사 결정을 지원하기 때문입니다.\n\n\n데이터의 중요성이 어느때보다 부각되는 요즘, DevOps 업계에서 Amazon Kinesis 제품군의 존재감은 나날이 커지고 있는 것입니다.\nAmazon Kinesis에는 아래와 같이 총 4종의 서비스가 존재하는데요.\n\nKinesis Data Streams\nKinesis Data Firehose\nKinesis Video Streams\nKinesis Data Analystics\n\n종류도 많고 이름도 비슷해서 처음엔 헷갈리실 수 있지만, 이번 글을 같이 따라오시다보면 어느새 명확히 구분하게 되실 겁니다.\nKinesis Data Streams: 실시간 데이터 파이프라인의 중심\n\n(출처: AWS 공식 문서)\nKinesis Data Streams는 데이터를 실시간으로 수집, 저장, 처리하는 데 최적화된 서비스입니다.\n데이터 스트림이란 데이터의 흐름을 뜻합니다. 그래서 Kinesis Data Streams는 데이터가 들어왔다가 특정 작업에 의해 처리될 수는 있으나, 데이터 스트림 내에 데이터가 계속 저장되는 것은 아닙니다.\nKinesis Data Streams로 들어온 데이터를 가져와 처리 작업을 수행할 애플리케이션을 생성할 수도 있는데요. 이를 Kinesis Data Streams 애플리케이션이라고 합니다.\nKinesis Data Streams의 특징과 사용 사례는 아래와 같습니다.\n특징:\n\n실시간 처리: 데이터를 1초 미만으로 처리하여 지연 없이 사용할 수 있습니다.\n데이터 보존: 상황에 따라 스트림을 확장하거나 축소하여 데이터의 유실을 방지하고 분석에 활용할 수 있습니다.\n동시 작업: 여러 Kinesis Data Streams 애플리케이션이 하나의 스트림으로부터 데이터를 가져와 각기 다른 작업을 동시에 수행할 수 있습니다.\n\n사용 사례:\n\n실시간 거래 분석: 금융 서비스에서 고객의 거래 데이터를 실시간으로 모니터링하고 이상 거래를 탐지\nIoT 센서 데이터 처리: 제조업에서 IoT 장치로부터 수집된 데이터를 처리하여 실시간 운영 최적화\n로그 및 클릭스트림 분석: 웹사이트와 애플리케이션의 사용자 활동을 분석하여 사용자 경험 개선\n\nKinesis Data Firehose: 데이터를 원하는 곳으로 전달하는 배출구\n\n(출처: AWS 공식 문서)\nKinesis Data Firehose는 데이터를 실시간으로 수집하여 S3이나 Elasticsearch, 혹은 원하는 HTTP 엔드포인트로 전송할 수 있는 완전 관리형 서비스입니다. 데이터의 전송과 변환을 간편하게 설정할 수 있어 운영 부담을 줄여주는 효과가 있는데요.\n그럼 이제 Kinesis Data Firehose의 특징과 사용 사례를 살펴보겠습니다.\n특징:\n\n데이터 변환: AWS Lambda를 활용하여 데이터를 전송 전에 포맷 변환 및 필터링이 가능합니다.\n목적지 통합: 데이터 웨어하우스, 데이터 레이크, 검색 및 분석 도구 등 다양한 목적지로 쉽게 데이터를 전송할 수 있습니다.\n\n사용 사례:\n\n실시간 데이터 웨어하우스 저장: 금융 서비스나 리테일 분야에서 실시간으로 데이터를 S3와 Redshift에 저장 및 분석\n데이터 시각화 도구와의 연동: Elasticsearch와 Kibana를 사용해 실시간 데이터 시각화\n클라우드 기반 로그 분석: 애플리케이션 로그 데이터를 Splunk로 전송하여 실시간으로 문제를 진단하고 해결\n\n그렇다면 Kinesis Data Firehose는 Kinesis Data Streams와 어떤 차이점이 있을까요?\n두 서비스를 직관적으로 비교하기 위해 아래 표로 정리해봤습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKinesis Data FirehoseKinesis Data Streams사용 목적데이터를 특정 목적지(S3, Redshift 등)로 전송실시간으로 데이터를 세밀히 처리 및 스트리밍데이터 변환 지원AWS Lambda를 활용한 데이터 변환 및 필터링 지원기본 제공되지 않으며 추가 개발 필요주요 사용 사례데이터 웨어하우스 저장, 시각화 도구 연동, 클라우드 로그 분석실시간 거래 분석, IoT 센서 데이터 처리, 클릭스트림 분석\nKinesis Data Firehose는 데이터 전송의 간편함과 목적지와의 통합에 초점이 맞춰져 있는 반면, Kinesis Data Streams는 세밀한 데이터 처리와 유연한 스트리밍 파이프라인 구성에 강점이 있습니다.\nKinesis Video Streams: 라이브 영상도 실시간으로 전송\n\n(출처: AWS 공식 문서)\nKinesis Video Streams는 IoT 디바이스, 스마트폰 카메라, 드론 등에서 캡처한 라이브 영상을 AWS 클라우드로 전송하거나, 실시간 영상 처리 애플리케이션을 구축하는 데에 활용할 수 있는 서비스입니다.\n스트림으로 들어온 영상은 실시간 시청도 가능한데요. AWS 관리 콘솔을 이용해서 볼 수도 있고, Kinesis Video Streams API를 활용해서 직접 모니터링 애플리케이션을 개발할 수도 있습니다.\nKinesis Video Streams의 특징과 사용 사례는 아래와 같습니다.\n특징:\n\n실시간 영상 처리: 낮은 대기 시간으로 영상 데이터를 전송하며, 다양한 형식의 스트리밍을 지원합니다.\nAWS AI 서비스와의 통합: Amazon Rekognition과 연동하면 영상 데이터에서 객체 감지, 얼굴 인식 등의 분석이 가능합니다.\n데이터 보존 및 재생: 저장된 영상을 클라우드에서 다시 재생하거나 다른 서비스와 통합하여 활용 가능합니다.\n\n사용 사례:\n\n스마트 홈 보안 시스템: 보안 카메라에서 실시간으로 영상을 전송하고 분석하여 침입 탐지\n실시간 의료 영상 분석: 원격 진료에서 영상 데이터를 실시간으로 전송하여 의료진이 즉각적으로 분석\n드론 영상 스트리밍: 드론이 캡처한 영상을 실시간으로 전송하여 농업, 구조 작업 등에 활용\n\nKinesis Video Streams와 Kinesis Data Streams 모두 스트림과 관련된 서비스인데요. 이 둘의 차이점을 표로 비교하면 아래와 같습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKinesis Video StreamsKinesis Data Streams사용 목적영상 데이터의 스트리밍, 저장 및 분석이벤트 데이터, 로그 데이터를 실시간으로 처리데이터 형식 지원영상 및 오디오 데이터텍스트 기반 로그, 이벤트 데이터주요 사용 사례스마트 홈 보안, 원격 의료 영상 분석, 드론 영상 스트리밍실시간 거래 분석, IoT 센서 데이터 처리, 클릭스트림 분석\n표를 보면 알 수 있듯이, Kinesis Video Streams는 영상 처리와 AI 기반 분석에 적합한 반면, Kinesis Data Streams는 텍스트 기반의 데이터 처리와 실시간 이벤트 관리에 최적화되어 있습니다.\nKinesis Data Analytics: 실시간 데이터에서 인사이트 도출\n\n(출처: AWS 공식 문서)\n마지막으로 Kinesis Data Analytics는 스트리밍 데이터를 SQL 기반으로 실시간 분석할 수 있는 서버리스 서비스인데요. 복잡한 데이터 분석 워크로드를 간소화하며, 분석 결과를 즉각적으로 활용할 수 있습니다.\nKinesis Data Analytics의 특징과 사용 사례는 아래와 같이 정리할 수 있습니다.\n특징:\n\nSQL 기반 데이터 처리: 기존 SQL 기술을 사용해 복잡한 분석 작업을 쉽게 수행할 수 있습니다.\n실시간 데이터 처리: 스트리밍 데이터를 실시간으로 분석하여 빠르게 인사이트를 확보할 수 있습니다.\n자동 스케일링: 데이터 처리량과 쿼리 복잡성에 따라 자동으로 확장 및 축소됩니다.\n\n사용 사례:\n\n실시간 마케팅 캠페인: 고객 행동 데이터를 분석하여 맞춤형 프로모션 제공\nIoT 데이터 모니터링: 산업 IoT 센서에서 들어오는 데이터를 분석하여 장비 상태를 실시간으로 확인\n이상 탐지 및 알림: 금융 거래 데이터를 분석하여 실시간으로 이상 거래 탐지 및 알림\n\n마치며\n지금까지 Amazon Kinesis 서비스 4종을 알아봤는데요. 이들 모두 각기 다른 요구 사항에 맞춰 강력한 솔루션을 제공하고 있습니다.\n데이터의 흐름을 관리하고 분석하는 작업이 점점 중요해지는 시대에서, 유연하고 강력한 기능을 제공하면서 다양한 데이터 관련 작업을 지원하는 Amazon Kinesis 제품군은 앞으로도 더욱 주목받을 것으로 예상됩니다.\nReferences\n\naws.amazon.com/kinesis/\ndocs.aws.amazon.com/streams/latest/dev/introduction.html\ndocs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\ndocs.aws.amazon.com/en_us/kinesisvideostreams/latest/dg/what-is-kinesis-video.html\ndocs.aws.amazon.com/en_us/kinesisanalytics/latest/dev/what-is.html\n"},"blog/컨테이너-런타임-보안":{"title":"컨테이너 런타임 보안 알아보기","links":[],"tags":["Security"],"content":"\nContainer Runtime Security(컨테이너 런타임 보안)이란 컨테이너화된 애플리케이션이 동작 중일 때 보안성을 높이기 위한 활동들을 의미합니다.\n컨테이너 런타임 보안이 중요한 이유\n컨테이너 내부의 애플리케이션은 실제로 동작하는 개체이면서, 대부분 데이터를 처리하는 역할을 수행하기 때문에 보안 공격 대상이 되기 쉽습니다.\n게다가 컨테이너는 호스트의 커널을 공유하기 때문에, 공격자가 컨테이너 내부 애플리케이션에 무단 침입 후 컨테이너 외부로 빠져나오게 된다면 해당 호스트나 다른 컨테이너에도 접근하게 되므로 심각한 보안 사고로 이어질 수 있습니다.\n그렇기 때문에 동작 중인 컨테이너에 대한 보안은 중요하다고 할 수 있겠습니다.\n컨테이너 런타임 보안 활동이 이뤄지는 방식\n이러한 컨테이너 런타임 보안 활동에는 컨테이너에 대한 실시간 모니터링 및 보호 활동 등이 포함됩니다.\n컨테이너가 실행되면서 발생하는 Linux 커널의 System Call을 감시하는 것이 일반적이라고 할 수 있겠습니다.\n이렇게 감시를 진행하다가 프로세스의 이상 행위나 잠재적인 보안 유해 행위가 감지된다면 이를 사용자 또는 관리자에게 알리는데, 더 나아가 미리 정의된 규칙에 따라 이러한 행위를 제한하는 경우도 있습니다.\n컨테이너 런타임 보안을 위해 개발된 툴\n컨테이너 런타임 보안 활동을 위해 개발된 툴에는 여러가지가 있습니다. 그 중 대표적인 3가지를 간략히 소개해드리겠습니다.\n\nFalco\n\nLinux 커널 및 Kubernetes API Call을 통해 노드 및 컨테이너 내 이상 행위를 감지할 수 있는 툴 (관련 글)\n\n\nAqua Security\n\n컨테이너의 런타임 보호 기능을 제공하며, 보안 제어 자동화를 도와줌\n\n\nSysdig Secure\n\n런타임 보안, 포렌식, 취약점 관리 기능 등을 제공함\n\n\n\nReferences\n\nwww.wiz.io/academy/container-runtime-security\nIntroduction: what is container runtime security?\n"},"blog/컨테이너-보안-스캐닝":{"title":"컨테이너 보안 스캐닝 알아보기","links":[],"tags":["Security"],"content":"컨테이너 보안 스캐닝(Container Security Scanning)이란 컨테이너 이미지에 존재할 수 있는 보안 취약점이나 이슈를 컨테이너 스캐닝 툴로 검사하고 분석하는 행위를 말합니다.\n\n컨테이너 스캐닝이 필요한 이유\n현재 운영 중인 서비스 대부분이 컨테이너 이미지 기반으로 동작하기 때문에 컨테이너 이미지에 보안 취약점 또는 이슈가 있다면 엄청난 보안 사고 및 재정적 피해로 이어질 수 있습니다.\n컨테이너 보안 스캐닝을 수행하면 개발자가 해당 컨테이너에 어떤 취약점이 존재하는지 인지할 수 있고 사전에 조치를 취할 수 있게 됩니다.\n또한, 실제 서비스에 대해서 주기적으로 컨테이너 보안 스캐닝을 수행할 경우, 최종 사용자에게 해당 서비스가 계속 모니터링되어 안전하다는 신뢰감도 줄 수 있습니다.\n컨테이너 스캐닝이 수행되는 방식\n컨테이너 스캐닝 툴은 보통 원격 레지스트리 또는 로컬에 저장된 컨테이너 이미지를 대상으로 스캔합니다.\n이때 스캐닝 툴은 스캔을 위해 해당 이미지를 레이어(Layer) 단위로 해체하는데, Base 이미지와 애플리케이션 코드, 각 디펜던시 역시 해체 대상에 포함됩니다.\n스캐닝 툴의 스캔 방식은 아래와 같이 크게 2가지로 나뉩니다.\n\n지식 기반(Signature-Based) 스캔: 기존에 알려진 취약점(CVE 등)을 기반으로 스캔 수행\n행동 기반(Behavioral-Based) 스캔: 컨테이너 실행 시점에 비정상적인 프로세스나 네트워크 트래픽 같은 이상 행동에 대해 스캔 수행\n\n컨테이너 보안 스캐닝 툴\n\nTrivy: 컨테이너 이미지, 파일 시스템, Kubernetes 클러스터 등 다양한 대상에 대해 보안 이슈와 취약점 스캔 가능합니다. CLI로 스캔 작업을 실행합니다. (관련 문서)\nClair: 함께 설치되는 DB에 스캔 대상 컨테이너 이미지에 대한 정보와 레이어를 저장 후 보안 취약점을 스캔하는 것이 특징입니다. CLI로 스캔 작업을 실행하며, Webhook을 통한 알림 기능도 지원합니다. (관련 문서)\n\nReferences\n\nwww.wiz.io/academy/container-security-scanning\nsnyk.io/learn/container-security/container-scanning\n"},"blog/컨테이너-엔진-관련-취약점-(CVE-2024-21626)":{"title":"컨테이너 엔진 관련 취약점: CVE-2024-21626","links":[],"tags":["CVE","Container"],"content":"2024년 1월 말, 컨테이너 엔진으로 널리 쓰이는 runc에서 취약점(CVE-2024-21626)이 발견되었습니다. runc는 Docker와 Kubernetes 등이 컨테이너 이미지를 빌드하거나 실행할 때 사용하는 컨테이너 엔진입니다.\n\nCVE-2024-21626 취약점이란\n\n공격자가 컨테이너에서 탈출(Container Escape)하여 호스트 시스템에 비허가 접근 위험이 있는 취약점(CVE-2024-21626)이 runc에서 발견되었습니다.\n해당 취약점은 runc가 컨테이너 이미지를 생성하거나 실행하는 특정 시점에 호스트 내 파일레 접근 가능한 통로가 여전히 남아있기 때문에 발생한 것입니다.\n악성 컨테이너 이미지를 실행하거나, 악성 Dockerfile 또는 악성 Base Image로 컨테이너 이미지를 빌드할 때 해당 취약점의 영향을 받을 수 있습니다.\n\nCVE-2024-21626이 위험한 이유\n\n공격자가 컨테이너에서 벗어나 호스트 시스템에 저장되어있던 모든 데이터(계정 정보, 사용자 정보 등)를 획득 가능합니다.\n또한 공격자가 컨테이너에서 벗어난 뒤 호스트 시스템에서 추가 공격이 가능합니다.\n컨테이너 엔진인 runc에 영향을 주는 취약점이므로, runc를 기반으로 작동하면서 널리 사용되는 컨테이너 툴 Docker, Kubernetes도 영향을 받게 되어 파급력이 큽니다.\n\nCVE-2024-21626에 대한 대처 방법\n\n\n사용 중인 컨테이너 툴 버전을 해당 취약점에 대해 보완된 버전으로 최신 업데이트합니다.\n\nrunc &gt;= 1.1.12\ncontainerd &gt;= 1.6.28\nDocker Desktop &gt;= 4.27.1\n\n\n\n현재 사용 중인 컨테이너 툴을 바로 업데이트하기 어렵다면…\n\n신뢰할 수 있는 컨테이너 이미지만 사용합니다.\n컨테이너 이미지 빌드 시, 신뢰할 수 있는 Dockerfile 또는 신뢰할 수 있는 Base Image만 사용합니다.\n\n\n"},"blog/컨테이너-이미지-보안을-위한-Dockerfile-작성-팁-5가지":{"title":"컨테이너 이미지 보안을 위한 Dockerfile 작성 팁 5가지","links":["blog/컨테이너-보안-스캐닝"],"tags":["Docker"],"content":"\nDocker 이미지는 애플리케이션이 동작할 수 있는 독립된 환경을 제공해서 이미 많은 개발자가 사용하고 있는데요.\n이미 누군가 만들어둔 Docker 이미지를 사용하기도 하지만, 직접 Dockerfile을 작성해서 Docker 이미지를 빌드하는 경우도 많죠.\n더욱 안전한 Docker 이미지를 제작할 수 있는 Dockerfile 작성 팁이 있다는 사실, 알고 계셨나요?\n⚠️Dockerfile을 작성할 때 보안에 신경써야 하는 이유\n오늘날 대부분의 웹 서비스는 마이크로서비스 아키텍처를 따르고 있습니다. 하나의 커다란 서비스를 배포하는 것이 아닌, 기능이나 성격에 따라 나눠진 작은 서비스 여러 개를 배포 및 운영하는 방식을 마이크로서비스 아키텍처라고 하는데요.\n이때 마이크로서비스를 각각의 Docker 컨테이너 내부에서 동작하도록 구성하는 것이 일반적입니다. 하지만 이렇게 실제로 배포된 컨테이너 이미지가 외부 공격에 취약하다면… 생각만 해도 아찔한 보안 사고로 이어지겠죠.\n그래서 우리는 ==Dockerfile==을 작성할 때부터 보안에 신경써야 합니다.\n🔒더욱 안전한 이미지를 만드는 Dockerfile 작성 팁 5가지\n그렇다면 Dockerfile을 어떻게 작성하면 좋을까요? 여기 보안에 강한 Dockerfile 작성 팁 5가지를 소개해드립니다.\nMulti-Stage 방식으로 빌드\n\nDockerfile 내에서 애플리케이션 빌드 명령어 실행 후 나오는 최종 산출물을 또다른 Base Image로 복사하고, 해당 Base Image를 최종 Docker Image로 빌드하는 기법을 Multi-Stage라고 하는데요.\nMulti-Stage 방식으로 빌드된 Docker 이미지 내에는 애플리케이션 구동에 필요한 최소한의 요소만 담겨있으므로, 자연스럽게 잠재된 보안 취약점도 더 적어집니다.\n\n필요없는 패키지 제거\n\nDocker 컨테이너 내 애플리케이션 구동에 필요없는 패키지를 제거하는 것 역시 잠재 보안 취약점을 줄여주기 때문에 권장됩니다.\n\nRoot가 아닌 별도의 사용자를 생성 및 사용\n\nContainer 내에 Root 계정이 사용될 경우, 침입 사고 발생 시 피해가 커질 수 있습니다. 그래서 권한이 제한된 별도의 사용자를 생성하고 사용하는 것이 안전합니다.\n\nContainer 내 민감한 파일들은 Read Only로\n\n배포 이후 Docker Container 내에서 추후 수정이 필요하지 않는 파일들을 Read Only로 설정하면 보안성을 높일 수 있습니다.\n\nShell 접근 제거\n\n컨테이너 안에서 실행 가능한 sh나 bash 등의 Shell은 동작 중인 컨테이너 내 애플리케이션을 디버깅할 때 사용되는 경우가 있지만, 이런 통로를 제거하면 컨테이너 내부 침입이 어려워져 보안성이 높아집니다.\n하지만 Shell 접근을 제거할 땐 아래 주의사항에 대해 고려해야 합니다.\n\n컨테이너 이미지에서 동작하는 ==애플리케이션이 Shell을 사용하고 있지 않은지 확인==해야 합니다.\n==해당 이미지를 배포한 뒤에 애플리케이션 디버깅할 수 있는 다른 대안을 미리 마련==해야 합니다.\n\n\n\n🔎컨테이너 보안을 지킬 수 있는 또다른 방법\n지금까지 Dockerfile을 작성할 때 보안성을 높이기 위한 작성 팁에 대해 알아봤는데요. Docker 이미지를 직접 빌드하는 경우가 아닌, 이미 누군가 제작한 Docker 이미지를 사용할 때 그 이미지가 안전한지는 어떻게 알 수 있을까요?\n이럴 때 사용할 수 있는 방법이 바로 컨테이너 보안 스캐닝입니다. 대표적인 컨테이너 보안 스캐닝 툴로는 Trivy와 Clair가 있는데요.\n컨테이너 이미지 보안 스캐닝에 대해서는 지난 글에서 다룬 적이 있기 때문에 혹시 관심 있으시다면 확인해보시기 바랍니다.😊\nReferences\n\n# Best practices for writing Dockerfiles\n"},"blog/컴팩트하게-이해하고-바로-구현하는-Kubernetes-ELK-로그-모니터링-시스템-강의-출시":{"title":"[강의 출시 안내] 컴팩트하게 이해하고 바로 구현하는 Kubernetes ELK 로그 모니터링 시스템","links":[],"tags":["Monitoring","Course"],"content":"안녕하세요, 제가 그동안 준비하고 제작한 Kubernetes ELK 로그 모니터링 시스템 구축 강의를 드디어 🌱인프런에 출시했습니다!\n강의 이름은 컴팩트하게 이해하고 바로 구현하는 Kubernetes ELK 로그 모니터링 시스템입니다.\n\n강의 대상 및 예상 효과\n본 강의는 아래와 같은 분들을 대상으로 제작했습니다.\n\nKubernetes에 대해 학습 중이면서 실습 경험을 쌓고 싶으신 분\nKubernetes에서 ELK 로그 모니터링 시스템을 구축해보고 싶으신 분\n\n그리고 이번 강의를 수강하신다면…\n\nELK 스택을 직접 배포하고 Log Monitoring 시스템을 구축하여 실질적인 Kubernetes 사용 경험을 얻게 됩니다.\nKubernetes infrastructure에 대한 기본적인 이론 수업과 ELK 스택의 각 구성 요소를 Kubernetes 기본 리소스만으로 배포하여 Kubernetes를 더욱 깊게 이해할 수 있습니다.\nELK 스택의 각 구성 요소에 대한 기본적인 지식을 습득하게 됩니다.\n실습을 통해 기본적인 Logstash의 데이터 가공 경험과 Kibana의 데이터 시각화 경험을 얻게 됩니다.\n\n본 강의는 이론 파트와 실습 파트로 나눠지는데요.\n이론 파트에서는 ELK 모니터링 시스템 구축 및 기본 활용에 필요한 핵심 정보를 알려드립니다. 쉬운 이해를 위해 모든 이론 수업은 아래와 같이 도식으로 진행됩니다.\n\n실습 파트에선 ELK 스택을 Kubernetes에 올리기 위한 Manifest 파일을 같이 작성하고, 각 툴을 실행 및 활용하게 되는데요.\nManifest 작성 및 ELK 스택 활용 시 Kubernetes와 ELK의 공식 문서를 함께 살펴보기 때문에, 공식 문서에 익숙치 않으시다면 그 거리감을 줄이는 데에도 도움이 될 것입니다.\n\n본 강의의 실습 파트는 Kibana 웹 대시보드에서 기본적인 데이터 시각화 실습으로 마무리됩니다. Kibana로 대시보드를 만들어 보고 싶었지만 어디서부터 시작해야 할지 감이 잘 안 오셨다면 이번 강의가 도움이 될 것입니다.\n\n이번 강의는 제가 업무 중 ELK 스택을 Kubernetes에 직접 배포해보면서 수많은 이슈를 겪고 연구한 경험이 있었기 때문에 나올 수 있었는데요.\n이 강의를 수강하시는 분들은 Kubernetes에 ELK 스택을 배포할 때 필요한 핵심 정보를 컴팩트하게 이해하고 바로 구현할 수 있으면 하는 마음입니다.\n저의 첫 강의에 많은 관심 부탁드립니다!\n보다 자세한 강의 안내 및 신청은 아래 페이지에서 확인해주세요.\n▶️컴팩트하게 이해하고 바로 구현하는 Kubernetes ELK 로그 모니터링 시스템◀️"},"blog/플랫폼-엔지니어링이란":{"title":"플랫폼 엔지니어링 소개","links":[],"tags":["PlatformEngineering"],"content":"⚙️플랫폼 엔지니어링이란?\n우리가 DevOps에 대해 검색을 하다보면 함께 떠오르는 키워드가 하나 있습니다. 바로 플랫폼 엔지니어링인데요.\n최근 저도 플랫폼 엔지니어링이란 키워드를 많이 접하게 되면서 플랫폼 엔지니어링이 구체적으로 무엇을 말하는지 궁금했고, 이번 기회에 정리를 해봤습니다.\n그럼 같이 알아보시죠!\n\n플랫폼 엔지니어링은 개발자의 생산성을 높여주는 내부 플랫폼(Internal Developer Platform, IDP)을 개발하고 유지보수하는 활동을 의미합니다. 즉, 플랫폼 엔지니어링이란 개발자를 위한 개발이라고 볼 수 있겠는데요.\n그렇다면 플랫폼 엔지니어링은 DevOps와 어떤 차이가 있을까요?\n🔎DevOps와 플랫폼 엔지니어링의 차이\n두 가지 모두 조직 내 개발자를 대상으로 하는 활동이기 때문에 그 차이점이 분명하지 않을 수 있습니다. 그래서 두 활동을 함께 비교해보겠습니다.\n\n먼저 DevOps는 소프트웨어 개발 접근 방식 중 하나로, 개발 주체과 운영 주체 간의 긴밀한 협업을 위한 것입니다.\n그리고 플랫폼 엔지니어링은 DevOps에 필요한 각종 도구와 작업 프로세스 한 곳에 모은 플랫폼을 제공하는 활동입니다.\n즉, DevOps 팀은 조직에게 적절한 개발 ,빌드, 테스트, 배포, Configuration, 자동화 도구를 도입함으로써 개발 프로세스를 정립하는 역할을 수행하고, 플랫폼 엔지니어링 팀은 이렇게 선정된 도구들을 파악, 추가 구현, 유지보수하여 조직 내 개발자들이 사용할 수 있도록 플랫폼(IDP)으로 만들어내는 역할을 수행하는 것이죠.\n이렇게 IDP를 구성하게 된다면, 조직 내 다른 개발자들은 플랫폼이나 도구 자체에 대한 세세한 이해 없이도 플랫폼이 제공하는 일관성과 생산성을 누릴 수 있게 됩니다.\n지금까지 살펴본 것처럼, 플랫폼 엔지니어링과 IDP는 실과 바늘같은 관계이기 때문에 IDP에 대해서도 그냥 넘어갈 수는 없겠습니다.\nIDP는 내부 개발자 플랫폼이라고 했었죠. 이런 IDP는 대규모 개발 팀에게만 필요하다고 생각될 수도 있지만, 개발부터 배포까지 꾸준히 이뤄지는 팀이라면 안정적인 작업 프로세스를 위해 IDP를 고민해볼만 합니다.\n사실 IDP에 대한 표준이 정립된 것은 아니기 때문에 각 조직마다 IDP에 대한 요구사항은 다를 수 있습니다. 또한 여러 개의 툴을 유기적으로 활용해서 IDP로 구성할 수도 있는 것이죠.\n조직마다 차이는 있더라도 IDP가 가져야 할 필수 요소는 아래와 같이 7가지로 정리할 수 있습니다.\n🛠️IDP의 필수 구성요소 7가지\n파이프라인\n소프트웨어 코드를 개발 중이라면 코드에 문제가 없는지 검증이 필요한데요. 이런 코드 검증을 자동으로 해주는 파이프라인이 있다면 개발자들의 생산성이 향상될 것입니다.\n파이프라인 관련 툴로는 GitHub Action, GitLab CI Runner, Jenkins 등이 있습니다.\n아티팩트 저장소\n컴파일이 완료되었거나 컨테이너화된 이미지를 저장하는 아티팩트 저장소는 조직 내 개발 프로세스와 보안을 위해서 필수적입니다. 또한 아티팩트 저장소가 있다면 문제가 발생했을 경우 신속하게 이전 버전의 이미지를 가져와서 사용할 수도 있겠죠.\n아티팩트 저장소 관련 툴로는 CRI-O, Docker, Nexus 등이 있습니다.\n런타임 관련\n애플리케이션이 배포되어 동작하는 런타임 기간 중에 보안, 네트워크 관련 이슈 등을 실시간으로 제어하는 것이 중요합니다. 런타임 보안 관련 툴로는 Falco 등이 있습니다.\n또한 애플리케이션 배포할 수 있는 오케스트레이션 툴 역시 여기에 포함되는데요. 오토스케일링이나 셀프 힐링(Self-healing), 로드 밸런싱 기능이 강력한 Kubernetes가 대표적인 툴이라고 할 수 있을 것입니다.\nAPI Gateway / Service Proxy\n두 요소 모두 애플리케이션을 외부로 서비스하기 위해 필수적인 것입니다.\n관련 툴로는 Envoy, Nginx, Traefik 등이 있습니다.\n모니터링\n소프트웨어 사이클에서 모니터링과 로그 추적은 정말 중요한 요소입니다. 애플리케이션에 발생한 문제를 해결하고 안정적인 서비스를 보장하는 데에 결정적인 역할을 하기 때문이죠.\n모니터링 관련 툴로는 ELK 스택, PLG 스택이 있습니다.\nFinOps 및 지속가능성 관련\nFinOps는 클라우드 운영에 소요되는 비용을 관리하는 활동을 의미합니다.\n그리고 여기서 말하는 지속가능성이란, 사용하지 않는 리소스의 동작을 멈춰 불필요한 에너지 소비를 줄이는 것을 말합니다.\n지속가능성과 비용 절약을 위한 툴로는 kube-green 등이 있습니다.\n데이터 관리\n애플리케이션 동작에 필요한 데이터뿐만 아니라, 서비스 운영에 사용되는 다양한 시스템으로부터 실시간으로 발생하는 데이터 역시 중요할 수 있습니다.\nReferences\n\nplatformengineering.org/blog/what-is-platform-engineering\nwww.techtarget.com/searchitoperations/tip/Platform-engineering-vs-DevOps-Whats-the-difference\nwww.cncf.io/blog/2023/04/28/7-core-components-of-an-internal-developer-platform/\n"},"index":{"title":"🔭 DevOps 여행을 위한 안내서","links":[],"tags":[],"content":"\n\n\n🧑‍💻Aiden Kim\nDevOps Engineer / CNCF Kubestronaut\n안녕하세요, DevOps와 클라우드 여정을 함께할 수 있는 DevOps 여행을 위한 안내서에 오신 것을 환영합니다.\n매주 뉴스레터 DevOps 여행을 위한 소식지를 함께 발행하고 있습니다.\n출시 강의\n📊Kubernetes ELK 로그 모니터링 시스템 구현 (인프런)\n\n컴팩트하게 이해하고 Kubernetes ELK 시스템을 바로 구축할 수 있는 강의입니다.\n\nContact\n📧Email: edu.ukulelekim@gmail.com"}}