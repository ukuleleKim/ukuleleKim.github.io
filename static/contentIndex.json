{"blog/2024-우아콘-참여-후기---1":{"title":"[2024 우아콘 후기] AI 데이터 분석가 '물어보새' 소개 및 실시간 추천 검색어 모델링 사례 공유","links":[],"tags":["Conference"],"content":"10월 30일, 우아한형제들에서 2024 우아콘을 개최했습니다. 저도 이 행사에 참여하게 되어서, 행사 참여 후기와 제가 참석한 세션 내용을 함께 공유드리려고 합니다.\n\n이번 행사는 참여형 트랙을 포함한 총 8가지 트랙으로 구성되고, 각 트랙은 시간별로 6가지 세션으로 나눠지기 때문에, 정말 다양한 주제로 발표가 이뤄졌는데요.\n시간대별로 한 가지 세션만 선택해서 참여하는 방식이었지만, 데이터와 AI, 인프라를 주제로 한 관심가는 세션들이 동시간대에 진행되는 경우가 많아서 고르기가 쉽지 않았습니다😅\n\n그렇게 고르고 골라서 참석한 세션이다보니, 발표 내용을 정리한 분량도 꽤 커서 이번 행사 참여 후기 글은 분량을 나눠서 공유하도록 하겠습니다.\n🗂️AI 데이터 분석가 ‘물어보새’ 등장: 데이터 리터러시 향상을 위한 나만의 데이터 분석가\n\n2024 우아콘에서 저는 가장 먼저, 우아한형제들의 내부 AI 기반 데이터 분석 서비스인 물어보새를 소개하는 세션에 참석했었습니다. 이 세션에서는 ‘물어보새’ 서비스를 개발하게 된 계기와, 개발 과정 중에서 마주한 고민들을 어떻게 해결해나갔는지에 대해 발표해주셔서 재밌게 들었는데요.\n‘물어보새’ 서비스는 우아한형제들의 BADA(Baemin Advanced Data Analytics)팀이 내부 구성원들의 데이터 활용 능력을 향상시키기 위해 제공하는 AI 기반의 데이터 쿼리 생성 및 분석 툴이라고 합니다.\n‘물어보새’ 서비스가 탄생했던 계기는 아래와 같은 고민들이었다고 하는데요.\n\n사용하려는 데이터가 제대로 추출된 것이 맞을까?\n원하는 데이터 생성을 위해 어떤 테이블과 조건을 사용해야 할까?\n전달받은 쿼리문을 좀 더 쉽게 이해할 수 있는 방법은 없을까?\n\nBADA팀 이런 고민들이 생겨난 원인은 구성원들의 데이터 활용 능력이 부족하기 때문이라고 파악했고, 먼저 데이터 리터러시를 아래와 같이 정의했다고 합니다.\n\n데이터 이해\n데이터 생성\n데이터 분석\n데이터 기반 의사소통\n\n그래서 BADA팀은 데이터 기반 의사결정 역량을 강화하면 구성원들의 업무 효율이 증가할 것으로 예상하고, 이를 위해 아래와 같이 ‘물어보새’의 기능을 정의했습니다.\n\n데이터 디스커버리\n\n데이터 쿼리문 해설\n\n\nText-To-SQL\n\n데이터 쿼리문 생성 및 생성된 쿼리 요약 설명\n\n\nAgentic Analytics\nKnowledge Sharing\n\n이런 배경으로 ‘물어보새’가 개발되었고, 현재 위 기능 중 데이터 디스커버리와 Text-To-SQL을 서비스 중이라고 합니다.\n그렇다면 BADA팀에선 ‘물어보새 서비스를 구현하기 전에 어떤 요소들을 고려했을까요? 총 4개의 키워드로 나눈 고려 요소는 아래와 같습니다.\n\n체계화 - 체계적인 정보 수집\n\n데이터 플랫폼, 거버넌스, 엔지니어링 지원을 통한 정보 관리\n\n\n효율화 - 사내 정보를 쉽고 빠르게 검색\n\n사내 정보 검색 기술을 개발\n\n\n자동화 - 365일 24시간 상시 응답\n\n데이터 담당자 없이도 언제든 이용 가능하도록 지원\n\n\n접근성 - 익숙한 채널로 소통 연결\n\n업무 주 도구를 적극 활용\n\n\n\n\n(우형 기술 블로그에서도 확인 가능한 ‘물어보새’의 아키텍처입니다.)\nBADA팀은 ‘물어보새’가 개발되는 과정에서도 여러 고민들을 마주했고, 또 해결했다고 하는데요.\n이번 세션에서 공유해주신 3가지 고민과 그 해결방식을 정리하면 아래와 같습니다.\n고민 1.  우아한형제들에서 어떤 데이터를 어떻게 활용하고 있는지 LLM에게 잘 알려줄 수 있는 방법은?\n해결방식:\n\nRAG를 위해 참고할 수 있는 여러 Vector DB 구축\n\n테이블 DDL: 테이블명과 컬럼명, 컬럼 설명, 컬럼별 데이터 타입과 값 정보\n\nLLM이 테이블 구조에 대해 이해할 수 있음\n\n\n테이블 Meta: 테이블별 주요 사용 목적과 관련 서비스, 주요 키워드, 관련 질문 등\n\n질문에 답변하기 위해 필요한 테이블을 더 잘 검색할 수 있음\n\n\n비즈니스 용어 사전: 우형에서 쓰는 용어에 대한 설명\n\nLLM이 우형만의 용어를 잘 이해할 수 있게 됨\n\n\nSQL Few Shot: 우형에서 쓰는 비즈니스 로직이 반영된 쿼리문 예시\n\n\n\n고민 2. LLM이 다양하고 복잡한 질문들에 대해 더 똑똑하게 답변할 수 있는 방법은?\n해결방식:\n\nRouter Superviser로 질문 분류\n\n데이터 관련 질문인지 여부\n데이터 질문 분류\n\n질문 유형 분류\n\n\n\n\n프롬프트 최적화\n\nDynamic Prompting\n성능 평가 지표 활용\n\n\n\n고민 3. 이 서비스를 편리하게 제공할 방법은?\n해결방식:\n\n접근성\n\n가장 많이 사용되는 채털인 Slack을 통해 서비스 제공\n\n\n신속성\n\nGPT Cache DB를 활용한 빠른 답변 제공\n\n\n안정성\n\nAzure OpenAI Multi Region Load Balancing 구현\nGrafana를 활용한 서비스 실시간 모니터링\n\n\n고도화\n\n사용자 피드백 버튼 기반의 의견 수집\nLLMOps 기반 서비스 지속 고도화\n\n\n\n마지막으로, ‘물어보새’ 향후 방향성에 대해서도 아래와 같이 공유해주셨는데요.\nAgentic Analytics\n\nChain 기반에서 Graph 기반으로 변경\n데이터 분석 서비스 기능 제공\n\nKnowledge Sharing\n\n사내 모든 데이터로 지식 범위를 확장\n데이터 의사소통 전문가로서의 역할로 확장\n\n’물어보새’가 현재는 업무 지원 단계에 있지만, 최종적으로는 ‘물어보새’가 의사결정을 담당하는 업무 자율화 단계로 거듭나고자 한다는 내용을 마지막으로 이번 세션은 마무리되었습니다.\n개발한 서비스를 유지보수하는 것을 넘어서, 더 나은 방법론을 채택하고 더 넓은 영역으로 서비스를 확장하려는 우아한형제들의 고민이 생생하게 느껴지는 세션이었습니다.\n💻그래프, 텍스트 인코더를 활용한 실시간 추천 검색어 모델링\n\n제가 두 번째로 참여한 세션은, ‘배달의민족’ 앱의 추천 검색어 기능에 사용된 모델 적용 기법을 소개하는 자리였습니다.\n‘배달의민족’의 여러 핵심 페이지를 소개하는 내용을 시작으로, 추천 검색어 기능을 도입하게된 계기인 퀵 커머스(Quick Commerce)에 대해 공유해주셨는데요.\n퀵 커머스란 1시간 이내에 원하는 장소에서 원하는 상품을 배송받는 서비스를 의미하며, 퀵 커머스의 특징은 아래와 같이 정리할 수 있습니다.\n\nExplore - 사용자는 여러 개의 상품을 둘러보는 움직임을 보임\nReal-time - 사용자의 관심사는 실시간으로 변함\nSearch - 사용자는 원하는 상품에 접근하기 위해 검색에 의존\n\n이런 배경에서 ‘만약 원하는 검색어를 타이핑 없이 클릭만으로 검색 가능하다면 편리하지 않을까?‘란 생각을 시작으로 추천 검색어 기능이 도입되었다고 합니다.\n추천 검색어 기능에 대한 소개 다음으로, 추천 검색어에 사용된 2가지 모델 적용 기법인 Batch와 Real-Time에 대해 공유해주셨는데요. 각 모델링 기법을 정리하면 아래와 같습니다.\nBatch\n\n데이터: 장기(Long-term) 히스토리\n추론: Batch Inference\n모델 사용 시점: 앱 최초 진입 시, 과거 클릭 데이터 활용 (24시간 동안 동일한 추론 결과 제공)\n프로세스\n\n데이터 불러오기\n차원 축소 (NMF)\n분류 (Multi-Layer Perceptron)\n추론 (Chunk로 나눈 다음 추론)\n후처리 작업\n\n\n\nReal-Time\n\n데이터: 단기(Short-term) 피드백\n추론: Real-time inference\n모델 사용 시점: 앱 진입 이후, 상품 클릭 및 검색 데이터를 실시간으로 활용\n프로세스\n\n인코더를 사용하여 사용자의 피드백을 임베딩 벡터로 변환\n시퀀스 모델이 임베딩 벡터를 시퀀스 임베딩으로 변환\n\n\n\n사용자의 앱 사용 시점별로 적절한 모델을 사용하여 추천 검색어 기능을 구현한 덕에, Batch 모델로 인한 CTR(Click-Through Rate, 클릭률) 개선율은 64%, Real-Time 모델로 인한 CTR 개선율은 134%를 기록했다고 하는데요. 각 모델 적용으로 눈에 띄는 변화가 있었기 때문에 유효한 개선을 이뤄냈다고 평가하셨습니다.\n해당 세션에서는 추천 검색어 기능에 적용되는 모델링 기법에 대해 기술적인 부분을 자세히 설명해주셨는데요. 비록 제가 관련 지식이 깊지 않아 모두 이해하기는 어려웠지만, 사용자의 앱 사용 시점에 따라 다른 모델링 기법을 적용하여 성과를 낸 점이 인상깊었던 세션이었습니다.\nReferences\n\ntechblog.woowahan.com/18144/\n"},"blog/Admission-Controller-소개":{"title":"Admission Controller 소개","links":[],"tags":["Kubernetes","Policy"],"content":"Admission Controller는 k8s 클러스터에서 관리자가 정의한 정책(Policy)을 수행(Admission Control)하는 플러그인입니다.\n\nAdmission Controller를 사용하는 이유\n\n보안성 측면: 인증 및 인가를 거친 요청에 대해 미리 정의한 정책을 적용하므로 보안성을 높일 수 있습니다.\n제어성 측면: 클러스터에 배포하는 App에 적절한 Label을 자동으로 기입하거나 App이 사용할 Memory 또는 CPU의 규모를 자동으로 제한하여, 클러스터 내 배포하는 리소스를 더욱 효과적으로 제어할 수도 있습니다.\n\nAdmission Controller 작동 방식\n위의 표처럼, k8s API 서버로 온 요청은 인증(Authentication)과 인가(Authorization) 과정을 먼저 거친 다음 해당 요청에 대한 Admission Control이 수행됩니다.\nAdmission Control은 아래와 같이 2단계로 나눠 진행됩니다.\n\n변경(Mutate) 단계: 들어온 요청과 매칭되는 정책이 해당 요청의 일부 값을 변경하는 Mutate 정책일 경우, 해당 요청에서 변경되어야 하는 부분을 알려주는 값을 k8s API 서버로 반환합니다.\n검증(Validate) 단계: 들어온 요청과 매칭되는 정책이 해당 요청을 허용/거부하는 Validate 정책일 경우, 해당 요청이 정책에 부합하는지에 대해 true/false 값을 k8s API 서버로 반환합니다.\n\nAdmission Controller를 사용하는 방법\nk8s 클러스터에는 Pod에 사용하는 Image나 인증서 등에 관한 다양한 Admission Controller가 기본적으로 준비되어 있습니다. (관련 링크) Admission Controller를 사용하려면 k8s API 서버의 config에 아래 내용을 추가합니다.\n--enable-admission-plugins={추가하려는 Plugin 이름}\n\nAdmission Controller를 직접 구현하는 방법도 있습니다. k8s API 서버로부터 오는 요청을 받아서 특정 정책에 따른 로직을 수행 후 Admission Control 결과 양식에 따라 결과값을 반환하는 서비스를 구현하는 방식인데요.\n이때 Admission Webhook이 사용되며, 이는 외부에서 작동하는 Admission Controller가 Admission Control 요청을 받고 이에 따른 특정 행위를 수행하도록 도와주는 역할을 합니다. 서드파티로 개발된 Kyverno와 같은 정책 엔진도 Admission Webhook을 사용합니다.\nReferences\n\nAdmission Controllers Reference\nA Guide to Kubernetes Admission Controllers\n"},"blog/CIS-Kubernetes-Benchmark":{"title":"CIS Kubernetes Benchmark","links":[],"tags":["Security"],"content":"\nCIS Benchmark란, 사이버 보안 향상을 위해 활동하는 국제 조직 Center of Internet Security에서 제안하는 IT 인프라 보안 관련 우수 사례 및 가이드라인을 의미합니다. 현재 다양한 종류의 CIS Benchmark가 존재하며, 이 중 Kubernetes와 관련된 것이 CIS Kubernetes Benchmark입니다.\nCIS Kubernetes Benchmark의 목적\n\nKubernetes에 적합한 보안 가이드라인을 기업과 조직에게 제공하여, 현재 알려져있는 Kubernetes 관련 취약점과 부적절한 설정(Misconfiguration)으로부터 안전한 Kubernetes 환경을 구축하는 것입니다.\n기업 내 엔지이너의 입장에선 일정한 툴을 사용하는 것만으로 Kubernetes 클러스터가 CIS Benchmark에서 권장하는 기준에 부합하는지 확인하고 적절한 조치를 취할 수 있게 됩니다.\n\nCIS Kubernetes Benchmark에서 다루는 범위\n\nKubernetes 클러스터 설정\nWorker Node 설정\nNetwork Policy\nRole-Based Access Control (RBAC)\nSecret 관리\nLogging 및 Monitoring\nPod 보안 정책\n컨테이너 보안\n\nCIS Kubernetes Benchmark의 구성 요소\nBenchmark는 다수의 권장 사항으로 구성되어 있습니다. 그리고 각 사항의 검토 결과는 아래 4가지 속성을 가지고 있습니다.\n\n\nScoring\n\nScored: 권장 사항이 Scored라면, 해당 권장 사항을 준수하지 못할 경우 최종 Benchmark 점수가 감소합니다.\nNot Scored: 권장 사항이 Not Scored라면, 해당 권장 사항을 준수하지 못해도 최종 Benchmark 점수는 감소하지 않습니다.\n\n\n\nLevels\n\nLevel 1: 명확한 보안 이점을 제공하거나, 기능성을 제한하지 않는 권장 사항이라는 뜻입니다.\nLevel 2: Level 1의 확장된 수준입니다. 심층적이거나 기능성에 부정적인 영향을 줄 수도 있는 권장 사항을 의미합니다.\n\n\n\nResults\n\nPass: 권장 사항을 준수했음을 의미합니다.\nFail: 권장 사항을 준수하지 못함을 의미합니다.\n\n\n\nResponsibility\n\n권장 사항을 준수해야 하는 주체를 의미합니다.\n주로 조직(Organization)으로 명시되지만, 특정 경우에는 서비스 제공자(vendor) 등이 명시되기도 합니다.\n\n\n\nCIS Kubernetes Benchmark 툴\n가장 보편적으로 쓰이는 툴은 kube-bench입니다.\n\nkube-bench는 해당 레파지토리에서 제공하는 yaml 파일을 받아온 뒤, 검토를 원하는 Kubernetes 클러스터 안에서 Job object로 실행하는 방식입니다.\n실행한 Job이 완료되면 해당 Pod의 로그로 Benchmark 검토 결과를 확인할 수 있습니다.\n\nReferences\n\nwww.armosec.io/glossary/cis-kubernetes-benchmark\nKubernetes CIS Benchmark: Why You Need It and Getting Started\n"},"blog/CKA-취득-후기":{"title":"CKA 취득 후기","links":[],"tags":["Certificate"],"content":"\n그동안 저는 Linux 재단에서 주관하는 Kubernetes 자격증을 꾸준히 취득하고 있었습니다. 그러다 마침내 저번주 KCSA 자격증 취득을 마지막으로, Linux 재단의 Kubernetes 자격증 5종을 모두 취득하게 되었는데요.\n이를 기념하고자 각 Kubernetes 자격증에 대한 소개와 저의 취득 후기를 시리즈로 구성하여 공유해보겠습니다.😃\n그 시작은 제가 가장 처음에 취득한 CKA(Certified Kubernetes Administrator)입니다.\n\nCKA 시험은 Kubernetes 관련 자격증 중 가장 많이 알려져있고 대중적입니다. CKA 자격증에 대한 정보는 아래와 같이 정리할 수 있습니다.\n\n응시료: $395\n\n코드를 입력하면 20% 또는 30% 할인해주는 쿠폰을 자주 배포하므로, 응시료 결제 전에 cka discount coupon을 찾아보는 것이 응시료를 절약할 수 있는 방법입니다.\n\n\n기본 언어: 영어\n\nLinux 재단 주관 Kubernetes 시험은 모두 기본 언어가 영어이므로, 시험 지문 역시 모두 영어인 점을 참고해야 합니다.\n\n\n시험 유형: 온라인 감독 환경에서 실습형\n응시 가능 기간: 결제 후 1년\n자격증 유효 기간: 합격 후 2년\n불합격 시 1회 재응시 기회 제공\n\nCKA에 응시한 이유\nKubernetes에 관심이 있어 공부를 하고 싶었는데, 어떻게 공부하면 좋을지 고민이 됐었습니다. 그러던 중에 무언가 남는 것이 있으면 좋겠다는 생각에 Kubernetes 자격증을 응시하기로 한 것이죠.\nKubernetes 자격증에 대해 조사해보니 CKA가 가장 많이 알려져있었고, Kubernetes 관리자(Administrator)를 위한 시험이기도 하니 Kubernetes에 대해 포괄적으로 다룰 수 있을 것 같아 응시하게 되었습니다.\nCKA 준비 방법\n가장 큰 도움을 받은 것은 Udemy의 Certified Kubernetes Administrator (CKA) with Practice Tests 강의입니다.\n\nKubernetes의 전반적인 이론과 개념을 알기 쉽게 설명해주는 강의였습니다.\n실습 과정이 포함되어 있어 Kubernetes의 기본기를 익히기에 좋았습니다.\n참고로 해당 강의는 영어로 촬영된 강의입니다. (Udemy의 기계 번역 자막이 지원되었던 걸로 기억합니다.)\n\nCKA 취득으로 얻은 것들\n그렇다면 저는 CKA를 취득하면서 어떤 것을 얻었을까요?\n가장 먼저, Kubernetes 기본 지식을 학습하면서 Kubernetes에 대한 관심이 더 깊어졌습니다. 이는 저에게 큰 영향을 주었고, 이후 제가 클라우드 엔지니어링 쪽으로 커리어를 쌓고자 한 계기가 되었습니다.\n그리고 CKA 자격증 취득 후, 제가 Kubernetes에 관심이 있고 꾸준히 공부하고 있음을 쉽게 알릴 수 있었습니다. 그리고 이러한 점은 이직 당시에도 유리하게 작용했다고 생각합니다."},"blog/CKS-취득-후기":{"title":"CKS 취득 후기","links":["📆-Project/Guide-to-DevOps/KO/CKA-취득-후기"],"tags":["Certificate"],"content":"\nCKS(Certified Kubernetes Security Specialist)는 클라우드 보안 지식을 중점적으로 확인하는 시험이며, 제가 두 번째로 취득한 Kubernetes 자격증입니다. CKS 자격증 취득을 계기로 저는 클라우드 보안과 클라우드 네이티브 프로젝트들에 대해 더 관심을 가지고 공부하게 되었죠.\n출제 범위를 제외한 CKS 자격증의 대한 응시 정보는 CKA와 거의 유사합니다.\n\n응시료: $395\n기본 언어: 영어\n시험 유형: 온라인 감독 환경에서 실습형\n응시 가능 기간: 결제 후 1년\n자격증 유효 기간: 합격 후 2년\n불합격 시 1회 재응시 기회 제공\n\nCKS에 응시한 이유\nCKS는 다른 Kubernetes 자격증과 달리 유일하게 응시 조건이 있었는데요. 바로 CKA 자격증을 먼저 취득해야 하는 것입니다. 그래서 CKS가 CKA의 상위 자격증이란 인식이 있었고, 실제로 난이도도 Kubernetes 자격증 중 가장 어렵다는 평이 있었기에, Kubernetes 역량을 더 키우고자 도전하게 되었습니다.\n또한 CKS의 출제 범위인 클라우드 보안에 대해서도 저는 거의 몰랐기 때문에 이 분야에 대해 공부하고 싶은 마음도 있었습니다.\nCKS 준비 방법\nUdemy 온라인 강의인 Kim Wüstkamp의 Kubernetes CKS Complete Course를 수강하면서 시험을 준비했었는데요. 앞서 말씀드렸던 것처럼, 저는 보안쪽 지식이 거의 없었기에 이 강의에 포함되었던 모든 이론 설명 영상과 실습 문제를 풀면서 개념을 익히고 시험을 준비했습니다.\n해당 강의를 모두 수강 후 시험에 합격할 수 있었으니, 클라우드 보안에 대한 개념을 잘 잡아주는 강의였다고 생각합니다.\n또한 강의 중간중간에 클라우드 보안과 관련된 좋은 블로그 글이나 컨퍼런스 발표 영상도 공유해줘서 관련 개념을 이해하는 데에 도움이 되었습니다.\n그리고 KodeKloud의 강의와 유사하게 Killer Shell이라는 플랫폼을 통해 클라우드 보안 관련 실습 문제를 풀 수 있었는데요. 역시 관련 개념을 잡기에 좋았습니다.\nCKS 취득으로 얻은 것들\nCKS를 취득하면서 클라우드 보안 분야에 대한 기본적인 지식을 얻었고, 해당 분야에 대해 더 깊이 관심을 가지게 되었습니다. 그동안 경험했던 DevOps 엔지니어링 분야와는 또다른 느낌이어서 신선하기도 했고, 보안도 소프트웨어 개발 및 운영에 있어 필수적인 것이라는 생각이 들어서요.\n그리고 CKS의 출제 범위에는 다양한 보안 관련 Third-party 툴들이 포함되는데요. 그 덕분에 이런 툴들을 대략적으로 경험할 수 있었고, 이로 인해 클라우드 네이티브 프로젝트들에 대해서도 관심을 가지게 되었습니다.\n마지막으로, 가장 난이도 높은 Kubernetes 자격증을 취득했다는 사실에 Kubernetes에 대한 자신감이 더 상승했습니다.😁\n물론 실무에서 활용할 수 있는 역량도 함께 길러야 하겠지만요.👍"},"blog/CNCF에서-졸업한-KubeEdge-프로젝트-소개":{"title":"최근 CNCF에서 졸업한 KubeEdge 프로젝트란?","links":[],"tags":["Kubernetes"],"content":"🎓CNCF로부터 졸업한 KubeEdge\n\n2024년 10월, KubeEdge라는 엣지 컴퓨팅(Edge Computing) 오픈소스 프로젝트가 졸업(Graduated) 단계가 되었다고 CNCF가 밝혔습니다.\n클라우드 네이티브 소프트웨어 생태계 발전을 위해 설립된 CNCF(Cloud Native Computing Foundation)는 오픈소스 클라우드 네이티브 프로젝트를 육성하고 유지하기 위해 다양한 프로젝트를 관리하고 있는데요.\nCNCF에서 관리하는 프로젝트들은 성숙도에 따라 샌드박스(Sandbox), 인큐베이팅(Incubating), 졸업(Graduated) 단계를 부여받습니다.\n프로젝트가 졸업 단계에 돌입했다는 것은, 충분히 성숙하여 실제 운영환경에 해당 프로젝트를 안정적으로 적용할 수 있음을 의미하는데요.\nKubernetes, Argo CD, Helm, Prometheus 등 우리에게 익숙한 여러 클라우드 네이티브 프로젝트가 이미 CNCF의 졸업 단계를 거쳤답니다.\n☁️KubeEdge는 어떤 프로젝트?\n\n그렇다면 이번에 CNCF에서 졸업한 KubeEdge는 어떤 프로젝트일까요?\n위에서 KubeEdge는 오픈소스 엣지 컴퓨팅 프로젝트라고 잠깐 언급했었는데요. 엣지 컴퓨팅이란, 데이터를 생성한 위치와 가까운 곳(또는 가장자리라고 해서 Edge라는 단어가 붙습니다.)에서 처리, 분석 및 저장하는 기술을 말합니다.\n예를 들면, 자율 주행 차량에 설치된 다양한 IoT 센서가 실시간으로 수집하는 수많은 데이터를 즉각적으로 처리할 수 있도록 엣지 컴퓨팅 기술을 사용할 수 있는데요. 수집한 데이터를 처리하기 위해 중앙 데이터 센터로 전송하던 기존의 방식보다 대기 시간과 통신 비용이 대폭 줄어든다는 장점이 있습니다.\nKubeEdge는 이런 엣지 컴퓨팅 기술을 Kubernetes 기반에서 구현할 수 있도록 도와주는 프로젝트입니다. 컨테이너에서 동작하는 애플리케이션을 디바이스(Edge) 내에 배포해서 사용할 수 있는 것인데요. KubeEdge는 클라우드와 엣지 환경으로 구분되어, 관리자가 클라우드와 연결된 Kubernetes API Server를 이용하여 중앙 집중형 관리가 가능합니다.\n\nKubeEdge를 사용해서 얻을 수 있는 또 다른 이점은 아래와 같습니다.\n개발 간소화\n개발자는 쉽게 접할 수 있는 HTTP 또는 MQTT 기반으로 애플리케이션을 개발하고 이를 클라우드나 엣지 환경 중 원하는 곳에 동작시킬 수 있습니다.\n네이티브한 Kubernetes 지원\nKubeEdge를 사용하면 애플리케이션 배포, 노드(디바이스) 관리, 모니터링과 같은 활동을 기존의 Kubernetes 환경과 동일하게 수행 가능합니다.\n배포 간소화\n이미 Kubernetes 상에서 배포되어 동작하던 리소스는 엣지 환경에서도 쉽게 재배포가 가능합니다.\n❕KubeEdge CNCF 졸업의 시사점\nKubeEdge 프로젝트는 2018년 화웨이 클라우드에서 오픈소스로 공개되었고, 2019년에 CNCF에서 첫 번째 클라우드 네이티브 엣지 컴퓨팅 프로젝트로서 샌드박스 단계로 합류했는데요. 2020년 인큐베이팅 단계, 그리고 올해 졸업 단계를 거치면서 110여개의 조직의 컨트리뷰션을 받아왔습니다.\nCNCF는 KubeEdge가 스마트 에너지, 자동차, 물류, 블록체인, 위성 시스템 분야까지 폭넓게 적용되었다고 언급했는데요.\nKubernetes 기반 엣지 컴퓨팅 프로젝트인 KubeEdge가 CNCF의 공식 졸업 단계를 거침으로써, 앞으로 Kubernetes의 적용 범위는 더 큰 폭으로 확장될 것으로 예상됩니다.\nReferences\n\nwww.cncf.io/announcements/2024/10/15/cloud-native-computing-foundation-announces-kubeedge-graduation/\nkubeedge.io/docs/\nwww.redhat.com/ko/topics/edge-computing/what-is-edge-computing\nwww.cncf.io/projects/\n"},"blog/Clair-소개":{"title":"Clair 소개","links":["blog/컨테이너-보안-스캐닝"],"tags":["Security"],"content":"Clair는 컨테이너 이미지의 구성요소를 분석하여 취약점을 보고해주는 애플리케이션입니다. 이러한 활동을 컨테이너 보안 스캐닝이라고 하며, 더 자세한 설명은 여기서 확인하실 수 있습니다.\nClair의 동작 방식\nClair는 Indexer, Matcher, Notifier 컴포넌트로 구성되며, 취약점 분석은 Indexing과 Matching 순서로 진행됩니다.\n\n\n\nIndexing\n\n먼저 Clair의 Indexer가 스캔 대상인 컨테이너 이미지를 분석하여 이미지에 포함된 패키지, 이미지의 Base 이미지, 이미지에서 사용되는 레파지토리 등의 정보를 얻어냅니다.\n이렇게 얻어낸 컨테이너 이미지 관련 정보(Clair 공식 문서에선 Manifest라고 합니다.)는 DB에 저장되며, 해당 인덱싱에 대한 정보도 IndexReport라는 데이터 구조로 DB에 저장됩니다.\nIndexReport는 Matching 단계에서 취약점 분석에 쓰입니다.\n\n\n\nMatching\n\nClair의 Matcher가 DB에 저장된 IndexReport를 가져와 취약점 분석을 진행합니다.\n취약점 분석은 DB에 미리 저장되어있는 최신 취약점 데이터를 기반으로 진행됩니다.\nMatcher는 취약점 분석 후 VulnerabilityReport라는 취약점 보고서 객체를 생성합니다.\n취약점 보고서는 HTML 양식으로도 생성되어 웹 브라우저에서 조회 가능합니다.\n또한 Matcher는 주기적으로 최신 취약점 데이터를 DB에 업데이트하는 역할도 수행합니다.\n\n\n\nNotifications\n\nClair는 취약점 분석에 대한 알림 기능도 지원합니다.\nClair의 Notifier는 Matcher가 수행하는 DB의 최신 취약점 데이터 업데이트를 추적하고, 기존에 Indexer가 DB에 저장했던 컨테이너 이미지의 Manifest 중 취약점 데이터 업데이트에 영향을 받는 것이 있으면 알려주는 역할을 수행합니다.\nNotifier의 알림은 Webhook 등으로 받아볼 수 있습니다.\n\n\n\nClair의 활용 방안\n\nCI/CD에 Clair를 활용하면 서비스를 배포하기 전에 컨테이너 이미지를 스캔하여 보안 취약점을 사전에 최소화할 수 있습니다.\nClair의 Mathcer와 Notifier를 활용하여 최신 취약점 업데이트 및 알림을 팀내 전파하는 시스템을 구축하면, 개발자가 사전에 적절한 조치를 취할 수 있고 보안 이슈를 미리 예방할 수 있습니다.\n\nReferences\n\nwww.wiz.io/academy/container-security-scanning\nsnyk.io/learn/container-security/container-scanning\n"},"blog/Dockerfile의-ADD-vs-COPY":{"title":"Dockerfile의 ADD vs COPY","links":[],"tags":["Docker"],"content":"ADD 명령어와 COPY 명령어의 공통점\n두 명령어 모두 호스트의 특정 경로(출발 경로)에 있는 파일 또는 디렉토리를 Docker 이미지 안의 특정 경로(도착 경로)로 복사할 수 있습니다.\nADD 명령어의 또다른 기능들\nCOPY 명령어와 달리, ADD 명령어에는 일반 복사 외에 추가로 지원하는 기능이 있습니다.\nURL을 활용하여 파일 다운로드\n\nADD 명령어의 출발 경로를 입력하는 부분에 호스트 경로 대신 URL을 입력할 수 있습니다.\nURL을 입력할 경우, 해당 원격지로부터 파일을 다운로드하여 Docker 이미지의 도작 경로에 추가됩니다.\n\n압축 자동 해제 및 추출\n\nADD 명령어의 출발 경로에 호스트 내에 압축(gz, bz2, xz)된 tar 아카이브 파일이 들어갈 수도 있습니다.\n압축 파일은 자동으로 해제되며, 이때 추출된 디렉토리가 Docker 이미지의 도착 경로에 저장됩니다.\n"},"blog/Dockerfile의-RUN,-CMD,-ENTRYPOINT-명령어":{"title":"Dockerfile의 RUN, CMD, ENTRYPOINT 명령어","links":[],"tags":["Docker"],"content":"Dockerfile을 통해 Docker 컨테이너를 실행하는 과정에서, 특정한 작업이나 애플리케이션의 프로세스를 실행시킬 때 RUN, CMD, ENTRYPOINT 명령어를 사용합니다. 각 명령어에 어떤 차이점이 있는지 알아보겠습니다.\nRUN 명령어\n먼저 RUN 명령어는 Dockerfile에서 이미지를 생성할 때 가장 많이 쓰이는 명령어로, 애플리케이션이나 패키지를 설치할 때 사용합니다. 기존 이미지 위에 새로운 레이어를 생성하는 명령어입니다.\nFROM ubuntu:18.04\nRUN apt-get update\nRUN apt-get install -y nginx\n위 Dockerfile 예제에서는 Ubuntu 18.04 환경 위에 apt-get update 명령어로 패키지 업데이트를 한 다음, nginx를 설치합니다. 다만 위와 같은 Docker 이미지는 패키지 업데이트 및 애플리케이션 설치만 될 뿐, 어떠한 프로세스가 실행되지 않기 때문에 컨테이너가 정상적으로 동작하지 않습니다.\n정상적으로 동작하는 Docker 이미지를 만들기 위해서는 컨테이너 내부에서 최종적으로 실행되는 프로세스를 CMD 또는 ENTRYPOINT 명령어로 정의해야 합니다.\nCMD 명령어\n컨테이너 내부에서 최종적으로 실행되는 프로세스의 명령어로, Docker CLI상에서 다른 명령어로 덮어씌울 수 있습니다.\nFROM ubuntu:18.04\nRUN apt-get update\nCMD [&quot;echo&quot;, &quot;Hello, World&quot;]\n위 Dockerfile 예제를 이용하여 Docker 이미지를 빌드하고, docker run [이미지 이름] 명령어로 실행 시 실행 결과는 아래와 같이 CMD 명령어로 정의한 프로세스가 실행됩니다.\nHello, World\n\n만약 이미지 빌드 후 docker run [이미지 이름] hostname 명령어로 실행할 경우, CMD 명령어로 정의한 프로세스가 아닌 hostname 프로세스만 실행되어 컨테이너의 호스트네임이 표시됩니다.\n4aa43esdd984\n\nENTRYPOINT 명령어\nCMD 명령어와 달리, Docker CLI상에서 다른 명령어로 덮어씌울 수 없습니다.\nFROM ubuntu:18.04\nRUN apt-get update\nENTRYPOINT [&quot;echo&quot;, &quot;Hello, World&quot;]\n위 Dockerfile 예제를 이용하여 Docker 이미지를 빌드하고, docker run [이미지 이름] 명령어로 실행 시 실행 결과는 아래와 같이 ENTRYPOINT 명령어로 정의한 프로세스가 실행됩니다.\nHello, World\n\n만약 이미지 빌드 후 docker run [이미지 이름] hostname 명령어로 실행할 경우, ENTRYPOINT 명령어로 정의한 echo Hello, World 뒤에 hostname 명령어가 따라 붙어 함께 실행됩니다.\nHello, World 4aa43esdd984\n"},"blog/Docker에서-사용하는-네트워크":{"title":"Docker에서 사용하는 네트워크","links":[],"tags":["Docker","Network"],"content":"Docker 네트워크의 종류\nDocker에서 사용하는 네트워크 중 대표적인 것은 아래 3가지입니다.\n\nBridge\nHost\nOverlay\n\nBridge 네트워크\n\n가상 인터페이스와 호스트의 인터페이스를 연결하여 도커 컨테이너가 외부와 연결 가능하게 해주는 네트워크입니다.\nDocker가 실행될 때 자동으로 Bridge 네트워크가 생성되며, Docker 컨테이너 생성 시 네트워크를 따로 지정하지 않으면 Docker 컨테이너가 미리 생성되어 있던 Bridge 네트워크와 연결됩니다.\n\nHost 네트워크\n\n호스트 네트워크 환경과 IP를 그대로 사용하며, Docker 컨테이너 내부 애플리케이션을 별도로 포트포워딩하지 않고 외부에서 접근 가능합니다.\n\nOverlay 네트워크\n\n다른 호스트 간에 네트워크를 공유하는 방식입니다. (호스트가 여러 개일 때 사용)\nOverlay 네트워크에 연결된 컨테이너들은 암호화 방식을 사용하여 서로 안전하게 통신할 수 있습니다.\n"},"blog/Docker에서-사용하는-파일-시스템":{"title":"Docker에서 사용하는 파일 시스템","links":[],"tags":["FileSystem","Docker"],"content":"UFS(Union File System)\n\nUFS는 여러 개의 파일 시스템을 하나의 파일 시스템에 마운트하는 방식입니다.\n현재 Docker에서는 UFS를 구현하기 위한 Storage Driver로 Overlay를 주류로 사용합니다.\nDocker 이미지에서 Layer는 각각의 파일 시스템을 겹쳐 놓은 형태와 유사합니다.\nLayer는 Container Layer와 Image Layer로 나뉩니다.\n\nContainer Layer:\n\n쓰기 작업이 가능한 Layer입니다.\n각 컨테이너의 최상단 Layer이며, 컨테이너 생성 후 모든 변경 작업이 이루어지는 Layer입니다.\nR/W 속도는 상대적으로 느립니다.\n\nImage Layer:\n\n읽기 작업만 가능한 Layer입니다.\n다른 컨테이너와 공유되는 Layer입니다.\n"},"blog/Elasticsearch-및-Kibana에-오픈소스-라이선스-적용-선언":{"title":"Elasticsearch 및 Kibana의 오픈소스 라이선스 적용 선언","links":[],"tags":["Monitoring"],"content":"✨Elastic의 오픈소스 라이선스 적용 선언\n\n2024년 8월 말, Elastic은 Elasticsearch 및 Kibana에 오픈소스 라이선스를 다시 적용하겠다고 발표했습니다. (원문 링크)\n현재 Elasticsearch 및 Kibana 최신 버전(v8.15.1) 이후에 릴리즈되는 버전부터 AGPL v3.0(GNU Affero General Public License v3.0) 라이선스가 적용될 수 있도록 수정할 예정이라는 게 이번 발표의 요지인데요.\nAGPL v3.0이 적용되는 소프트웨어를 수정 없이 상용 제품에 사용하는 경우, 제품의 코드까지 공개하지 않아도 된다는 점에서 사용자에게 유리합니다.\n하지만 AGPL v3.0이 적용되는 소프트웨어 자체를 수정한 경우에는 해당 소스코드를 공개해야 합니다. 게다가 아래 AGPL-3.0 파생 저작물 범위에 해당하는 제품의 소스 코드는 바이너리 형태로 재배포할 때뿐만 아니라 네트워크를 통해 사용자와 상호 작용을 하는 경우에도 공개해야 하는데요.\n\nAGPL이 적용되는 소프트웨어의 수정 코드\nAGPL이 적용되는 소프트웨어와 동일한 프로세스에서 동작하는 Module\nAGPL이 적용되는 소프트웨어와 링크로 연결한 Library\nAGPL이 적용되는 소프트웨어를 상속한 Class\n\nAGPL이 적용되는 소프트웨어와 제품이 어떻게 결합되느냐에 따라 원격 네트워크로 서비스하는 경우에도 제품 소스 코드를 공개해야 하는 리스크도 존재하기 때문에 세심한 주의가 필요합니다.\nElastic도 해당 발표에서 자신들도 이번 라이선스 적용으로 사용자들이 우려할 수 있음을 알고 있다고 밝혔는데요. ‘AGPL은 MongoDB와 Grafana 등의 프로젝트에도 적용된 라이선스이기 때문에 Elasticsearch와 Kibana 사용에 AGPL이 큰 영향을 미치진 않을 것’이라고 언급하면서, ‘AGPL 적용은 우리가 오픈소스 생태계로 다시 다가갈 수 있는 좋은 첫걸음이 될 거라 믿는다’고 했습니다.\n🧐기존 Elasticsearch와 Kibana의 라이선스는 어땠길래?\n그렇다면 Elasticsearch와 Kibana의 라이선스가 어떻게 변해왔길래 Elastic이 오픈소스로 다시 돌아왔다고 발표한 것일까요?\nElasticsearch를 포함한 ELK(Elasticsearch, Logstash, Kibana) 스택 툴들은 처음 릴리즈 당시 Apache-2.0 라이선스가 적용되고 있었습니다. Apache-2.0이 적용되는 소프트웨어는 수정하거나 상용 제품에서 활용되어도 제품의 소스코드를 공개할 의무가 없기 때문에 대표적인 오픈소스 라이선스라고 할 수 있는데요.\nELK 스택 툴들은 모두 7.10 버전대까지 Apache-2.0 라이선스가 적용되었고, 이 버전대의 ELK 툴들을 OSS(Open Source Software) 버전이라고도 합니다.\n이후 Elasticsearch와 Kibana는 2021년에 Elastic License와 SSPL(Server Side Public License) 중 하나를 선택해서 적용될 수 있도록 변경되었습니다.\nElastic License에는 ‘제품을 다른 사람에게 관리형 서비스(Managed Service)로 제공할 수 없다’는 내용이 포함되어 있는데요. 이는 외부 사용자가 Elasticsearch API를 직접 사용할 수 있거나 Kibana의 대시보드에 직접 접속할 수 있도록 제공하는 것 등을 제한하는 내용입니다. 만약 이런 제한을 풀고 싶다면, Elastic 담당자와 컨설팅 및 유료 버전을 구독해야 합니다.\nSSPL에는 ‘소프트웨어를 실행하는 데에 필요한 서비스 소스코드를 공개해야 한다’는 내용이 명시되어 있습니다. 라이선스가 적용되는 소프트웨어를 관리하기 위한 모니터링이나 백업용 프로그램의 이 ‘서비스 소스코드’에 포함되기 때문에, 원치 않는 소스코드를 공개해야 하는 리스크가 존재합니다.\n이렇게 2021년부터 Elasticsearch와 Kibana에 적용되었던 라이선스는 모두 오픈소스 라이선스라고 할 수 없었습니다. 그렇기 때문에 이번 AGPL v3.0 적용을 Elastic에선 오픈소스로 다시 돌아왔다고 표현하는 것이죠.\n\n하지만 AGPL v3.0 역시 위에서 살펴봤던 것처럼 일부 소스코드 공개 조건이 존재하기 때문에, 오픈소스와 Elastic 커뮤니티에선 관련해서 여러 논의가 이어질 것으로 보입니다.\nElastic에서 발표한 새로운 라이선스 정책은 Elasticsearch 및 Kibana를 사용하려던 분들이 참고할 만한 내용이라 생각해서 다뤄봤는데요.\n여담으로 제가 ELK 스택 모니터링 시스템과 Kubernetes를 주제로 한 온라인 강의를 제작 중이어서, 저에게도 이번 내용이 흥미로웠습니다.😄\n제작 중인 강의는 10월 중 릴리즈를 목표로 하고 있는데, 관련 소식은 추후 다시 공유드려보겠습니다.🌱\nReferences\n\nwww.elastic.co/kr/blog/elasticsearch-is-open-source-again\nsktelecom.github.io/guide/use/obligation/agpl-3.0\nbonohubby.com/entry/Elasticsearch-License\n"},"blog/Falco-소개":{"title":"Falco 소개","links":["blog/컨테이너-런타임-보안"],"tags":["Security"],"content":"\nFalco는 Linux 시스템을 대상으로 개발된 클라우드 네이티브 보안(Cloud-native Security) 툴입니다. Linux 커널 이벤트에 대한 규칙(Rule)을 지정하면 이에 따른 실시간 알림을 보내주는 역할을 수행합니다.\nFalco가 지원하는 이러한 행위를 컨테이너 런타임 보안 활동이라고 하는데요. 이와 관련된 자세한 설명은 여기서 확인하실 수 있습니다.\nFalco를 사용하는 이유\n현재 컨테이너 런타임 보안 활동을 지원하기 위해 다양한 툴이 개발 및 유지보수되고 있습니다. 그 중 Falco는 아래와 같은 장점을 지니고 있습니다.\n\nKubernetes, Docker Swarm과 같은 다양한 Container Orchestration Platform에서 사용 가능\nKubernetes의 Daemonset으로 배포 및 실행을 지원하므로 모든 노드에 대한 모니터링이 용이함\nKubernetes API Call을 활용할 수 있어 클러스터 내 Node 및 Pod의 상태를 모니터링 가능\nFalco 가 사용할 수 있는 Default Ruleset이 미리 정의되어 있어 컨테이너 런타임 보안 활동을 쉽고 빠르게 시작 가능\n특정 조건에 대한 이상 행위를 감지하는 Custom Ruleset을 쉽게 정의 가능\n\nFalco의 동작 방식\nFalco의 동작 프로세스는 크게 아래 3단계로 나눌 수 있습니다.\n\nLinux Kernel의 System Call 또는 Kunernetes API Call 포착\n사전 정의된 Ruleset (또는 Default Ruleset)에 따라 표시할 Output 처리\n사전 정의된 채널로 Output 전송\n\nFalco는 동작 중인 컨테이너에서 생성되는 여러 요청을 감시하기 위해 Ruleset을 사용하는데요. Falco의 Ruleset은 아래와 같이 정의됩니다.\n- rule: Detect bash in a container # Rule 이름\n  desc: You shouldn&#039;t have a shell run in a container # Rule 설명\n  condition: container.id != host and proc.name = bash # 프로세스의 Call 중 Falco가 포착하는 조건\n  output: Bash ran inside a container (user=%user.name command=%proc.cmdline %container.info) # 해당 Rule에 대해 Falco가 표시하는 Output 양식\n  priority: INFO # Falco가 표시하는 Output의 Type\nFalco는 다양한 채널을 통해 Output을 전송합니다. 그 중 대표적인 채널은 아래와 같습니다.\n\nStandard Output\nFile\nSyslog\nHTTP[s]\ngRPC\n\nFalco 설치 방식\n\n\nKubernetes 클러스터에 Helm Chart로 설치\n\nKubernetes 클러스터에 Falco를 가장 쉽고 빠르게 설치하는 방법은 Helm을 이용하는 것입니다.\nHelm으로 Falco를 설치하면, 배포된 Falco Pod의 로그를 확인함으로써 Falco Ouput 모니터링이 가능합니다.\nCustom Ruleset을 yaml 파일로 정의한 다음, Helm으로 배포되어있는 Falco에 적용하는 것도 가능합니다.\n\n\n\nLinux 호스트에 패키지로 설치\n\nFalco 공식 패키지 저장소에서 각 시스템 아키텍처에 맞는 버전을 직접 설치 가능합니다.\n이렇게 설치한 Falco의 Configuration 및 Ruleset 파일 역시 호스트에 저장 및 사용되므로 수정이 필요하다면 호스트 내 파일을 직접 수정해야 합니다.\n\n\n\nLinux 호스트에 컨테이너 이미지로 배포\n\nFalco의 공식 컨테이너 이미지 저장소에서 Falco 실행이 가능한 컨테이너 이미지를 받아와 호스트에 배포할 수도 있습니다.\n\n\n\nReferences\n\nfalco.org/\ngithub.com/falcosecurity/charts/tree/master/charts/falco\nsysdig.com/blog/intro-runtime-security-falco/\n"},"blog/Fluent-Bit에서-발견된-보안-취약점-(CVE-2024-4323)":{"title":"Fluent Bit에서 발견된 보안 취약점 (CVE-2024-4323)","links":[],"tags":["CVE","Monitoring"],"content":"\n다양한 클라우드 제공자와 IT 기업에서 널리 사용 중인 로그 데이터 수집 툴 Fluent Bit에서 보안 취약점(CVE-2024-4323)이 발견되었습니다.\n해당 취약점은 Fluent Bit의 내장 HTTP 서버에 존재하던 버퍼 오버플로(Buffer Overflow)가 원인이라고 합니다.\n해당 취약점이 위험한 이유\n이번 취약점에 대한 테스트 결과, Buffer Overflow 동작으로 인해 인접한 메모리에 저장되어있던 정보가 HTTP 응답으로 반환되는 경우가 있었다고 합니다. 이는 민감한 정보가 유출될 위험이 있음을 보여주는데요.\n그 외에도 서비스 거부(Denial of Service), 원격 악성 코드 실행 공격 등에 해당 취약점이 이용될 수도 있다고 합니다.\n다만 해당 취약점을 이용해서 원격 악성 코드 실행을 수행하는 것은 비교적 어렵기 때문에, 해당 공격 위험이 급박한 것은 아니라고 합니다.\n대처 방법\n해당 보안 취약점에 대처할 수 있는 가장 확실한 방법은, Fluent Bit를 v3.0.4 이상으로 업그레이드하는 것입니다.\n만약 버전을 바로 업그레이드하기 어렵다면, 인증된 사용자와 서비스만 Fluent Bit의 모니터링 관련 API에 접근할 수 있도록 설정하는 것이 권장된다고 합니다.\nReferences\n\nCritical Fluent Bit flaw affects major cloud platforms, tech companies’ offerings (CVE-2024-4323)\nwww.tenable.com/plugins/nessus/197568\n"},"blog/Infrastructure-as-Code-(IaC)-알아보기":{"title":"Infrastructure as Code (IaC) 알아보기","links":[],"tags":["Infrastructure"],"content":"⚙️Infrastructure as Code란\n\n우리가 애플리케이션을 배포하고 서비스를 운영하려면 서버, 스토리지, 네트워크 등과 같은 인프라 설정이 필요한데요.\n인프라 구축을 애플리케이션 배포 때마다 매번 직접 수동으로 한다면 시간도 오래 걸리고 지루한 작업이 될 것입니다.\n만약 이런 인프라 준비 작업을 코드 기반으로 관리하고 자동화할 수 있다면 어떨까요?\n이렇게 등장한 개념이 바로, **Infrastructure as Code(IaC)**입니다.\n✅IaC를 적용해야 할 이유\n\n앞서 살펴본 것처럼, IaC는 인프라 환경을 수동으로 준비하고 설정하는 대신에 코드를 기반으로 필요한 인프라 리소스를 프로비저닝하고 관리할 수 있는 개념입니다.\n이런 IaC는 DevOps와 CI/CD를 구현하는 데에도 중요한데요.\nDevOps 접근법을 기반으로 개발팀과 운영팀의 배포 환경을 직접 수동으로 맞춘다면 갖가지 오류나 불일치 문제가 발생할 수 있고, 극단적인 경우에는 애플리케이션을 수동으로 배포해야만 하는 상황이 생길 수도 있습니다.\n하지만 IaC를 도입하면 개발팀과 운영팀이 사용할 애플리케이션 배포 환경을 수월하게 맞출 수 있어서 애플리케이션 배포 자동화가 가능하고, 이는 DevOps 접근법에도 부합합니다.\nIaC를 적용하면 인프라 환경 역시 CI/CD 파이프라인을 통해 테스트와 버전 컨트롤이 가능하다는 장점이 있기 때문에 DevOps 관점에서도 IaC 도입이 권장됩니다.\nIaC의 개념에 대해 살펴봤으니 IaC를 수행할 수 있는 툴을 살펴볼 차례인데요.\n각 IaC 툴에 대해 알아보기 전에 IaC 툴을 사용하면서 얻을 수 있는 이점을 정리하면 아래와 같습니다.\n\n인프라 구축 프로세스를 자동화하여 인프라 프로비저닝 및 관리에 필요한 시간 단축\n인프라 리소스 구축에 대한 정의를 소스코드로 관리할 수 있기 때문에 휴먼 에러(Human error)를 줄이고, 일관성 있는 환경 구축 가능\n코드 기반으로 인프라를 자동으로 구축할 수 있기 때문에 인프라 구성을 보다 쉽게 확장할 수 있고, 상황에 맞게 인프라 환경을 더욱 신속히 변경 가능\n인프라 변경 이력이 관련 코드 수정으로 기록되기 때문에 이력 추적과 문서화에 용이\n\n이런 이점을 가진 대표적인 IaC 툴 2가지 Terraform과 Crossplane에 대해 간단히 알아보려 하는데요. 첫 번째 순서는 Terraform입니다.\n🛠️IaC 툴 알아보기\nTerraform\n\nHashiCorp에서 제공하는 Terraform은 퍼블릭 클라우드와 온프레미스 인프라 리소스를 읽기 쉬운 코드로 정의할 수 있도록 도와줍니다.\nHashiCorp와 Terraform 커뮤니티에서 개발한 수많은 Provider 덕분에 다양한 클라우드 플랫폼과 서비스의 리소스를 API를 이용해서 자체 개발한 문법의 코드로 인프라 구축이 가능한데요.\nTerraform의 워크플로우는 아래와 같이 3단계로 나눌 수 있습니다.\n\nWrite(작성): 필요한 인프라 리소스를 개발자가 코드 기반으로 정의\nPlan(계획): 정의한 코드를 기반으로 Terraform이 리소스 생성, 업데이트, 제거 준비\nApply(적용): 계획에 대해 승인이 될 경우, Terraform이 알맞은 순서에 맞게 정의된 리소스에 대한 작업 수행\n\nCrossplane\n\n이제 Crossplane에 대해 간단히 알아보겠습니다. Crossplane은 Kubernetes 커스텀 리소스를 이용해서 인프라 관련 리소스를 선언적으로 관리할 수 있는 오픈소스 Kubernetes 애드온인데요.\nKubernetes 클러스터에 설치된 Crossplane을 통해, 사용자는 Kubernetes와 상호작용하는 것만으로 AWS, Azure, GCP 같은 외부 리소스를 관리할 수 있게 됩니다.\nTerraform과 Crossplane의 차이\n두 IaC 툴 모두 인프라 관련 리소스를 쉽고 효율적으로 구축하고 관리할 수 있도록 도와준다는 것을 알 수 있는데요. 그렇다면 Terraform과 Crossplane의 차이점은 무엇일까요?\n네이티브 k8s와의 호환성\nCrossplane은 Kubernetes 기반으로 설계된 반면, Terraform은 독자적으로 사용 가능하고 Kubernetes와의 연동은 별도로 제공된 Provider를 이용합니다.\n리소스 선언 문법\n두 가지 툴 모두 선언적 문법을 사용하는데요. 다만 Crossplane은 Kubernetes에서도 사용하는 YAML 문법을 사용하고, Terraform은 독자적인 HCL 문법을 사용합니다.\n프로비저닝\nCrossplane은 Kubernetes Controller를 통해 리소스를 프로비저닝하지만, Terraform은 별도로 개발된 API/SDK를 통해 직접 프로비저닝합니다.\n생태계\nTerraform은 Provider에 대한 생태계가 성숙한 상태이지만, Crossplane의 생태계는 계속 성장 중에 있습니다.\nIaC 툴과 Helm chart의 차이\nKubernetes를 관리하고 운영하다보면 프로젝트 단위로 리소스를 쉽게 관리하기 위해 Helm chart를 사용하는 경우가 많습니다.\n그래서 위에서 소개한 IaC 툴들과 Helm chart가 동일한 성격의 툴로 보이기도 하는데요.\n하지만 IaC 툴과 Helm chart에도 명확한 차이점은 존재합니다.\nIaC 툴은 Kubernetes뿐만아니라 AWS, GCP, Azure와 같은 퍼블릭 클라우드의 인프라 리소스도 구성이 가능합니다. 하지만 Helm chart는 Kubernetes 클러스터 내의 리소스만 배포하고 관리할 수 있도록 도와주죠.\n또한 IaC 툴은 새로운 Kubernetes 클러스터 생성이 가능하지만, Helm chart는 기존 Kubernetes 클러스터 내의 리소스만 생성할 수 있습니다.\nReferences\n\nwww.redhat.com/en/topics/automation/what-is-infrastructure-as-code-iac\nspacelift.io/blog/infrastructure-as-code-tools\ndeveloper.hashicorp.com/terraform/intro\ndocs.crossplane.io/latest/getting-started/introduction/\nmedium.com/@bijit211987/crossplane-vs-terraform-which-infrastructure-as-code-tool-is-best-a5f41f65e8a1\nThe Difference between Terraform and Helm Charts - Coralogix\n"},"blog/Istio-소개":{"title":"Istio 소개","links":["blog/Service-Mesh-소개"],"tags":["Network"],"content":"\nIstio는 클라우드 환경에서 Service Mesh를 구현할 수 있는 오픈소스 솔루션입니다. Service Mesh가 무엇이고 왜 사용되는지 궁금하시다면, 여기를 확인해보세요.\nIstio를 사용하는 이유\nIstio는 Service Mesh의 주요 기능들을 모두 지원하고 있는데요. Istio를 사용함으로써 얻을 수 있는 이점을 각 측면 별로 정리하면 아래와 같습니다.\n\n가시성\n\nService Mesh에서 발생하는 모든 Service 통신 관련 로그와 메트릭을 쉽게 모을 수 있음\n\n\n트래픽 관리\n\nService 사이의 트래픽과 API 요청을 별도의 레이어에서 제어 가능\nCircuit Breaker, Timeout, Retry와 같은 Service에 대한 속성을 별도 영역에서 쉽게 설정 가능\n\n\n보안성\n\n트래픽 허용 관련 Policy나 TLS 암호화, 인증 등의 기능 제공\n\n\n도입 용이성\n\nIstio 생태계의 규모가 큰 덕분에 다양한 기술 문서나 적용 사례를 참고 가능\n\n\n\nIstio의 동작 방식\nIstio는 Service Mesh와 마찬가지로 데이터 플레인(Data Plane)과 컨트롤 플레인(Control Plane)으로 구성됩니다.\n\n데이터 플레인\n\nKubernetes의 Service 간의 통신을 담당\nEnvoy라는 오픈소스 프록시를 각 Pod 내에 Sidecar로 배포\nEnvoy 프록시가 Service에 대해 발생하는 네트워크 트래픽을 가져와서 Istio의 각 기능을 수행\nEnvoy 프록시는 컨트롤 플레인에서 Configuration한 설정이 일괄 적용됨\n\n\n컨트롤 플레인\n\nService에 추가되는 프록시에 대한 Configuration을 담당하는 영역\n\n\n\nReferences\n\nistio.io/latest/about/service-mesh\n"},"blog/Jenkins와-Pipeline-(Declarative-vs-Scripted)":{"title":"Jenkins와 Pipeline (Declarative vs Scripted)","links":[],"tags":["Jenkins"],"content":"Jenkins와 Pipeline\nJenkins는 워크 플로우를 자동화할 수 있는 오픈 소스 툴입니다. 소프트웨어의 지속적 통합(Continuous Integration, CI)과 지속적 배포(Continuous Delivery, CD)를 간단하게 구현할 수 있도록 도와주죠. 특히 Jenkins의 Pipeline을 사용하면 배포 과정을 더욱 쉽고 빠르게 구축할 수 있기 때문에, CI/CD 프로세스를 구현할 때 Jenkins를 사용하는 경우가 많습니다.\n지속적 배포(CD) 파이프라인을 예로 들어 Jenkins의 Pipeline에 대해 더 살펴보겠습니다. CD 파이프라인은 소프트웨어를 버전 관리 시스템(예: GitHub)에서부터 사용자 및 클라이언트에게로까지 바로 전달하는 일련의 프로세스라고 할 수 있습니다. 소프트웨어의 새로운 버전을 릴리즈하는 데에 필요한 모든 워크 프로우, 활동, 자동화 과정이 여기에 포함되며, 이러한 과정을 Jenkins의 Pipeline 파일의 코드로 구현할 수 있는 것입니다.\nJenkins Pipeline 코드를 작성하는 방식에 따라 Declarative Pipeline과 Scripted Pipeline으로 나뉩니다.\nDeclarative Pipeline\nDeclarative 방식은 Scripted 방식보다 최근에 Jenkins에 추가되었으며, Pipeline 작성에 특화된 문법을 따르기 때문에 코드 작성과 관리가 더욱 용이해졌습니다.\n코드 예제\npipeline {\n  agent any\n  stages {\n    stage(&#039;Hello World) {\n      steps {\n        sh &#039;echo Hello World&#039;\n      }\n    }\n  }\n}\n장점\n\n보다 구조화된 Pipeline 코드 작성이 가능하여 코드 관리 용이성과 가독성을 높일 수 있습니다.\nBlue Ocean 인터페이스와 연동하기 쉽습니다.\n\n단점\n\nScripted 방식보다 코드 작성의 자유도가 떨어지고, 복잡한 로직의 Pipeline 코드를 작성하기 어려울 수도 있습니다.\nDeclarative 방식을 지원하지 않는 플러그인도 존재합니다.\n\nScripted Pipeline\nScripted 방식은 Jenkins가 기존에 지원하던 Pipeline 작성 방식입니다. Groovy 언어가 제공하는 대부분의 기능을 사용할 수 있어 Pipeline 코드를 유연하게 작성 가능합니다.\n코드 예제\nnode {\n  stage(&#039;Hello World) {\n    sh &#039;echo Hello World&#039;\n  }\n}\n장점\n\nPipeline 코드 작성이 비교적 자유롭습니다.\n복잡한 로직의 Pipeline 코드 작성도 가능합니다.\n\n단점\n\n구조화된 양식이 없기 때문에 불안정한 Pipeline 코드를 작성할 위험이 있으며, 코드를 이해하거나 관리하기 어려워질 수도 있습니다.\n"},"blog/KCNA와-KCSA-자격증-취득-후기":{"title":"KCNA와 KCSA 자격증 취득 후기 및 핵심 키워드","links":["🗃️-Infinity-Drawer/CKA-취득-후기","🗃️-Infinity-Drawer/CKS-취득-후기"],"tags":["Kubernetes"],"content":"📜Kubernetes와 클라우드 네이티브 지식을 검증하는 KCNA / KCSA 자격증 시험\n\n지난 글에서 Kubernetes의 대표적인 자격증인 CKA와 CKS의 취득 후기를 공유드렸습니다.\n저는 그 이후에 CKAD와 KCNA, KCSA 자격증을 취득해서 CNCF 재단의 Kubernetes 자격증을 모두 모았고, CNCF의 Kubestronaut 자격을 얻게 되었습니다. Kubestronaut에 대해 궁금하시다면, 제가 정리한 뉴스레터 글을 참고해주세요.\nCKA와 CKAD, CKS는 모두 실습형 시험입니다. 저는 실무에서 Kubernetes를 다루고 있기 때문에, 주어진 요구사항에 맞춰 Kubernetes 환경에서 각 리소스를 다루고 환경을 설정하는 실습형 시험이 낯설지는 않았습니다. 그래서 시험을 준비할 때에도 어떻게 공부해야 할지 명확했던 기억이 있습니다.\n하지만 KCNA와 KCSA는 객관식 시험입니다. 저에겐 오히려 이런 방식이 시험을 준비할 때 더 까다로웠습니다. 출제 범위는 넓은데, 그 내용을 영어로 풀어쓴 문항과 보기를 잘 이해하고 그 중 정답을 골라야 했기 때문이죠.\n그래서 이번 글에선 KCNA와 KCSA의 출제 범위와, 각 주제의 핵심 키워드를 소개해드리고자 합니다. 소개해드린 키워드와 연결 링크의 영문 문서를 참고하시면 시험 준비에 도움이 될 것입니다.\n최근 Kubernetes 자격증에 대한 관심이 많아지고 Kubestronaut를 목표하시는 분들도 많아지고 있는데, KCNA와 KCSA 시험 준비가 막막하신 분들에게 도움이 되었으면 하는 마음입니다.\n🔰KCNA의 출제 범위 및 핵심 키워드\n\nKCNA(Kubernetes and Cloud Native Associate) 시험은 Kubernetes와 클라우드 네이티브 관련 지식을 가지고 있는지를 확인합니다. 출제 범위는 위 그림과 같고, 각 출제 비중 역시 퍼센티지로 확인하실 수 있습니다.\nKubernetes Fundamentals\nKubernetes의 기본 구성요소에 대한 지식을 물어보는 문제 비중이 가장 큰데요. 아래 키워드에 대해서는 링크로 첨부한 영문 공식 문서를 참고해보시는 것을 추천드립니다. 각 구성요소가 어떤 역할을 하고 어떻게 동작하는지 이해하는 것이 중요합니다.\n\nkube-apiserver\nkube-scheduler\nkube-controller-manager\nkubelet\nkube-proxy\n\n그리고 Kubernetes의 주요 Workload 리소스에 대해서도 공식 문서 내용을 참고하는 걸 추천드립니다.\n\nDeployment\nStatefulset\nDaemonSet\nJob\nCronJob\n\nContainer Orchestration\n컨테이너 오케스트레이션을 구현하는 데에 필요한 인터페이스도 자주 출제되는 주제입니다. 관련 키워드는 아래와 같습니다.\n\nContainer Networking Interface (CNI)\nContainer Storage Interface (CSI)\nContainer Runtime Interface (CRI)\n\nCloud Native Architecture\n클라우드 네이티브의 대표적인 기술에 대해 물어보는 경우도 많은데요. 관련 주요 키워드는 아래와 같습니다.\n\n오토 스케일링(Autoscaling)\n서버리스(Serverless)\n\nCloud Native Observability\n클라우드 네이티브의 옵저빌리티 및 모니터링에 대해서는 아래 클라우드 네이티브 애플리케이션이 주요 키워드입니다.\n\nPrometheus\nGrafana\n\nCloud Native Application Delivery\n애플리케이션 배포 관련 문제도 출제될 수 있는데요. 아래 키워드에 대해 살펴보는 것을 추천드립니다.\n\nGitOps\n\nArgoCD\n\n\nCI/CD\n\nBlue-green deployment\nCanary deployment\n\n\n\n🛡️KCSA의 출제 범위 및 핵심 키워드\n\nKCSA(Kubernetes and Cloud Native Security Associate) 시험은 Kubernetes와 클라우드 네이티브의 보안 관련 지식을 가지고 있는지를 확인합니다. 이 역시 위 그림에서 출제 범위와 출제 비중을 확인하실 수 있습니다.\nOverview of Cloud Native Security\n클라우드 네이티브 관점에서 보안이 무엇이고, 관련 기술에는 어떤 것이 있는지 알고 있는 것이 중요합니다. 주요 키워드는 아래와 같습니다.\n\nCloud Native Security Layer - 4C\nCIS Benchmark\n\nkube-bench\n\n\nMITRE ATT&amp;CK\nNetwork Policies\nRBAC (Role-Based Access Control)\n\nKubernetes Cluster Component Security\nKubernetes 클러스터 구성요소의 보안은 문제로 자주 나오는 주제입니다. 주요 키워드는 아래와 같습니다.\n\nkube-apiserver 접근 제어\nkubelet 접근 제어\netcd 접근 제어\n\nKubernetes Security Fundamentals\nKubernetes에서 보안을 위해 사용되는 기능 역시 출제 비중이 높은 주제입니다. 주요 키워드는 아래와 같습니다.\n\nPod Security Standards\nPod Security Admission\nService Account\nSecret Management\nControl Plane Isolation\nData Plane Isolation\nAuditing\n\nKubernetes Threat Model\nKubernetes의 보안 취약점이 될 수 있는 요소에 대해 물어보는 경우도 있습니다. 관련된 주요 키워드는 아래와 같습니다.\n\nTrust Boundary\nDenial of Service(DoS)\nPrivilege Escalation\n\nPlatform Security\n플랫폼 보안에 대한 주요 키워드는 다음과 같습니다.\n\nSupply Chain Security\nImage Repository\nObservability\nService Mesh\nAdmission Control\n\nCompliance and Security Frameworks\n보안 준수 관련 표준에 대한 문제도 출제 가능성이 있습니다. 주요 키워드는 아래와 같습니다.\n\nNVD(National Vulnerability Database)\nSTRIDE\nCNCF Supply Chain Security\nKubescape\n\nReferences\n\nfaun.pub/how-to-ace-kcna-kubernetes-and-cloud-native-associate-exam-ac7bceace1eb\nmedium.com/@wattsdave/kubernetes-cloud-native-security-associate-kcsa-study-notes-and-exam-prep-f4c8f84d1c4f\n"},"blog/Kubernetes-v1.31-릴리즈!---주요-업데이트-소개":{"title":"Kubernetes 1.31 릴리즈! - 주요 업데이트 소개","links":[],"tags":["Kubernetes"],"content":"지난 8월에 Kubernetes v1.31이 릴리즈되었습니다. 프로젝트 네임은 Elli인데요.\nKubernetes 10주년 이후 처음 릴리즈되는 이번 버전에서도 다양한 기능이 안정화되고 새로운 기능도 추가되었습니다.\n\n그중 아래 주요 업데이트에 대해 소개해드리겠습니다.\n\nAppArmor 지원 기능 Stable 단계로 상향\nPersistentVolume의 lastTransitionTime Stable 단계로 상향\n이미지 볼륨 지원 기능 추가 (Alpha 단계)\n\n🛡️AppArmor 지원 기능 Stable 단계로 상향\nAppArmor는 리눅스 서버를 보호하기 위해 특정 애플리케이션이 네트워크나 파일 등에 접근하는 것을 제한하는 보안 모듈입니다. Ubuntu나 SUSE 등 많은 리눅스 배포판에서 AppArmor가 기본으로 적용되어 있는데요.\n특정 애플리케이션에 누군가 악의적으로 침투해서 동작 중인 리눅스 호스트로 접근하는 것을 막기 위해 AppArmor가 사용되는 거죠.\n\nAppArmor의 보호 대상으로서 접근이 제한되는 리소스를 정의한 것을 Profile이라고 하는데요. 리눅스 호스트에는 여러 개의 Profile을 정의해서 관리할 수도 있습니다.\n이전부터 AppArmor의 Profile을 Kubernetes 상에서 배포된 애플리케이션에도 적용할 수 있도록 securityContext의 appArmorProfile 필드를 지원하고 있었는데요.\n그동안 Beta 단계였던 AppArmor 지원 기능이 이번 Kubernetes 1.31 버전 릴리즈부터 Stable 단계로 상향되었고, 이제 Kubernetes에서 AppArmor를 안정적으로 지원한다고 볼 수 있습니다.\n만약 Kubernetes 클러스터에 배포하는 애플리케이션의 호스트 리소스 접근을 제한하고 싶다면 AppArmor를 고려해보는 건 어떠신가요?\n\n\n                  \n                  Info\n                  \n                \n\n참고로 Kubernetes v1.30 이전까진 Pod의 securityContext 필드가 아닌 annotations 필드에서 AppArmor Profile을 사용했습니다.\nKubernetes v1.30 이후부터 securityContext 필드에서 AppArmor Profile을 정의하도록 지원하기 시작했답니다.\n\n\n⏱️PersistentVolume의 lastTransitionTime 기능 Stable 단계로 상향\nPersistent Volume(PV)은 Pod에서 로컬이나 외부 스토리지를 부분적으로 사용할 수 있도록 만들어진 Kubernetes 리소스입니다. PV는 아래 3가지 중 하나의 상태를 가집니다.\n\nPending: PV와 Pod가 연동되기 위해 준비하는 상태\nBound: PV와 Pod가 연동된 상태\nReleased: Pod가 종료되거나 기타 이유로 PV를 사용하지 않아 Pod와 PV간의 연동이 끊어진 상태\n\n\nKubernetes v1.28부터 이런 PV의 상태가 변경되는 시점을 기록하는 lastTransitionTime 필드가 도입되었는데요. lastTransitionTime 기능을 사용하면 PV가 상태를 변경할 때마다 업데이트되어 PV의 최근 상태 변경 시점을 알 수 있게 됩니다.\n그렇기 때문에 아래와 같이 활용 가능합니다.\n\nRetention(데이터 보존 및 삭제) 정책 구현\n\n예: 특정 기간 동안 계속 Released 상태인 PV 삭제\n\n\n스토리지의 Health 모니터링\n\n예: 비정상적으로 긴 시간 동안 Pending 상태인 PV 감지 및 조치\n\n\n\n만약 PV 관리와 관련된 정책이나 모니터링이 필요하다면 PV의 lastTransitionTime 필드가 도움이 될 것입니다.\n💿이미지 볼륨 지원 기능 추가 (Alpha 단계)\nv1.31 버전부터 Alpha 단계로 새로 추가된 기능도 다양한데요. 그 중 이번에 소개드릴 기능은 이미지 볼륨 지원 기능입니다.\n이미지 볼륨 지원 기능이란, Pod가 PV로 일반적인 스토리지를 사용하는 것처럼 Container Image를 볼륨으로 사용할 수 있도록 지원함을 의미합니다.\n\n이미지 볼륨을 사용하는 Pod의 Yaml의 예시는 아래와 같습니다.\nkind: Pod\nspec:\n  containers:\n    - …\n      volumeMounts:\n        - name: my-volume\n          mountPath: /path/to/directory\n  volumes:\n    - name: my-volume\n      image:\n        reference: my-image:tag\n이 기능은 특히 인공지능 개발 분야에서 활용될 것으로 기대된다고 하는데요. 인공지능 모델 서버 컨테이너에 모델 Weight가 담긴 이미지를 볼륨 마운트한다면, 인공지능 모델을 서비스하는 이미지와 모델이 저장된 이미지를 분리해서 운용할 수 있기 때문에 효율적인 배포가 가능할 것이라고 합니다.\n그 외에도 여러 컨테이너에 공용으로 필요한 데이터가 담긴 이미지를 볼륨으로 마운트해서 보안 사고 가능성과 전체적인 이미지 크기를 함께 줄이는 등, 다양하게 활용할 수 있을 것으로 보입니다.\nReferences\n\nkubernetes.io/blog/2024/08/13/kubernetes-v1-31-release/\nkubernetes.io/docs/concepts/security/linux-kernel-security-constraints/#apparmor\nkubernetes.io/blog/2023/10/23/persistent-volume-last-phase-transition-time/\nkubernetes.io/blog/2024/08/16/kubernetes-1-31-image-volume-source/\n"},"blog/Kubernetes-컨테이너-디자인-패턴":{"title":"Kubernetes 컨테이너 디자인 패턴 소개","links":[],"tags":["Kubernetes"],"content":"☁️Kubernetes 컨테이너 디자인 패턴이란?\n디자인 패턴은 소프트웨어를 디자인하는 과정에서 자주 발생하는 문제들에 대한 일반적인 해결책을 의미합니다. 소프트웨어 개발 중에 자주 마주치는 문제를 어떻게 해결할 수 있을지 유형별로 정리된 디자인 패턴은, 특히 객체 지향 프로그래밍을 위해 고안된 것이 일반적인데요.\n전통적인 소프트웨어 개발뿐만 아니라, 컨테이너 형태로 애플리케이션을 배포할 때 사용할 수 있는 디자인 패턴도 있다는 사실, 알고 계셨나요?\n그래서 오늘은, Kubernetes에서 컨테이너를 배포할 때 활용 가능한 대표적인 컨테이너 디자인 패턴 4가지를 알아보겠습니다.\n🔎컨테이너 디자인 패턴 알아보기\n사이드카(Sidecar) 패턴\n\n사이드카 패턴은 기존에 존재하던 컨테이너 애플리케이션의 코드를 수정하지 않으면서도, 해당 애플리케이션의 로그를 기록하거나 모니터링과 같은 기능을 별도로 추가하고 싶을 때 사용하는데요.\nKubernetes의 Pod 내부에 기존 컨테이너 외로 필요한 기능을 수행하는 컨테이너를 추가하는 방식으로 사이드카 패턴을 구현할 수 있습니다.\n예를 들어, 웹서버 애플리케이션 컨테이너가 동작 중인 Pod 내에 로그 수집기 fluentd를 별도 컨테이너로 추가한 다음, 기존에 동작하던 웹서버의 로그를 fluentd 컨테이너에서 수집하는 식이죠.\n앰배서더(Ambassador) 패턴\n\n앰배서더 패턴은 Kubernetes에 배포된 컨테이너의 네트워크 연결을 전담하는 별도의 컨테이너를 두는 패턴인데요. 이 패턴을 적용하면, 복잡한 네트워크 통신과 관련된 기능은 별도의 컨테이너로 빼내고, 기존 컨테이너는 애플리케이션이 본래 수행해야 하는 핵심 기능에만 집중할 수 있다는 장점이 있습니다.\n그래서 앰배서더 패턴으로 추가한 컨테이너를 프록시(Proxy) 컨테이너라고도 하는데요. 외부 환경과 내부 서버 사이에서 통신이 오고갈 수 있는 가교 역할을 하는 프록시 서버와 비슷하게 동작해서 그렇습니다.\nKubernetes 환경에서 앰배서더 패턴 구현이 가능한 이유는, 동일한 Pod 내에서 동작하는 컨테이너는 서로 Localhost로 통신할 수 있기 때문입니다.\nHTTP 웹서버로 자주 사용되는 Nginx는 앰배서더 패턴으로 추가하하기 좋은 애플리케이션 중 하나입니다.\n어댑터(Adapter) 패턴\n\n외부와의 연결을 별도의 컨테이너에서 전담하여 간소화시키는 앰배서더 패턴과는 달리, 어댑터 패턴은 기존 컨테이너가 외부로 보내는 결과물을 통일화하는 데에 활용됩니다.\n특히 어댑터 패턴은 모니터링을 위해 외부 서비스와 연동할 때 외부 서비스에서 요구하는 양식이나 인터페이스를 맞추기 위해 사용되는데요. 기존의 컨테이너는 그대로 두면서, 컨테이너에서 나오는 결과물을 외부 규격에 맞추는 별도의 컨테이너를 동일한 Pod 내에 두는 방식으로 어댑터 패턴을 구현할 수 있습니다.\n만약 컨테이너로 동작 중인 로그 데이터 저장소 Elasticsearch의 상태와 관련된 로그를 외부 모니터링 툴 Prometheus 서비스에 저장해야 한다면, 동일한 Pod에 별도의 Prometheus Exporter 컨테이너를 추가하여 Prometheus 규격에 맞는 Elasticsearch의 상태 로그를 내보낼 수 있는 것이죠.\n초기화 컨테이너(Init Container) 패턴\n\n초기화 컨테이너 패턴은 기존 컨테이너가 실행되기 전에 필요한 작업을 별도의 컨테이너에서 수행하는 디자인 패턴입니다.\nKubernetes에선 Pod의 스펙을 정의하는 Manifest에서 initContainers라는 필드로 초기화 컨테이너 패턴을 정식으로 지원하고 있는데요.\n예를 들어 기존 컨테이너의 Elasticsearch 애플리케이션이 동작하려면 /usr/share 내 폴더의 소유권을 다른 사용자로 바꿔야 하는 경우, 별도의 초기화 컨테이너에서 해당 작업을 수행한 다음 Elasticsearch 컨테이너를 실행할 수 있는 겁니다.\nReferences\n\nmedium.com/@bijit211987/container-design-patterns-for-kubernetes-3742fca51b19\nitnext.io/4-container-design-patterns-for-kubernetes-a8593028b4cd\nstatic.googleusercontent.com/media/research.google.com//en/pubs/archive/45406.pdf\n"},"blog/Kubernetes의-경량화-버전-비교---minikube-vs-k3s-vs-k0s":{"title":"Kubernetes 클러스터를 쉽고 가볍게 구축할 수 있는 경량화 버전: minikube, k3s, k0s","links":[],"tags":["Kubernetes"],"content":"Kubernetes 경량화 버전이란?\n\nKubernetes는 컨테이너를 배포 및 관리할 수 있는 강력한 플랫폼입니다. 하지만 처음부터 Kubernetes 클러스터를 구축하려면 설치 과정이 복잡하고, 클러스터 구축에 많은 리소스가 요구된다는 단점이 있습니다.\n이런 어려움을 해결하기 위해, 클러스터를 가볍고 편리하게 구축할 수 있도록 도와주는 Kubernetes의 경량화 버전들이 많이 나오고 있는데요.\n오늘은 이러한 Kubernetes 경량화 버전인 minikube, K3s, k0s에 대해 살펴보겠습니다.\n✨다양한 운영체제에서 Kubernetes 테스트 환경을 쉽게 구축 가능한 minikube\n\nminikube는 macOS, Linux, Windows 로컬 환경에 Kubernetes 클러스터를 쉽게 구축하도록 도와주는 툴입니다.\nminikube는 기본적으로 Docker와 같은 컨테이너 런타임을 이용해서 클러스터 구성요소를 로컬에 배포 후 작동시키는데요.\n최신 Kubernetes 릴리즈 버전을 꾸준히 지원하기 때문에 새로운 Kubernetes 기능을 테스트하고 싶을 때 유용하게 활용할 수 있습니다.\nminikube 설치를 완료하면 CLI 환경에서 간단한 명령어로 Kubernetes 클러스터를 시작할 수 있고 직관적인 아이콘들로 구축 상황을 알려주는 등, 사용자가 편리하도록 신경을 많이 썼다는 느낌을 받을 수 있는데요.\nminikube는 macOS, Linux, Windows 운영체제를 모두 지원하는 Cross-platform인 것도 장점입니다.\n\n다만, 이번에 살펴볼 경량화 버전 중에선 가장 많은 리소스(2 CPU / 2GB RAM)가 필요하고 Docker나 Hyper-V와 같은 컨테이너 툴 혹은 가상 머신이 먼저 준비되어 있어야 한다는 단점도 존재합니다.\nminikube는 Kubernetes를 학습하시는 분들에게 실습 환경으로 추천해드릴 수 있는 툴인데요. 물론 Kubernetes 관련 기능이나 애플리케이션을 테스트하는 환경으로 사용하기에도 좋습니다.\nminikube 설치 안내 페이지: 링크\n💫더 가벼운 Kubernetes 클러스터를 구축해주는 K3S와 K0S\n\n다음은 Kubernetes의 경량화를 지향하는 K3S와 K0S입니다.\nKubernetes를 줄여서 일컫는 k8s와 비교해보면, 이 프로젝트들의 이름(K3S, K0S)에서 얼마나 가벼움을 지향하고 있는지 느낄 수 있는데요.\n이 두 프로젝트들은 모두 Kubernetes 클러스터를 구축하는 데에 필수적인 구성요소들을 간추려서 단일 파일로 설치 및 실행하기 때문에 리소스 경량화와 클러스터 구축 간소화를 모두 얻을 수 있었습니다.\n기존 Kubernetes는 클러스터의 데이터 저장소로 etcd를 사용하는데요. K3S와 K0S는 경량화를 위해 etcd보다 더 가벼운 sqlite3를 기본 데이터 저장소로 사용합니다. 물론 etcd나 MySQL 같은 다른 저장소도 지원하기 때문에, 필요한 경우라면 사용할 수도 있습니다.\n이렇게 다양한 방법으로 경량화를 꾀한 K3S와 K0S의 최소 요구사항은 아래와 같습니다.\n\nK3S: 1 CPU / 512MB RAM\nK0S: 1 CPU / 1GB RAM\n\n두 프로젝트 모두 minikube의 최소 요구사항의 절반 수준이죠.\n그래서 K3S와 K0S 모두 테스트 및 배포 환경에도 적합하지만, 적은 리소스 사용량 덕에 IoT 환경에도 사용하기 적합하다고 하는데요.\n다만 두 프로젝트 모두 단일 파일로 클러스터 구성요소를 작동시키는 방식이기 때문에, 현재 Linux 계열 운영체제만 공식 지원하고 있습니다.\nK3S와 K0S는 Linux 환경에 익숙하시면서 Kubernetes 개발 또는 테스트 환경이 필요하신 분들에게 추천해드릴 수 있는 경량화 버전입니다.\nK3S 설치 안내 페이지: 링크\nK0S 설치 안내 페이지: 링크\nKubernetes를 개인적으로 실습하거나 테스트하기 위해 클러스터를 직접 구축하기엔 준비해야 할 것들이 많아서 쉽지 않을 수 있는데요.\n그럴 땐 오늘 소개해드린 경량화 버전을 한번 고려해보시는 것도 좋을 듯합니다.\nReferences\n\nalperenbayramoglu2.medium.com/simple-comparison-of-lightweight-k8s-implementations-7c07c4e6e95f\nminikube.sigs.k8s.io/docs\ndocs.k3s.io/\ndocs.k0sproject.io/head/\nmedium.com/@thakur.ajay/kubernets-vs-k3s-vs-k0s-32f1da81a306\n"},"blog/LLMOps를-위한-오픈소스-플랫폼-Dify":{"title":"LLMOps를 위한 오픈소스 플랫폼 Dify 알아보기","links":["blog/LLMOps에-대해-알아보기"],"tags":["LLMOps"],"content":"🛠️LLMOps와 오픈소스 툴 Dify\nLLMOps는 우리가 지난 글에서 알아본 것처럼, 대규모 언어 모델(LLM)을 효율적으로 개발하고 운영하기 위한 방법론과 프로세스를 의미하는데요.\n이런 LLMOps를 위한 오픈소스 LLM 애플리케이션 개발 플랫폼 Dify에 대해 소개해드리고자 합니다.\n\nDify는 AI 워크플로우, RAG 파이프라인, LLM 관리, 모니터링 등이 가능해서 LLM 애플리케이션을 빠르게 프로덕션 환경에 배포할 수 있도록 도와준다고 합니다.\n✨Dify의 장점은?\n그렇다면 다양한 LLM 개발 플랫폼 중 우리가 Dify를 고려해야 할 이유는 무엇일까요?\n셀프 호스팅\nDify는 다른 클라우드 기반 LLM 개발 플랫폼과는 달리 오픈소스로 개발되고있기 때문에 셀프 호스팅이 가능합니다. 즉, 직접 제어할 수 있는 서버에 배포하여 웹으로 접근 가능하기 때문에 보안에 민감한 데이터를 기반으로도 더욱 안전한 환경에서 LLM 애플리케이션을 개발하고 배포할 수 있는 것이죠.\n확장성\n또한 Dify는 다양한 모델 제공자를 지원하고 있습니다.\n이미 대중적인 OpenAI부터 Azure나 Google Cloud뿐만 아니라, Ollama 같은 로컬 LLM 제공자도 지원하고 있는데요.\n이외에도 수십가지의 모델 제공자를 지원하고 있으며, 자세한 정보는 이 페이지에서 확인하실 수 있습니다.\nWeb UI 기반\nDify는 보기 쉬운 Web UI로 LLM 애플리케이션 개발과 관리가 가능하다는 장점 또한 가지고 있습니다.\n아래에서도 직접 확인해보겠지만, 이런 직관적인 UI 덕분에 LLM 애플리케이션 개발 및 관리가 한 층 더 쉬워진다는 장점이 있습니다.\n💻Dify를 로컬에 배포하고 접속해보기\nDify에 대해 알아봤으니 이제 Dify를 직접 사용해봐야겠죠?\n공식 Github 레파지토리에 업로드된 Docker Compose 파일을 이용하면 Dify 구동에 필요한 모든 구성요소를 아래와 같이 로컬 환경에 쉽게 배포할 수 있습니다.\nDify 소스 코드 clone\n먼저 아래 명령어로 로컬에 Dify 소스코드를 가져옵니다.\ngit clone github.com/langgenius/dify.git\nDify 실행\n아래 명령어로 Dify 구동에 필요한 환경변수 파일을 생성하고 Docker compose로 실행합니다.\ncd dify/docker\ncp .env.example .env\ndocker compose up -d\nDify를 Docker compose로 배포한 결과는 아래와 같습니다.\n\nDocker compose 명령어로 Dify 관련 컨테이너를 확인해보면 아래와 같죠.\n\n이렇게 배포된 컨테이너 중 주요 서비스 3개는 다음과 같습니다.\n\napi\nworker\nweb\n\n그 외 디펜던시 구성요소는 아래와 같습니다.\n\nweaviate\ndb\nredis\nnginx\nssrf_proxy\nsandbox\n\n이렇게 배포된 Dify의 구성요소 중 nginx 컨테이너가 배포되었기 때문에, 웹 브라우저에 localhost 경로로 Dify Web UI에 접속이 가능합니다.\n\n관리자 계정 설정 후 로그인을 완료하면 아래처럼 로컬 환경에서 Dify 플랫폼을 이용할 수 있게 되는 거죠.\n\n✅Dify 활용하기\n그렇다면 Dify를 어떻게 활용할 수 있을까요? 아래와 같이 Dify의 활용 방안을 간략히 정리해봤습니다.\n로컬 LLM을 연동하여 로컬 LLM 애플리케이션 개발\n아래처럼 Dify는 다양한 모델 제공자와 연동할 수 있는데, 그 중 Ollama를 선택하면 로컬에서 구동 중인 LLM과 연동하여 애플리케이션 개발이 가능합니다.\n조직 특성상 보안에 민감하거나 규모가 크지 않은 팀에서 활용하기 적절한 방식입니다.\n\nDify에서 기본 제공하는 템플릿 활용하여 LLM 애플리케이션 개발\nDify는 아래 사진처럼 다양한 LLM 애플리케이션 개발용 템플릿을 제공하는데요.\n\n이 중 원하는 템플릿을 선택하면 아래 이미지와 같이 해당 애플리케이션 개발을 위한 스튜디오 화면과 테스트 창이 표시되어 LLM 애플리케이션을 빠르게 개발 및 테스트가 가능합니다.\n\nLLM 애플리케이션 개발을 위한 컨텍스트 생성\n개발하려는 LLM 애플리케이션에 주입할 텍스트 데이터 등의 컨텍스트를 아래와 같이 Web UI에서 생성할 수 있습니다.\n\n컨텍스트로 저장 가능한 파일 포맷으로는 TXT, markdown, PDF, XLSX, CSV 등이 있습니다.\n\nReferences\n\ngithub.com/langgenius/dify\n"},"blog/LLMOps에-대해-알아보기":{"title":"LLMOps에 대해 알아보기","links":[],"tags":["LLMOps"],"content":"요즘 ChatGPT의 인기 덕분에 LLM(Large Language Model)이 큰 관심을 받고 있습니다.\n혹시 LLMOps라는 컨셉에 대해 들어보셨나요?\n이미 많이 알고계신 DevOps(Development + Operations)와 같은 원리로 LLM과 Operations가 합쳐진 용어인데요.\nLLMOps는 LLM의 전체 라이프 사이클 주기 동안 모델을 효율적으로 개발, 배포, 관리할 수 있도록 특화된 방법론 및 프로세스를 의미합니다.\nLLMOps가 필요한 이유\n그렇다면 LLMOps가 왜 필요할까요? 가장 큰 이유는 보안 때문입니다.\n어떤 기업의 직원이 외부의 LLM 제공업체의 서비스를 이용한다고 가정해보겠습니다. 이 직원은 자신의 업무 수행에 도움을 받기 위해 외부 LLM 서비스에 이것저것 질문을 할 텐데요.\n이때 회사 내부 정보도 질문에 포함될 가능성은 충분히 있겠죠. 게다가 이런 질문들은 LLM 제공업체의 서버로 전송되기 때문에, 기업 입장에선 외부 LLM 서비스를 이용하기 어렵습니다.\n이런 이유로 기업 내부에서 자체 LLM 서비스를 운영 및 관리하는 경우가 생기기 시작합니다. 그리고 효율적이면서 고도화된 LLM 학습, 운영, 관리를 위해 LLMOps가 필요하게 되는 것이죠.\nLLMOps의 동작 방식\nLLMOps 주기는 아래와 같이 나눌 수 있습니다.\n\nFM(Foundation Model) 선정\nUse Case에 맞춰 적용\n평가(Evaluation)\n배포\n모니터링\n\n그리고 LLMOps의 Workflow를 도식화하면 아래 그림처럼 표현할 수 있죠.\n\nMLOps와 LLMOps의 차이점\nLLMOps가 나오기 전에 이미 MLOps라는 용어가 있었습니다. 머신러닝 모델의 학습, 개발, 관리를 위해 생겨난 개념인데요.\n두 개념 모두 AI 모델의 전체 개발 주기를 대상으로 한다는 공통점이 있지만, 차이점도 있습니다. LLMOps를 좀 더 잘 이해할 수 있도록 MLOps와의 차이점을 살펴보겠습니다.\n먼저 LLMOps에서는 FM(Foundation Model)을 사용합니다. 대규모 학습이 필요한 LLM의 특성상, AI 모델을 처음부터 새로 만들고 학습시키려면 엄청난 규모의 자원이 소모되기 때문입니다.\n\nFM은 기반 모델이란 뜻으로, 다양한 도메인의 대규모 데이터셋으로 미리 학습된 모델을 의미합니다.\n참고로 이렇게 FM을 기반으로 AI 모델을 학습/운영하는 접근법은 FMOps라고 하며, 현재 FMOps와 LLMOps라는 용어는 엄격히 구분하지 않고 혼용됩니다.\n\n또한 LLMOps 관점에선 사용자의 피드백이 성능 개선에 중요하므로, 피드백 데이터가 모델에 반영될 수 있는 파이프라인 설계가 추가로 필요하다는 차이점도 있습니다.\nReferences\n\nLLMOps가 주목받고 있는 이유\nwww.ibm.com/topics/llmops\nubiops.com/llmops-vs-mlops\nFMOps and LLMOps: Operationalize Generative AI at Scale\n"},"blog/Liveness,-Readiness,-Startup-Probe-소개-및-비교":{"title":"Liveness, Readiness, Startup Probe 비교","links":[],"tags":["Kubernetes"],"content":"🔎Kubernetes의 Container Probe란?\n\nKubernetes에서 Pod를 배포하면 Pod에서 정의한 컨테이너가 실행되는데요. 컨테이너 내부에서 실행되어야 하는 프로세스가 정상 작동할 때 비로소 Pod를 통해 원하는 서비스를 이용할 수 있게 됩니다.\nPod를 배포하고 운영하다보면 동작 중이던 컨테이너의 상태가 정상인지 주기적으로 확인이 필요할 때가 있습니다. 혹은 해당 컨테이너가 외부 트래픽을 받을 준비가 되었는지 알아야 할 때도 있죠.\n이렇게 컨테이너의 상태를 주기적으로 진단할 때 사용하는 것이 바로, Container Probe입니다.\n🩺Container Probe의 진단 유형과 Probe의 종류\nKubernetes의 Container Probe는 Manifest의 컨테이너 레벨에서 정의되는데요. 정의된 진단 설정에 따라 kubelet이 해당 컨테이너 내부에서 주기적으로 진단을 수행합니다.\nProbe는 아래와 같이 4가지 방법 중 하나로 컨테이너의 상태를 진단할 수 있습니다.\n\nexec\n\n컨테이너 내부에서 실행할 명령어 지정\n명령어 실행 후 상태 코드가 0이면 성공으로 진단\nexec는 수행될 때마다 컨테이너 내에 새로운 프로세스가 생성되기 때문에 Node의 CPU 사용량이 증가할 수 있으므로 주의가 필요\n\n\ngrpc\n\ngRPC 요청으로 진단하며, gRPC Health Check이 미리 구현되어 있어야 함\n응답 status가 SERVING이면 성공으로 진단\n\n\nhttpGet\n\n특정 port 및 path로 Pod의 IP에 대해 HTTP GET 요청으로 진단\n응답 status code가 200 이상 400 미만이면 성공으로 진단\n\n\ntcpSocket\n\n특정 port로 Pod의 IP에 대해 TCP 요청으로 진단\n해당 port가 열려 있으면 성공으로 진단\n\n\n\n이렇게 수행한 진단 결과는 아래 3가지 중 하나로 나옵니다.\n\nSuccess: 진단 성공\nFailure: 진단 실패\nUnknown: 진단 실패, Faliure와 달리 kubelet이 추가 진단 실행\n\nkubelet이 수행하는 Probe는 아래와 같이 3가지 유형으로 나뉘는데요.\n\nReadiness Probe\nLiveness Probe\nStartup Probe\n\n각 Probe에 대해 아래에서 더 자세히 알아보겠습니다.\n✨Readiness, Liveness, Startup Probe\nReadiness Probe\nReadiness Probe는 트래픽과 관련이 있는데요. Readiness Probe를 사용하면 해당 컨테이너로 트래픽이 들어오는 시점을 제어할 수 있기 때문입니다.\n즉, 컨테이너의 상태 진단 결과가 성공인 시점부터 해당 컨테이너에 트래픽이 들어오는 것을 허용하고 싶을 때 Readiness Probe를 사용하는 것이죠.\n각 Probe는 컨테이너 레벨에서 정의한다고 했는데요. 아래 Pod Manifest 예제로 Probe를 어떻게 정의하는지 알아보겠습니다.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: goproxy\n  labels:\n    app: goproxy\nspec:\n  containers:\n  - name: goproxy\n    image: registry.k8s.io/goproxy:0.1\n    ports:\n    - containerPort: 8080\n    readinessProbe:  # Readiness Probe를 정의하는 부분입니다.\n      httpGet:\n\t    path: /healthz\n        port: 8080\n      initialDelaySeconds: 15\n      periodSeconds: 10\n위 예제에서 Readiness Probe를 정의하는 부분만 살펴보면 해당 Probe의 작동 방식은 아래와 같습니다.\n\n위 Readiness Probe는 HTTP GET 요청으로 진단 수행 (httpGet)\nHTTP GET 요청을 보내는 Port와 경로는 8080 및  /healthz\nkubelet이 첫 Probe 진단을 수행되는 시점은 컨테이너 실행 후 15초 뒤 (initialDelaySeconds)\nkubelet은 10초 간격으로 Probe 진단 수행 (periodSeconds)\n\n위 예제에선 컨테이너에 readinessProbe 하나만 정의되었지만, 필요에 따라 컨테이너에 대해 Probe를 종류별로 모두 정의할 수도 있습니다.\nLiveness Probe\nLiveness Probe는 동작 중인 컨테이너의 상태를 주기적으로 진단할 때 사용됩니다.\nKubernetes에 배포된 컨테이너의 프로세스에 이슈가 생기거나 프로세스의 상태가 Unhealthy인 경우, kubelet이 이를 감지하여 자동으로 Pod의 restartPolicy(재시작 정책)에 따라 조치를 취하기 때문에 Liveness Probe가 반드시 필요한 것은 아닙니다.\n하지만 Probe의 컨테이너 상태 진단 결과에 따라 종료 또는 재시작이 필요한 경우라면 Liveness Probe를 사용할 수 있습니다.\n위 예제 코드에서 봤던 것처럼, 모든 Probe는 spec.containers 레벨에서 정의할 수 있는데요. 위 코드에 exec를 수행하는 Liveness Probe를 추가로 정의한다면 아래와 같겠습니다.\nspec:\n  containers:\n  - name: goproxy\n    image: registry.k8s.io/goproxy:0.1\n    ports:\n    - containerPort: 8080\n    readinessProbe:  # Readiness Probe를 정의하는 부분입니다.\n      httpGet:\n\t    path: /healthz\n        port: 8080\n      initialDelaySeconds: 15\n      periodSeconds: 10\n    livenessProbe:  # 동일한 컨테이너에 대해 Liveness Probe를 추가로 정의했습니다.\n      exec:\n        command:\n\t\t- cat\n\t\t- /tmp/healthy\n      initialDelaySeconds: 5\n      periodSeconds: 5\n위 예제처럼 Liveness Probe와 Readiness Probe를 동일한 컨테이너에 대해 함께 사용할 경우, 아래와 같이 각 Probe의 역할 수행을 기대할 수 있습니다.\n\nReadiness Probe로 해당 컨테이너가 준비될 때까지 들어오는 트래픽 제어\nLiveness Probe로 해당 컨테이너 동작 중에 진단 실패 시 자동으로 재시작\n\nStartup Probe\n마지막으로 Startup Probe는, 그 이름처럼 컨테이너가 정상적으로 시작했는지 여부를 진단합니다.\n가동하는 데에 시간이 오래 걸리는 컨테이너의 상태를 진단해야 하는 경우라면, Liveness Probe의 periodSeconds 값을 길게 설정해서 진단 주기를 넓히기보다는 Startup Probe를 정의해서 컨테이너의 시작 성공 여부를 파악하는 것이 좋은데요.\n그래야 Liveness Probe로는 동작 중인 컨테이너의 상태를 적절한 주기로 진단할 수 있기 때문입니다.\nStartup Probe는 아래와 같이 정의할 수 있습니다.\nstartupProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  failureThreshold: 30\n  periodSeconds: 10\n위 예제를 보면 failureThreshold(최대 실패 허용 횟수) 값이 설정되어 있는데요.\n해당 Startup Probe는 최대 300초(30 * 10) 동안 주기적으로 컨테이너가 정상적으로 실행했는지를 진단하고, 300초가 지나도록 진단에 실패하면 Pod의 restartPolicy(재시작 정책)에 따라 컨테이너를 재시작하거나 종료하게 됩니다.\n한 가지 참고할 점은, Startup Probe의 진단이 성공하기 전까진 Readiness Probe와 Liveness Probe가 실행되지 않다는 것입니다.\n이렇게 각 Probe는 수행 시점과 역할이 조금씩 다르기 때문에, 컨테이너의 성격이나 운영 정책에 따라 적절한 조합으로 Probe를 사용한다면 더욱 효과적으로 Pod를 배포하고 운영할 수 있습니다.\nReferences\n\n# Configure Liveness, Readiness and Startup Probes\n# Pod Lifecycle\n"},"blog/Service-Mesh-소개":{"title":"Service Mesh 소개","links":["📆-Project/Guide-to-DevOps/KO/Istio-소개"],"tags":["Network"],"content":"\n클라우드 개발 환경에서 종종 Service Mesh란 키워드를 들으신 적이 있을 수 있습니다. Service Mesh라고 하면 네트워크와 관련된 것 같은 느낌이 있는데요.\n이름에 담긴 느낌처럼, Service Mesh의 간략한 정의는 ‘애플리케이션의 서비스 간 모든 통신을 처리하는 소프트웨어 계층’이라고 할 수 있습니다.\nService Mesh를 도입하는 이유\n그렇다면 Service Mesh를 도입해서 서비스 간 모든 통신을 처리하고 관리해야 할 이유는 무엇일까요? 이는 아래와 같이 정리할 수 있습니다.\n\n가시성\n\nMSA 환경에서 서비스 간에 주고받는 트래픽에 대한 로그를 얻을 수 있으므로, 애플리케이션 외부에서 일어나는 이벤트의 가시성을 확보 가능\n\n\n트래픽 관리\n\n서비스 간에 주고받는 트래픽을 독립된 레이어에서 제어 가능\n\n\n보안성\n\n상호 TLS(mTLS) 설정으로 서비스간 통신에서 검증이 가능\n별도의 Policy를 적용하여 서비스간 통신 가능 여부를 제어할 수 있고, 이를 코드로 관리 가능\n\n\n\nService Mesh가 동작하는 방식\n그렇다면 Service Mesh는 어떻게 동작할까요? Service Mesh의 두 구성 요소인 데이터 플레인(Data Plane)과 컨트롤 플레인(Control Plane)으로 나눠서 알아보겠습니다.\n\n데이터 플레인\n\nService Mesh의 데이터 처리 역할 수행\n각 서비스에 Service Mesh를 위한 별도의 네트워크 프록시가 Sidecar로 추가되며, 해당 프록시는 서비스로 들어오고 나가는 모든 트래픽이 거치는 중간 게이트 역할\n서비스 간의 통신이 발생할 때 동작 과정\n\nSidecar로 추가되었던 네트워크 프록시가 요청을 받음\n받은 요청을 별도의 네트워크 연결로 캡슐화\n출발 프록시와 도착 프록시 간의 안전하고 암호화된 채널 설정\n\n\n이외에도 로드 밸런싱, Circuit Breaker, 트래픽 라우팅과 같은 역할도 수행\n\n\n컨트롤 플레인\n\nService Mesh의 중앙 관리 및 구성 계층 역할 수행\n서비스 간의 라우팅 규칙, 로드 밸런싱 정책, 보안 설정 등이 가능한 영역\n이외에도 Mesh 내 모든 서비스를 추적하는 레지스트리, Telemetry 데이터의 수집 및 집계 역할도 수행\n\n\n\nService Mesh를 구현하는 방법\n지금까지 알아본 Service Mesh는 어떻게 적용할 수 있을까요? 대표적인 Service Mesh 솔루션으로 Istio가 있습니다.\nIstio는 Kubernetes와 함께 작동하도록 설계되어 있어 호환성이 높으며, 이때 Istio의 네트워크 프록시가 Kubernetes의 Pod 내 Sidecar 컨테이너로 추가되는 방식으로 Service Mesh를 구현합니다.\nIstio와 관련된 더욱 자세한 글은 여기서 확인해보세요.\nReferences\n\nIstio는 무엇이고 왜 중요할까?\n서비스 메시란 무엇인가요?\n실무자를 위한 서비스 메시 - 지금 서비스 메시가 의미 있는 이유\n"},"blog/k8s-환경에서의-안정성을-위한-카오스-엔지니어링":{"title":"Kubernetes 환경에서의 안정성 테스트가 필요하다면? - 카오스 엔지니어링","links":[],"tags":["Cloud","Test"],"content":"🧪카오스 엔지니어링이란?\n\n카오스 엔지니어링이란, 개발 중인 시스템의 안정성을 테스트하기 위해 임의적으로 장애 상황을 조성하는 기법을 말합니다.\n그래서 혼란이란 뜻의 카오스(Chaos)가 이름에 붙은 것인데요.\n카오스 엔지니어링을 통해 시스템상에서의 취약점은 없는지 알 수 있기 때문에 시스템을 더욱 안정적으로 만들 수 있죠.\n☁️클라우드 네이티브 환경에서 카오스 엔지니어링이 필요한 이유\n\n클라우드 네이티브 환경은 다양한 컴포넌트가 유기적으로 연결되어 있어 그 자체로 복잡하기 때문에, 테스트하기 쉽지 않은데요.\n서비스의 부하 등을 측정하는 일반적인 테스트 툴로는 이런 분산 시스템 환경에서 효율적으로 테스트하기 어려우므로, 카오스 엔지니어링이라는 다른 접근법이 필요한 것입니다.\n카오스 엔지니어링은 시스템에 임의적인 장애 상황을 일으킨 다음, 시스템이 어떻게 반응하는지 확인하여 잠재적인 이슈를 잡아내고 사고를 미연에 방지할 수 있습니다.\n그렇다면 실제 카오스 엔지니어링은 어떻게 수행할 수 있을까요?\nCNCF 재단의 Incubating 프로젝트로 등록된 Chaos Mesh를 소개하며 알아보도록 하겠습니다.\n⛓️Kubernetes 카오스 엔지니어링에 특화된 Chaos Mesh\n\nChaos Mesh는 클라우드 네이티브 환경을 위한 오픈소스 카오스 엔지니어링 플랫폼입니다.\n다양한 종류의 장애 상황 시뮬레이션이 준비되어있기 때문에, 구성할 수 있는 장애 시나리오의 폭도 매우 넓다는 장점이 있습니다.\nChaos Mesh는 동작에 필요한 각 컴포넌트(contoller-manager, daemon, dashboard)가 Kubernetes 클러스터상에 CRD(Custom Resource Definition)로 배포되는 방식으로 설치하는데요.\n그래서 Helm을 사용하거나 Chaos Mesh가 자체로 작성한 쉘 스크립트를 실행해서 설치할 수 있습니다. 자세한 설치 정보는 공식 페이지에서 확인하실 수 있습니다.\nChaos Mesh 설치가 완료되면 Experiment라고 하는 테스트 객체를 생성해야 합니다. Chaos Mesh에는 기본적으로 다양한 장애 상황을 발생시킬 수 있는 리소스가 준비되어 있고, 이런 리소스를 Experiment로 사용하는데요.\nChaos Mesh에서 기본적으로 생성 가능한 대표적인 장애 발생 리소스는 아래와 같습니다.\n\nPodChaos: Pod 상태를 의도적으로 불안정하게 만들거나 Pod 내 특정 Container가 동작에 실패하는 상황 조성\nNetworkChaos: 네트워크 지연, 패킷 손실 등의 상황 조성\nStressChaos: CPU 또는 메모리에 대한 부하 조성\n\nExperiment는 k8s 리소스처럼 yaml 파일로 정의 후 kubectl로 실행할 수 있는데요.\n아래는 NetworkChaos로 만든 Experiment의 예시입니다. 이 Experiment를 실행하면 default 네임스페이스 내 &quot;app&quot;: &quot;test&quot; 라벨을 가진 Pod에 대해 12초 동안 10ms의 네트워크 지연을 유발시키는 테스트가 진행됩니다.\napiVersion: chaos-mesh.org/v1alpha1  \nkind: NetworkChaos  \nmetadata:  \n\tname: network-delay  \nspec:  \n\taction: delay\n\tmode: one\n\t\tnamespaces:  \n\t\t\t- default  \n\t\tlabelSelectors:  \n\t\t\t&#039;app&#039;: &#039;test&#039; \n\tdelay:  \n\t\tlatency: &#039;10ms&#039;  \n\t\tduration: &#039;12s&#039;\n또한 여러 개의 Experiment는 하나의 Workflow로 구성해서 관리할 수 있습니다. 즉 Workflow를 구성함으로써 전체적인 카오스 엔지니어링 프로세스를 정의하고 수행할 수 있는 거죠.\n그리고 Chaos Mesh의 기본 컴포넌트 중에는 Dashboard도 존재하는데요. Chaos Experiment를 직관적으로 관리하고 모니터링할 수 있어 테스트 시나리오를 관리하는 측면에서 유용할 것으로 보입니다.\n\nReferences\n\nchaos-mesh.org/docs/\nmedium.com/@seifeddinerajhi/chaos-engineering-on-kubernetes-a-beginners-guide-revel-in-chaos-0974ae9bee8b\n"},"blog/kube-bench-소개":{"title":"kube-bench 소개","links":["blog/CIS-Kubernetes-Benchmark"],"tags":["Kubernetes","Security"],"content":"kube-branch는 CIS Kubernetes Benchmark를 기반으로 k8s 환경이 보안적으로 안전한지 검토해주는 툴입니다.\nCIS Kubernetes Benchmark에 대해 궁금하시다면 여기서 확인해보세요.\nkube-bench를 사용하는 이유\nkube-bench는 공식 레파지토리에서 배포하는 Job yaml 파일 또는 Docker 이미지를 실행하는 것만으로도 CIS Kubernetes Benchmark 검토를 쉽게 수행할 수 있고, Benchmark 결과도 바로 확인할 수 있기 때문에 많이 사용됩니다.\nkube-bench 실행 방법\n컨테이너 이미지로 실행\n\nk8s 환경에서 아래 명령어로 kube-bench의 Docker 이미지를 실행합니다.\n\ndocker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t docker.io/aquasec/kube-bench:latest --version 1.28\n이때 --version은 현재 검토하려는 k8s 클러스터의 버전을 명시합니다.\n\n\n명령어를 실행하면 kube-bench의 공식 Docker 이미지를 통해 CIS Benchmark 검토가 진행되며, 완료 시 검토 결과를 바로 확인할 수 있습니다.\n\nk8s Job Object로 실행\n\nk8s 환경에서 kube-bench 공식 레파지토리에서 제공하는 Job yaml 파일을 가져옵니다. (파일 링크)\n가져온 yaml 파일을 이용해 kube-bench를 실행하는 k8s Job을 아래 명령어로 클러스터에 배포합니다.\n\nkubectl apply -f {가져온 yaml 파일 이름}.yaml\n\n\n배포한 Job 실행이 완료되었다면 아래 명령어로 Benchmark 검토 결과를 확인할 수 있습니다.\n\nkubectl logs {배포한 Job의 Pod 이름}\n\n\n\nkube-bench의 실행 결과\n\nkube-bench를 원하는 방법으로 실행하고나면, 위 이미지와 같은 검토 결과를 얻을 수 있습니다.\n각 카테고리 별로 CIS 보안 사항에 대한 검토 결과를 확인할 수 있으며, 현재 보안 사항이 준수되고 있는지의 여부(PASS/FAIL)와 보안 사항의 세부 설명에 대해서도 확인 가능합니다.\nReferences\n\ngithub.com/aquasecurity/kube-bench\n"},"blog/kubectl-플러그인-매니저-krew-사용해보기":{"title":"kubectl 플러그인 매니저 krew 사용해보기","links":[],"tags":["Kubernetes"],"content":"🔎krew란?\n\nKubernetes를 운영 및 관리할 때 가장 많이 사용하는 툴이라고 하면 역시 kubectl일 텐데요. kubectl 덕분에 우린 커맨드 창에서 Kubernetes의 각 Object를 배포, 관리, 테스트할 수 있습니다.\n이런 kubectl을 더욱 편리하고 효율적으로 사용할 수 있도록 도와주는 플러그인이 다양한 사람들에 의해 개발되고 있다는 사실, 알고 계셨나요?\nkrew는 이렇게 개발되는 kubectl 플러그인을 저장하고 배포하는 플러그인 매니저입니다.\n2024년 7월 기준 현재 264개의 kubectl 플러그인이 krew에 등록되어 있는데요.\nkrew를 사용하면 CLI상에서 kubectl 플러그인을 편리하게 검색하거나 로컬에 설치할 수 있습니다.\nmacOS, Linux, Windows 등 대부분의 운영체제에서도 지원되는 krew를 직접 사용해보고, 추천 플러그인도 소개해보겠습니다.\n🛠️krew 사용해보기\n로컬에서 krew를 사용하려면 git이 먼저 설치되어 있어야 합니다. 각 운영체제별 krew 설치 방법은 공식 링크에서 확인할 수 있습니다.\nkrew를 설치했다면 이제 직접 사용해볼 차례입니다. 먼저 아래 명령어로 최신 플러그인 리스트를 받아옵니다.\n$ kubectl krew update\n그리고 아래 명령어로 사용 가능한 플러그인을 탐색할 수 있습니다.\n$ kubectl krew search\n \nNAME                            DESCRIPTION                                         INSTALLED\naccess-matrix                   Show an RBAC access matrix for server resources     no\nadvise-psp                      Suggests PodSecurityPolicies for cluster.           no\nauth-proxy                      Authentication proxy to a pod or service            no\n[...]\n아래 명령어로 원하는 플러그인을 다운로드 가능합니다.\n$ kubectl krew install ctx # 예제에선 kubectx라는 플러그인을 다운로드합니다.\n설치한 플러그인의 최신 업데이트를 아래 명령어로 한 번에 설치할 수도 있습니다.\n$ kubectl krew upgrade\n더 이상 사용하지 않는 플러그인은 아래 명령어로 삭제할 수 있습니다.\n$ kubectl krew uninstall ctx # 예제에선 설치된 kubectx 플러그인을 삭제합니다.\nkrew는 kubectl 플러그인 매니저 툴이기 때문에 설치/사용 방법이 어렵진 않은데요. 이제 실제로 사용하기 좋은 kubectel 플러그인 3가지를 추천드리겠습니다.\n✨추천 kubectl 플러그인 소개\nkubectx\n가장 먼저 소개해드릴 추천 플러그인은 kubectx입니다. kubectx는 Kubernetes의 context object를 쉽게 변경할 수 있게 도와주는 플러그인인데요.\nKubernetes의 context란, 여러 Kubernetes 클러스터에 접근할 수 있도록 관련 config 값을 모아둔 Object를 말합니다.\n이런 context를 변경해서 다른 Kubernetes 클러스터에 접근하려면 기존에는 아래와 같은 과정을 거쳐야 했습니다.\n\nkubectl config get-contexts 명령어로 변경 가능한 context 이름 확인\nkubectl config use-context {context 이름} 명령어로 context 변경\n\n하지만 kubectx 플러그인을 사용하면 context를 간단한 CLI 명령어만으로 쉽게 변경 가능합니다.\nkubectx는 아래와 같이 설치할 수 있습니다.\n$ kubectl krew install ctx\n설치 완료 후 아래와 같은 명령어로 현재 context 확인 및 context 변경이 가능합니다.\n$ kubectl ctx\ncoffee\nminikube\ntest\n \n$ kubectl ctx coffee\nSwitched to context &quot;coffee&quot;\n \nneat\n다음으로 소개해드릴 kubectl 플러그인은 neat입니다.\nKubernetes 클러스터에 배포된 object의 Manifest를 참고하거나 가져오기 위해 kubectl get ... -o yaml 형태의 명령어를 사용할 때가 많은데요.\n하지만 이렇게 가져온 Manifest에는 아래 이미지와 같이 object 배포 당시 메타데이터나 현재 object의 상태와 관련된 데이터 등도 포함하고 있어서 가독성이 떨어집니다.\n\n이 때 neat 플러그인을 사용하면, 아래와 같이 object 배포에 필요없는 정보가 제외된 Manifest를 얻을 수 있습니다.\n\nneat는 아래와 같이 설치할 수 있습니다.\n$ kubectl krew install neat\nneat를 사용하는 가장 직관적인 방법은 파이프라인(|)을 사용하는 것입니다. 아래와 같이 기존 kubectl get 명령어에 kubectl neat 명령어를 연결하면 가독성이 더욱 좋은 Manifest를 얻을 수 있습니다.\n$ kubectl get ... -o yaml | kubectl neat\nkail\n마지막으로 소개해드릴 플러그인은 kail입니다. Kubernetes와 tail을 합친 이름을 지닌 kail은, Kubernetes의 다양한 Object에서 발생하는 로그를 실시간으로 보여주는 플러그인입니다.\nKubernetes 상에서 배포된 애플리케이션에 대한 로그를 확인하고 싶을 때 우린 kubectl logs {Object 종류}/{Object 이름} 명령어를 주로 사용합니다.\n하지만 이런 방식은 단일 Object에 대한 로그만 확인이 가능하며, 여러 Object의 로그를 한 번에 확인하는 등 좀 더 복잡한 로그 조회는 어렵죠.\nkail은 이런 문제를 해결해주는 플러그인입니다.\nkail은 아래와 같이 설치할 수 있습니다.\n$ kubectl krew install tail\nkail을 이용하면 Service, ReplicaSet, Deployment 등을 Argument로 필터링한 다음, 매칭되는 Pod의 로그를 한 번에 확인할 수 있습니다.\n예를 들어, frontend라는 이름의 Service와 webapp이라는 이름의 Deployment에 포함된 Pod의 로그를 확인하고 싶다면 아래와 같은 kail 명령어를 사용하면 되는 것이죠.\nk tail --svc frontend --deploy webapp\nkail을 이용하면 특정 Namespace와 Node에서 동작 중인 Pod의 로그도 한 번에 확인할 수 있습니다.\n예제를 위해 stress란 이름의 Namespace에 일정 주기로 테스트용 로그를 생성하는 nginx 컨테이너 및 cache 컨테이너가 포함된 Deployment가 배포했습니다.\n그리고 해당 Object들은 playground라는 이름의 Node 위에서 동작하고 있죠.\n이때 kail로 stress Namespace와 playground Node에서 동작 중인 모든 Pod의 로그를 조회하면 아래와 같이 표시됩니다.\n&lt;stress Namespce에서 동작하는 모든 Pod 로그 조회&gt;\n\n&lt;playground Node에서 동작하는 모든 Pod 로그 조회&gt;\n\nReferences\n\nkrew.sigs.k8s.io\ngithub.com/ahmetb/kubectx\ngithub.com/itaysk/kubectl-neat\ngithub.com/boz/kail\n"},"blog/ollama와-Open-WebUI-로컬-배포":{"title":"ollama와 Open-WebUI 로컬 배포","links":[],"tags":["Docker","Ollama"],"content":"ollama와 Open-WebUI\nLLM을 활용한 서비스가 다양하게 출시되는 요즘, 로컬에서 LLM을 사용할 수 있도록 도와주는 ollama이라는 툴에 관심이 생겼습니다.\n오픈 LLM 모델의 GGUF 파일이 있다면 ollama를 이용해 로컬 환경에서 LLM과 상호작용이 가능한데요.\n로컬 LLM은 개인 정보 유출 위험이 적고 비용 발생도 없다는 장점이 있습니다.\n그래서 제 노트북의 로컬 환경에 직접 ollama를 실행시킨 다음, 공개된 LLM 모델을 가져와 테스트를 진행해봤습니다.\n테스트를 진행한 노트북 사양은 아래와 같습니다.\n\nCPU: AMD Ryzen 7 4800H with Radeon Graphics 2.90 GHz\nRAM: 32 GB\n\nCLI 환경에서 동작하는 ollama를 보다 쉽게 사용하기 위해, Open-WebUI라는 툴을 함께 사용했는데요.\nOpen-WebUI는 Chat GPT와 유사한 UI를 가지고 있고, 호스트에 실행 중인 ollama와 연동되어 웹 브라우저상에서 LLM에 질문을 하거나 다양한 LLM 관련 설정도 가능합니다.\nDocker Compose를 활용하여 로컬 배포\nollama와 Open-WebUI 로컬 배포에 대해 조사해보니 모두 로컬에 직접 설치하는 글이 대부분이었지만, 각 툴이 호스트 환경으로부터 독립되어야 일관된 기능이 보장될 수 있으므로 우리는 Container 환경에서 실행해보도록 하겠습니다.\n다행히 ollama와 Open-WebUI 모두 공식 Container Image가 공개되어 있어서 Docker로 실행하는 데엔 어려움이 없겠는데요.\n하지만 로컬 LLM을 사용하고 종료할 때마다 이 툴들의 Container Image를 실행하고 다시 종료하려면 손이 많이 갈 것 같습니다.\n그래서 여러 Container를 한 번에 배포할 수 있는 Docker Compose를 활용하도록 하겠습니다.\nDocker Compose는 한 개 이상의 Container를 항상 동일한 옵션과 조건으로 한 번에 실행할 수 있도록 도와주는 기능입니다. Container 실행에 필요한 각종 정보를 compose.yaml이라는 파일에 정의해두었다가, docker compose 명령어를 실행하면 yaml 파일에 정의된 Container들이 실행되는 방식입니다.\n배포 과정\n먼저 아래와 같이 Docker Compose 파일을 정의합니다.\nservices:\n  openWebUI:\n    image: ghcr.io/open-webui/open-webui:main\n    restart: always\n    ports:\n      - &quot;3000:8080&quot;\n    extra_hosts:\n      - &quot;host.docker.internal:host-gateway&quot;\n    volumes:\n      - open-webui-local:/app/backend/data\n \n  ollama:\n    image: ollama/ollama:0.1.34\n    ports:\n      - &quot;11434:11434&quot;\n    volumes:\n      - ollama-local:/root/.ollama\n \nvolumes:\n  ollama-local:\n    external: true\n  open-webui-local:\n    external: true\n다음은 Docker Volume 생성입니다. Volume은 Container 동작 중에 생성/수정되는 데이터를 저장하는 공간인데요.\n위 compose.yaml에서 정의한 바와 같이, ollama-local(ollama의 데이터 저장)와 open-webui-local(Open-WehUI의 데이터 저장)라는 이름의 Docker volume을 생성하기 위해 터미널에서 아래 명령어를 실행합니다.\n\ndocker volume create ollama-local\ndocker volume create open-webui-local\n\n이제 docker compose 명령어로 두 개의 Container를 로컬에 배포해볼 건데요. 그 전에 compose.yaml 파일이 정상적으로 실행되는지 확인하기 위해 아래 명령어로 dry run을 해보겠습니다. (dry-run은 어떤 명령어가 예상대로 동작하는지 모의 실행하는 것을 말합니다. 해당 명령어가 실제로 실행되는 것은 아닙니다.)\n\ndocker compose --dry-run up -d (compose.yaml 파일이 존재하는 경로에서 실행)\n\n\ndry run이 잘 실행되는 것을 확인했으니 이제 아래 명령어를 실행하여 실제로 로컬 배포를 진행해보겠습니다.\n\ndocker compose up -d (compose.yaml 파일이 존재하는 경로에서 실행)\n\n\n각 Container가 정상 실행되었다는 메시지를 확인 후, compose.yaml에서 정의한 Open-WebUI의 Port 번호를 참고하여 웹 브라우저에서 localhost로 접속합니다. (본 예제에서 Open-WebUI 경로: http://localhost:3000)\n\n웹 브라우저로 접속한 Open-WebUI 창에서 Sign up 버튼을 눌러 계정을 새로 만들고 접속합니다. (이렇게 만든 계정은 우리가 이전에 생성한 Open-WebUI의 Docker Volume에 저장되므로 Sign up은 최초 한 번만 필요하며, 이후엔 계정으로 로그인하면 됩니다.)\n\n아직 ollama에서 사용할 LLM 모델이 없으므로, Open-WebUI의 오른쪽 상단의 톱니바퀴 버튼을 누른 뒤 models 메뉴 내 Pull a model from Ollama.com 옵션 입력창에 원하는 LLM 모델의 태그를 입력합니다. (본 예제에서는 llama3:8b를 가져왔습니다. ollama에서 제공하는 LLM 목록은 여기서 확인 가능합니다.)\n\nLLM 모델 다운로드가 완료되면 홈 화면의 왼쪽 상단에서 다운로드한 모델 선택이 가능하고, 이후 Chat을 진행할 수 있습니다.\n\n\n로컬 배포한 Container 관리\n만약 Docker Compose로 로컬 배포한 ollama와 Open-WebUI Container를 종료하고 싶다면 아래 명령어를 실행합니다.\n\ndocker compose down (compose.yaml 파일이 존재하는 경로에서 실행)\n\n추후에 용량 관리를 위해 ollama와 Open-WebUI가 사용하던 Volume을 삭제하고 싶다면 아래 명령어를 실행합니다. (Backup하지 않은 Volume은 삭제 후 복구할 수 없습니다.)\n\ndocker volume rm {대상 Volume 이름}\n\nReferences\n\ngithub.com/ollama/ollama\ngithub.com/open-webui/open-webui\n"},"blog/ollama와-crewAI로-로컬-환경에-블로그-포스팅-시스템-구축":{"title":"ollama와 crewAI로 로컬 환경에 블로그 포스팅 시스템 구축","links":["blog/ollama와-Open-WebUI-로컬-배포"],"tags":["Ollama","CrewAI"],"content":"🦙🧑‍🤝‍🧑Ollama와 CrewAI\nOllama는 로컬 환경에서 LLM을 실행하는 오픈소스 툴입니다. 지난 글에서 Docker로 Ollama와 Open-WebUI라는 툴을 실행해서  웹 브라우저로 로컬 LLM에게 질문을 해보는 튜토리얼을 진행한 적이 있었죠. (관련 블로그 글)\n이번엔 Ollama와 CrewAI를 활용해서 로컬 LLM 기반으로 블로그 글을 작성해주는 시스템을 구축해보려합니다.\nOllama만으로도 충분히 블로그 글을 자동으로 작성할 수 있지 않냐고요? 물론 Ollama로 실행한 LLM에게 부탁해도 글을 써줍니다. 하지만 CrewAI라는 툴을 사용하면, 각자의 역할과 목표를 가지고 있는 여러 LLM 기반 작업자(에이전트)가 일련의 프로세스를 거쳐 더욱 체계적으로 글을 써줄 수 있거든요.\n\n방금 이야기한 블로그 글 작성 시스템을 예로 들면서 알아보겠습니다.\nDevOps 관련 블로그 글을 쓸 때는 보통 아래와 같은 프로세스로 진행이 될 텐데요.\n\n인터넷 자료 조사\n조사한 내용을 토대로 글쓰기\n작성한 글에 오탈자는 없는지 검수하기\n\n이러한 각 과정을 수행하는 에이전트들을 둬서 서로 상호작용하며 작업을 수행하도록 시스템을 만드는 것이 CrewAI의 역할입니다.\n게다가 CrewAI는 파이썬 기반으로 개발되었고 직관적인 명령어들을 사용하기 때문에, 쉽고 빠르게 여러 에이전트로 구성된(Multi-Agent) 작업 수행 시스템을 구축할 수 있다는 장점도 있습니다.\nCrewAI의 에이전트는 역할, 목표, 배경으로 정의하는데요. 각 에이전트가 작업을 수행할 때 자신은 어떤 배경을 가지고 있고, 어떤 목표를 수행하는지 등을 미리 알려주는 거죠.\n\n위의 블로그 글 작성 시스템으로 다시 돌아와서, 위에서 언급한 프로세스의 3가지 작업을 담당하는 CrewAI 에이전트들을 아래처럼 정의해보겠습니다.\n\n인터넷 자료 조사\n\n역할: Researcher\n목표: 최신 DevOps 관련 토픽 조사\n배경: IT 대기업에서 근무 중인 세계적인 Researcher\n\n\n글쓰기\n\n역할: Writer\n목표: DevOps 관련 블로그 글 작성\n배경: IT 관련 글 작성에 특화된 최고의 Technical Writer\n\n\n검수하기\n\n역할: Proofreader\n목표: 기술 블로그 글 검수\n배경: IT 분야에 특화된 유명 Proofreader\n\n\n\n그리고 각 수행되어야 하는 작업도 아래와 같이 정의할 수 있습니다.\n\n최신 DevOps 관련 뉴스 조사\n\n담당 에이전트: Researcher\n출력물 설명: 약 3문단 분량의 최신 DevOps 관련 리포트\n\n\n조사 리포트를 기반으로 DevOps 관련 블로그 글 한 편 작성\n\n담당 에이전트: Writer\n출력물 설명: 약 4문단 분량의 Markdown 형식 DevOps 관련 블로그 글\n\n\n제공된 블로그 글을 보다 자연스럽게 검수\n\n담당 에이전트: Proofreader\n출력물 설명: 약 4문단 분량의 Markdown 형식 DevOps 관련 블로그 글\n\n\n\n\n웹 접근 관련 유의사항\n로컬 LLM을 사용하는 상황에서 웹 접근이 필요한 Researcher 같은 경우엔 Google search API 서비스 등을 별도로 이용해야 합니다.\n그래서 이번 실습에선 카드 등록 없이 이메일 등록으로 최대 2,500회 Google Search 쿼리가 가능한 Serper 서비스를 이용했습니다.\n\n🖥️Ollama와 CrewAI로 블로그 글 작성 시스템 구축하기\n이제 로컬에서 직접 Ollama와 CrewAI를 실행해서 블로그 글 작성 시스템을 구축해보도록 하겠습니다. 각 툴은 Docker로 로컬에 배포합니다.\n이번 실습에선 Ollama의 llama3(8b) 모델을 활용할 예정인데요. 그럴려면 먼저 Ollama를 이용해서 llama3 모델을 로컬에 가져와야겠죠.\nOllama의 모델이 저장될 공간인 Docker volume을 먼저 아래 명령어로 생성합니다.\ndocker volume create ollama-local\nDocker volume을 생성했다면 이제 compose.yaml라는 이름의  Docker compose 파일을 생성하겠습니다. Docker compose는 여러 Docker 컨테이너의 배포 설정을 쉽게 관리하고 실행할 수 있도록 정의하는 파일인데요. 지금은 우선 Ollama에 대해서만 정의해보겠습니다.\ncompose.yamlservices:\n  ollama:\n    image: ollama/ollama:0.1.34\n    container_name: ollama\n    ports:\n      - &quot;11434:11434&quot;\n    volumes:\n      - ollama-local:/root/.ollama #LLM이 저장될 Volume 지정\nvolumes:\n  ollama-local:\n    external: true\ncompose.yaml 작성이 끝나면 해당 파일이 있는 경로의 터미널에서 아래 명령어로 Docker compose를 실행하겠습니다. 지금은 Docker compose 파일에 정의되어 있는 Ollama만 실행되겠죠?\ndocker compose up -d\nOllama가 정상 실행되었다면 터미널에 아래처럼 표시가 될 겁니다.\n\n이제 실행된 Ollama 컨테이너에 접속해서 우리가 사용할 llama3 LLM을 가져오겠습니다.\n먼저 아래 터미널 명령어로 Ollama 컨테이너에 접속합니다.\ndocker exec -it ollama bash\n접속한 터미널은 아래와 유사한 모습일 겁니다.\n\n이 상태에서 아래 Ollama 명령어를 입력해서 공개된 원격 저장소에서 LLM을 가져옵니다.\nollama pull llama3:8b\n위 명령어를 입력하면 아래처럼 LLM을 가져오는데요. 가져온 LLM은 처음에 생성했던 Docker volume ollama-local에 저장됩니다.\n\ncompose.yaml 파일에서 Ollama 컨테이너와 ollama-local volume 연동 설정을 넣어두었기 때문에, compose.yaml 파일로 실행하는 Ollama는 llama3:8b LLM을 계속 사용할 수 있게 됩니다.\nOllama로 LLM 설치는 완료되었으니, exit 명령어로 컨테이너에서 나옵니다.\n이제 CrewAI를 Docker로 실행해볼 건데요. CrewAI는 파이썬 패키지이므로, 우리가 작성해야 할 파일은 아래와 같이 총 3가지입니다.\n\nCrewAI를 Docker에서 실행하기 위한 Dockerfile(crewai.Dockerfile)\nCrewAI 관련 설정과 정의 후 실행하는 Python 스크립트(main-crewai.py)\nmain-crewai.py 실행에 필요한 패키지를 정의한 requirements.txt\n\ncrewai.DockerfileFROM python:3.12.4\n \nWORKDIR /app\nCOPY requirements.txt ./requirements.txt\nRUN pip install -r requirements.txt\n \nCOPY main-crewai.py ./\n \nCMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;main-crewai.py&quot; ]\nmain-crewai.pyimport os\nfrom crewai import Agent, Task, Crew, Process\nfrom crewai_tools import SerperDevTool\n \n# 로컬에 실행 중인 ollama에서 원하는 LLM 가져옴. 예제에선 llama3 (파라미터 사이즈 8b) 사용\nfrom langchain.llms import Ollama\nollama_model = Ollama(\n    base_url=&#039;http://ollama:11434&#039;,\n    model=&quot;llama3:8b&quot;)\n \nos.environ[&quot;OTEL_SDK_DISABLED&quot;] = &quot;true&quot;\n \n# researcher Agent가 웹에 접근해서 최신 IT 정보를 찾을 수 있도록 server.dev API 서비스 이용\nos.environ[&quot;SERPER_API_KEY&quot;] = &quot;{자신의 serper API key를 넣어주세요}&quot;  # serper.dev API key\nsearch_tool = SerperDevTool()\n \n# crewai 패키지로 원하는 Agent의 역할(role)과 목표(goal) 설정\n# 최신 DevOps 관련 토픽을 조사하는 Agent 정의\nresearcher = Agent(\n    role=&#039;Researcher&#039;,\n    goal=&#039;Discover a newest and attracting topic about DevOps&#039;,\n    backstory=&quot;You&#039;re world class researcher working on a big IT company&quot;,\n    verbose=True,\n    allow_delegation=False,\n    llm=ollama_model,\n    tools=[search_tool]\n)\n \n# 블로그 글을 작성하는 Agent 정의\nwriter = Agent(\n    role=&#039;Writer&#039;,\n    goal=&#039;Create DevOps blog post&#039;,\n    backstory=&quot;You&#039;re a best technical writer who is specialized on writing IT content&quot;,\n    verbose=True,\n    allow_delegation=False,\n    llm=ollama_model\n)\n \n# 작성된 글을 검수하는 Agent 정의\nproofreader = Agent(\n    role=&#039;Proofreader&#039;,\n    goal=&#039;Edit and proofread technical article&#039;,\n    backstory=&quot;You&#039;re a famous proofreader who is specialized on IT domain&quot;,\n    verbose=True,\n    allow_delegation=False,\n    llm=ollama_model\n)\n \n# 정의한 Agent들로 수행할 작업(Task) 정의\nresearch_task = Task(\n    description=&#039;Investigate the latest DevOps news&#039;,\n    agent=researcher,\n    expected_output = &#039;A comprehensive 3 paragraphs long report on the latest and famous DevOps.&#039;\n)\n \nwriting_task = Task(\n    description=&#039;Write a blog post about DevOps with one topic provided from the researcher&#039;,\n    agent=writer,\n    expected_output=&#039;A 4 paragraph article about DevOps formatted as markdown.&#039;,    \n)\n \nproofreading_task = Task(\n    description=&#039;Proofread the provided blog post to make more natural article&#039;,\n    agent=proofreader,\n    expected_output=&#039;A 4 paragraph article about DevOps formatted as markdown.&#039;,    \n)\n \n# 위 Agent와 Task, 작업 프로세스를 정의\ncrew = Crew(\n  agents=[researcher, writer, proofreader],\n  tasks=[research_task, writing_task, proofreading_task],\n  llm=ollama_model,\n  verbose=2, # crew 작업 중에 발생하는 로그의 자세한 정도를 설정 가능.\n  process=Process.sequential # Task가 순차적으로 실행될 수 있도록 sequential로 정의.\n)\n \n# 정의한 crew 실행 및 작업 과정에서 발생하는 로그 출력\nresult = crew.kickoff()\nprint(result)\nrequirements.txtcrewai==0.32.0\ncrewai-tools==0.2.6\nlangchain==0.1.20\nCrewAI 관련 파일 준비가 끝났다면, 아래 Docker 명령어로 CrewAI가 실행될 Docker 이미지를 생성합니다.\ndocker build -t my-crewai -f crewai.Dockerfile .\nCrewAI Docker 이미지 빌드가 끝났다면, 위에서 정의했던 compose.yaml 파일을 아래와 같이 최신화합니다.\ncompose.yamlservices:\n  ollama:\n    image: ollama/ollama:0.1.34\n    container_name: ollama\n    ports:\n      - &quot;11434:11434&quot;\n    volumes:\n      - ollama-local:/root/.ollama\n  crewai:\n    image: my-crewai\n    container_name: crewai\n    depends_on:\n      - ollama\n    extra_hosts:\n      - &quot;telemetry.crewai.com:127.0.0.1&quot; # To avoid &#039;Connection to telemetry.crewai.com timed out&#039; error when using local LLM\nvolumes:\n  ollama-local:\n    external: true\n이제 터미널에서 아래 Docker compose 명령어를 입력하면, CrewAI의 각 에이전트가 작업을 수행하는 과정과 최종 결과물을 터미널에서 확인할 수 있습니다. (명령어 마지막에 docker compose down을 연결한 것은 CrewAI 작업이 모두 완료되면 Ollama와 CrewAI 컨테이너 모두 정상 종료시키기 위함입니다.)\ndocker compose up -d &amp;&amp; docker compose logs crewai -f &amp;&amp; docker compose down\n🗂️CrewAI의 에이전트들의 작업 과정과 최종 생성 결과물\n이렇게 실행한 CrewAI의 로그를 살펴보면, 각 에이전트가 일하는 과정을 로그로 확인할 수 있습니다.\n\n\n또한 에이전트가 내놓은 최종 결과물도 확인할 수 있죠.\n\nCrewAI의 Researcher 에이전트가 작성한 리포트를 토대로 Writer 에이전트가 블로그 글을 써주는 등, 각 에이전트가 미리 정의된 프로세스대로 상호작용하는 것을 보니 정말 흥미로웠는데요.😄\n블로그 글 작성 외에도 CrewAI를 활용해서 어떤 작업 프로세스를 수행할 수 있을지 궁금해지네요.😊\nReferences\n\ndocs.crewai.com/\nfossengineer.com/ai-agents-crewai/#building-the-crewai-container\n# Create a Blog Writer Multi-Agent System using Crewai and Ollama\n"},"blog/더-안전한-CICD-파이프라인-만들기---SLSA-표준":{"title":"CI/CD 파이프라인을 더 안전하게 만드는 방법: SLSA 표준","links":[],"tags":["CICD"],"content":"🛡️SLSA의 보안 수준 4단계\n\n오늘날 수많은 기업과 조직에서 효율적인 개발 프로세스를 위해 CI/CD를 도입하는 경우가 많습니다. CI/CD 파이프라인으로 개발 이후의 코드 통합과 배포를 자동화하면 애자일한 개발이 가능하지만, 이런 파이프라인 내부에 보안 취약점이 존재한다면 심각한 보안 사고로 이어질 수도 있겠죠.\n그래서 2023년 4월, Google에서 소프트웨어 공급 프로세스의 보안 기준을 제시했는데요. 바로 SLSA(Supply-chain Levels for Software Artifacts)입니다.\nSLSA는 소프트웨어 산출물(Software Artifacts)을 공급하는 프로세스(Supply Chain)의 보안 수준(Levels)을 4단계로 정의한 표준인데요. SLSA의 단계는 보안 준수 정도에 따라 Build L0부터 Build L3까지 나눠집니다. SLSA의 특정 단계를 준수하고 있다는 것은 그만큼의 보안 조치를 취하고 있음을 알리는 것이죠.\nSLSA에서 정의한 보안 수준 4단계를 간략히 소개하면 아래와 같습니다.\n\nBuild L0:\n\nSLSA 조건에 부합하지 않은 상태\n개발과 테스트 빌드가 동일한 머신에서 수행되고, SLSA 표준 고려가 없는 상태를 의미\n\n\nBuild L1:\n\n패키지가 어디서 어떻게 빌드되었는지 명세된 문서(Provenance)가 있는 상태\n빌드 프로세스 변경 없이 SLSA 보안 지침을 일정 부분 준수하려는 개발 조직 대상\n\n\nBuild L2:\n\n호스팅되는 플랫폼 상에서 CI/CD 프로세스의 빌드가 이뤄지고, 빌드 후Provenance가 생성되는 상태\n호스팅되는 빌드 플랫폼으로 전환하여 SLSA 보안 지침을 적정 수준으로 준수하려는 개발 조직 대상\nBuild L1을 먼저 준수해야 함\n\n\nBuild L3:\n\n보안에 강한 빌드 플랫폼을 사용하고 있는 상태\nBuild L2를 먼저 준수해야 함\n\n\n\n🔎SLSA에서 정의한 CI/CD 파이프라인 대상 위협과 개선방안\n\nSLSA에서는 CI/CD 프로세스가 받을 수 있는 보안 위협도 위 그림과 같이 8가지로 정의했습니다. 그림에 표시된 각 위협과 개선방안을 살펴보면 아래와 같습니다.\n\nA: 소스코드 무단 변경\n\n개선방안: 소스코드 커밋에 대해 두 명 이상이 리뷰\n\n\nB: 소스코드 레파지토리 무단 침입\n\n개선방안: 보안이 강화된 소스코드 플랫폼 사용\n\n\nC: 무단으로 수정된 소스코드로부터 빌드\n\n개선방안: SLSA를 준수하는 빌드 서버 사용하여 빌드 후 생성되는 Provenance 확인\n\n\nD: 무단으로 수정된 디펜던시(의존성) 사용\n\n개선방안: Provenance를 확인 가능한 환경 조성을 위해 SLSA 준수\n\n\nE: 빌드 프로세스 무단 침입\n\n개선방안: 더 높은 SLSA 단계를 준수하여 안전한 빌드 환경 조성\n\n\nF: 기존 빌드 프로세스를 거친 것이 아닌, 무단으로 수정된 패키지 업로드\n\n개선방안: 패키지의 Provenance를 조회할 수 있도록 하여 정상적인 레파지토리에서 정상적인 과정으로 빌드되었는지 확인\n\n\nG: 패키지 저장소 무단 침입\n\n개선방안: F의 개선방안과 유사하게, 패키지의 Provenance를 조회할 수 있도록 환경 조성\n\n\nH: 무단으로 수정된 패키지 사용\n\n해당 위협에 대해 SLSA에서 개선방안을 직접 제안하지는 않지만, 패키지의 Provenance를 활용 가능할 것\n\n\n\n지금까지 SLSA에서 정의한  CI/CD 프로세스 대상 위협과 개선방안에 대해 살펴봤는데요. 이런 소프트웨어 공급 프로세스에 대한 위협은 프로세스를 통해 만들어진 패키지를 공격자의 의도대로 악용할 수 있도록 무단 수정하는 방식이 대부분이었습니다.\n그렇기 때문에 SLSA 표준에서는 패키지가 어디서 어떻게 빌드되었는지 그 기원을 기록한 Provenance를 중요하게 여기고 있는 것으로 보이는데요.\n만약 여러분이 개발하고 있거나 운영 중인 CI/CD 파이프라인의 보안을 개선하고 싶으셨다면, SLSA의 가이드를 참고해봐도 좋을 듯합니다.\nReferences\n\nslsa.dev/\ndeveloper.cyberark.com/blog/what-is-slsa-supply-chain-levels-for-software-artifacts/\n"},"blog/무중단-배포의-종류와-설명":{"title":"무중단 배포의 종류와 설명","links":[],"tags":["Deployment"],"content":"무중단 배포의 종류\n무중단 배포는 아래와 같이 크게 3가지 방식으로 나뉩니다.\nRolling 방식\n동작 중인 인스턴스를 점진적으로 업데이트하는 방식입니다.\n\n\n장점:\n\n인스턴스를 추가로 늘리지 않아도 괜찮습니다.\n인스턴스마다 차례로 버전이 전환되기 때문에 상황에 따라 롤백이 가능합니다.\n\n\n\n단점:\n\n신버전을 배포하고 구버전의 인스턴스 수가 감소하면서 사용 중인 인스턴스에 트래픽이 몰릴 수 있습니다.\n배포 과정에서 구버전과 신버전이 동시에 존재하는 시점이 생기고, 이때 사용자들이 통일되고 균일한 서비스를 받지 못하게 된다.\n\n\n\nBlue / Green 방식\n동작 중인 인스턴스 환경과 동일한 환경에서 새로운 버전을 배포한 뒤, 로드밸런서를 통해 모든 트래픽을 새로운 버전의 인스턴스 환경으로 한 번에 전환하는 방식입니다.\n\n\n장점:\n\n구버전의 인스턴스가 그대로 남아있기 때문에 롤백하기 쉽습니다.\n새 버전의 테스트가 용이합니다.\n\n\n\n단점:\n\n인스턴스 가동에 필요한 시스템 자원이 두 배로 필요합니다.\n인스턴스를 새로 가동하는 환경에 대한 테스트를 사전에 완료해야 합니다.\n\n\n\nCanary 방식\n동작 중인 인스턴스 환경과 동일한 환경에서 새로운 버전을 배포한 뒤, 소수의 사용자 트래픽을 새로운 버전으로 보내 문제가 없음을 확인합니다.\n문제가 없다면 점점 더 많은 사용자 트래픽을 새로운 버전으로 전달하는 방식입니다.\n참고로 Canary(카나리)라는 이름은, 광부들이 유독 가스에 민감한 ‘카나리아’라는 새를 자신들의 작업 환경에 미리 풀어 가스 누출 여부를 감지했던 것에서 유래되었습니다.\n\n\n장점:\n\nA/B 테스트로 활용가능합니다.\n\n\n\n단점:\n\n네트워크 트래픽 제어 작업이 추가로 필요합니다.\n\n\n"},"blog/보안에-강한-Dockerfile-작성-팁-5가지":{"title":"보안에 강한 Dockerfile 작성 팁 5가지","links":[],"tags":["Docker","Security"],"content":"\n들어가기\nDockerfile은 Docker 컨테이너 이미지를 빌드할 때 사용되는 각종 설정과 명령어를 선언한 파일입니다. 우리가 개발 환경을 구축하거나 서비스를 배포할 때 누군가 미리 만들어놓은 Docker 이미지를 사용하기도 하지만, 직접 Dockerfile을 작성해서 Docker 이미지로 빌드하는 경우도 많은데요.\n더욱 안전한 Docker 이미지를 제작할 수 있는 Dockerfile 작성 팁이 있다는 사실, 알고 계셨나요?\nDockerfile을 작성할 때 보안을 신경써야 하는 이유\n오늘날 대부분의 웹 서비스는 마이크로서비스 아키텍처를 따르고 있습니다. 하나의 커다란 서비스를 배포하는 것이 아닌, 기능이나 성격에 따라 나눠진 작은 서비스 여러 개를 배포 및 운영하는 방식을 마이크로서비스 아키텍처라고 하는데요.\n이때 마이크로서비스를 각각의 Docker 컨테이너 내부에서 동작하도록 구성하는 것이 일반적입니다. 하지만 이렇게 실제로 배포된 컨테이너 이미지가 외부 공격에 취약하다면… 생각만 해도 아찔한 보안 사고로 이어지겠죠.\n그래서 우리는 Dockerfile을 작성할 때부터 보안에 신경써야 합니다.\n보안을 위한 Dockerfile 작성 팁 5가지\n그렇다면 Dockerfile을 어떻게 작성하면 좋을까요? 여기 보안에 강한 Dockerfile 작성 팁팁 5가지를 소개해드립니다.\nMulti-Stage 방식으로 빌드\nDockerfile 내에서 애플리케이션 빌드 명령어 실행 후 나오는 최종 산출물을 또다른 Base Image로 복사하고, 해당 Base Image를 최종 Docker Image로 빌드하는 기법을 Multi-Stage라고 하는데요.\nMulti-Stage 방식으로 빌드된 Docker 이미지 내에는 애플리케이션 구동에 필요한 최소한의 요소만 담겨있으므로, 자연스럽게 잠재된 보안 취약점도 더 적어집니다.\nMulti-Stage를 사용해서 간단한 Golang 애플리케이션을 빌드하는 Dockerfile 예제를 같이 살펴보겠습니다.\n# /src/main.go 파일을 빌드하는 build 스테이지입니다.\nFROM golang:1.21 as build \nWORKDIR /src\nCOPY &lt;&lt;EOF /src/main.go\npackage main\n \nimport &quot;fmt&quot;\n \nfunc main() {\n  fmt.Println(&quot;hello, world&quot;)\n}\nEOF\nRUN go build -o /bin/hello ./main.go\n \n# build 스테이지로부터 빌드된 산출물들만 가져와 실행하는 최종 스테이지입니다.\nFROM scratch\nCOPY --from=build /bin/hello /bin/hello\nCMD [&quot;/bin/hello&quot;]\n위 Dockerfile을 실행하면 먼저 build라는 이름이 붙여진 스테이지가 먼저 실행되는데요. golang:1.21 이미지 위에 go build 명령어로 애플리케이션이 빌드되는 구간입니다.\n이후 또다른 스테이지가 실행되고, build 스테이지에서 빌드된 최종 산출물을 복사한 뒤 실행하는 로직이 수행되는데요. 위 Dockerfile로 빌드되는 이미지는 이 최종 스테이지의 내용만 담게 되는 것입니다.\n필요없는 패키지 제거\nDocker 컨테이너 내 애플리케이션 구동에 필요없는 패키지를 제거하는 것 역시 잠재 보안 취약점을 줄여주기 때문에 권장됩니다.\nRoot가 아닌 별도의 사용자를 생성 및 사용\nContainer 내에 Root 계정이 사용될 경우, 침입 사고 발생 시 피해가 커질 수 있습니다. 그래서 아래 예시와 같이 Root 계정이 아닌 별도의 그룹 및 사용자를 생성하고 사용하는 것이 안전합니다.\nRUN groupadd -r for-example &amp;&amp; useradd -r -g for-example for-example\nUSER for-example\nContainer 내 민감한 파일들은 Read Only로\n배포 이후 Docker Container 내에서 추후 수정이 필요하지 않는 파일들을 Read Only로 설정하면 보안성을 높일 수 있습니다.\nchmod -R a-w {폴더명} 또는 chmod a-w {파일명} 명령어를 사용하면, 모든 사용자는 해당 폴더 또는 파일에 대한 쓰기 권한을 잃게 됩니다.\nShell Access 제거\n컨테이너 안에서 실행 가능한 sh나 bash 등의 Shell은 동작 중인 컨테이너 내 애플리케이션을 디버깅할 때 사용되는 경우가 있지만, 이런 통로를 제거하면 컨테이너 내부 침입이 어려워져 보안성이 높아집니다.\nShell 접근을 제거할 경우, 아래 주의사항에 대해 고려해야 합니다.\n\n컨테이너 이미지에서 동작하는 애플리케이션이 Shell을 사용하고 있지 않은지 확인해야 합니다.\n해당 이미지를 배포한 뒤에 디버깅할 수 있는 다른 대안을 미리 마련해야 합니다.\n\n컨테이너 보안을 지킬 수 있는 또다른 방법\nDocker 이미지를 직접 제작할 땐 위와 같은 방법으로 안전한 이미지를 만들어 사용할 수 있지만, 만약 이미 누군가가 제작한 Docker 이미지를 사용할 때엔 그 이미지가 안전한지 어떻게 알 수 있을까요?\n이럴 때 사용할 수 있는 것이 바로, 컨테이너 보안 스캐닝 툴입니다.\n대표적으로 Trivy가 있는데요. 컨테이너 보안 스캐닝 툴에 대해서는 추후 다른 글에서 소개해보겠습니다.\nReferences\n\n# Best practices for writing Dockerfiles\n"},"blog/블록체인-개발에-적용-가능한-DevOps":{"title":"블록체인 개발에 적용 가능한 DevOps","links":["blog/Infrastructure-as-Code-(IaC)-알아보기"],"tags":["DevOps","블록체인"],"content":"⛓️블록체인 개발에 DevOps를 도입한다면\n블록체인은 최근 큰 관심을 이끈 기술입니다. 분산 거래 기록 시스템이라고도 불리는데요.\n지금도 블록체인은 다양한 곳에서 활용되고 있고, 수많은 블록체인 프로젝트도 개발 및 유지보수되고 있습니다.\n\n블록체인은 네트워크를 구성하고 있는 노드들이 네트워크에서 발생하는 데이터를 함께 검증해서 무결성을 증명해내는 기술이기 때문에, 시간이 흘러 블록체인 네트워크가 성숙해질수록 효율적인 네트워크 관리와 보안 사고 예방이 매우 중요해집니다.\n이렇게 블록체인 프로젝트를 개발하면서 가시화되는 문제를 정리하면 아래와 같을 것입니다.\n\n복잡성과 규모\n\n블록체인 네트워크는 수많은 노드와 방대한 양의 데이터로 점점 더 복잡해지고 있습니다.\n그로 인해 효율적인 노드 관리가 더욱 필요해지고 있습니다.\n\n\n보안\n\n트랜잭션과 데이터의 보안은 블록체인 기술에서 가장 중요한 요소입니다.\n그렇기 때문에 블록체인 네트워크에 대한 악의적인 공격을 실시간으로 감지하고 대응하는 것이 필요합니다.\n또한 스마트 컨트랙트나 블록체인 애플리케이션 코드에 취약점이 있는지 미리 검사하여 보안 사고를 예방하는 것 역시 중요해졌습니다.\n\n\n급속한 트렌드 변화\n\n블록체인 도메인은 빠르게 성하고 있기 때문에 이런 흐름에 맞추려면 애플리케이션이나 서비스를 짧은 주기로 자주 업데이트하고 적응하는 것이 필요합니다.\n\n\n\n위 내용은 DevOps를 통해 해결할 수 있는 문제와 많이 닮아있는데요.\n그래서 블록체인 개발 프로젝트에 DevOps를 도입하여 이런 문제들을 해결하는 경우가 많습니다.\n🧑‍💻블록체인 개발에 적용 가능한 DevOps\n블록체인 프로젝트 개발에 적용할 수 있는 DevOps 요소는 아래와 같이 크게 4가지로 나눠서 정리할 수 있습니다.\n\nCI/CD\n\n테스트 자동화\n\n스마트 컨트랙트 및 블록체인 애플리케이션 테스트 자동화\n\n\nCI/CD 파이프라인 구축\n\n블록체인 애플리케이션 빌드, 테스트, 배포를 자동화하는 CI/CD 파이프라인 구축\n\n\n\nIaC\n\n인프라 프로비저닝 자동화\n\nTerraform 등의 툴로 블록체인 네트워크 구축에 필요한 인프라 리소스 프로비저닝 자동화\n우리는 이미 IaC에 대해 알아본 적이 있는데요. 지난 글에서 더 자세한 내용을 확인해보세요.\n\n\n일관성 및 확장성\n\n필요 시 블록체인 네트워크가 일관성있게 재구축 가능하며, 네트워크의 규모도 일관성 있게 변경 가능\n\n\n\n보안\n\n보안 취약점 검사\n\n스마트 컨트랙트나 블록체인 애플리케이션을 배포하기 전 코드에 대한 보안 취약점을 검사하여 보안 사고를 사전에 예방\n\n\n네트워크 공격 감지\n\n블록체인 네트워크를 실시간으로 모니터링하고 의심되는 행위 발견 시 관리자 또는 개발자에게 알림을 보내는 시스템 구축\n\n\n\n모니터링 및 로깅\n\n네트워크 모니터링\n\n블록체인 네트워크의 상태와 퍼포먼스를 모니터링할 수 있는 시스템 구축\n\n\n트랜잭션 모니터링\n\n트랙잭션 로그를 수집하고 추적 가능한 시스템 구축\n\n\n\n✅블록체인 개발에 DevOps를 적용 사례들\n이미 여러 조직에서 블록체인 개발 프로세스에 DevOps를 적용하고 있는데요. 아래와 같이 사례를 간단히 정리해봤습니다.\n\nIBM 블록체인 플랫폼\nIBM은 블록체인 플랫폼을 Kubernetes 클러스터에서 이용할 수 있도록 별도의 Kubernetes Operator를 제공하고 있습니다.\nKubernets 클러스터 위에 블록체인 네트워크 운영에 필요한 컴포넌트를 배포하고 관리할 수 있기 때문에 확장성과 회복성을 제고할 수 있죠.\n\nEthereum Development\nEthereum의 패키지 중 Kurtosis는 Docker나 Kubernetes를 활용해서 프라이빗 Ethereum 테스트넷을 구성할 수 있습니다.\n블록체인 애플리케이션 테스트를 위한 테스트넷을 Kubernetes로 구성함으로써, 필요할 때마다 동일한 테스트 환경을 재구축하거나 설정 변경도 용이합니다.\nReferences\n\ndexoc.com/blog/how-devops-enhances-blockchain-development\nwww.ibm.com/docs/en/blockchain-platform/2.5.4\ngithub.com/ethpandaops/ethereum-package\n"},"blog/컨테이너-런타임-보안":{"title":"컨테이너 런타임 보안","links":[],"tags":["Security"],"content":"\nContainer Runtime Security(컨테이너 런타임 보안)이란 컨테이너화된 애플리케이션이 동작 중일 때 보안성을 높이기 위한 활동들을 의미합니다.\n컨테이너 런타임 보안이 중요한 이유\n컨테이너 내부의 애플리케이션은 실제로 동작하는 개체이면서, 대부분 데이터를 처리하는 역할을 수행하기 때문에 보안 공격 대상이 되기 쉽습니다.\n게다가 컨테이너는 호스트의 커널을 공유하기 때문에, 공격자가 컨테이너 내부 애플리케이션에 무단 침입 후 컨테이너 외부로 빠져나오게 된다면 해당 호스트나 다른 컨테이너에도 접근하게 되므로 심각한 보안 사고로 이어질 수 있습니다.\n그렇기 때문에 동작 중인 컨테이너에 대한 보안은 중요하다고 할 수 있겠습니다.\n컨테이너 런타임 보안 활동이 이뤄지는 방식\n이러한 컨테이너 런타임 보안 활동에는 컨테이너에 대한 실시간 모니터링 및 보호 활동 등이 포함됩니다.\n컨테이너가 실행되면서 발생하는 Linux 커널의 System Call을 감시하는 것이 일반적이라고 할 수 있겠습니다.\n이렇게 감시를 진행하다가 프로세스의 이상 행위나 잠재적인 보안 유해 행위가 감지된다면 이를 사용자 또는 관리자에게 알리는데, 더 나아가 미리 정의된 규칙에 따라 이러한 행위를 제한하는 경우도 있습니다.\n컨테이너 런타임 보안을 위해 개발된 툴\n컨테이너 런타임 보안 활동을 위해 개발된 툴에는 여러가지가 있습니다. 그 중 대표적인 3가지를 간략히 소개해드리겠습니다.\n\nFalco\n\nLinux 커널 및 Kubernetes API Call을 통해 노드 및 컨테이너 내 이상 행위를 감지할 수 있는 툴 (관련 글)\n\n\nAqua Security\n\n컨테이너의 런타임 보호 기능을 제공하며, 보안 제어 자동화를 도와줌\n\n\nSysdig Secure\n\n런타임 보안, 포렌식, 취약점 관리 기능 등을 제공함\n\n\n\nReferences\n\nwww.wiz.io/academy/container-runtime-security\nIntroduction: what is container runtime security?\n"},"blog/컨테이너-보안-스캐닝":{"title":"컨테이너 보안 스캐닝","links":[],"tags":["Security"],"content":"컨테이너 보안 스캐닝(Container Security Scanning)이란 컨테이너 이미지에 존재할 수 있는 보안 취약점이나 이슈를 컨테이너 스캐닝 툴로 검사하고 분석하는 행위를 말합니다.\n\n컨테이너 스캐닝이 필요한 이유\n현재 운영 중인 서비스 대부분이 컨테이너 이미지 기반으로 동작하기 때문에 컨테이너 이미지에 보안 취약점 또는 이슈가 있다면 엄청난 보안 사고 및 재정적 피해로 이어질 수 있습니다.\n컨테이너 보안 스캐닝을 수행하면 개발자가 해당 컨테이너에 어떤 취약점이 존재하는지 인지할 수 있고 사전에 조치를 취할 수 있게 됩니다.\n또한, 실제 서비스에 대해서 주기적으로 컨테이너 보안 스캐닝을 수행할 경우, 최종 사용자에게 해당 서비스가 계속 모니터링되어 안전하다는 신뢰감도 줄 수 있습니다.\n컨테이너 스캐닝이 수행되는 방식\n컨테이너 스캐닝 툴은 보통 원격 레지스트리 또는 로컬에 저장된 컨테이너 이미지를 대상으로 스캔합니다.\n이때 스캐닝 툴은 스캔을 위해 해당 이미지를 레이어(Layer) 단위로 해체하는데, Base 이미지와 애플리케이션 코드, 각 디펜던시 역시 해체 대상에 포함됩니다.\n스캐닝 툴의 스캔 방식은 아래와 같이 크게 2가지로 나뉩니다.\n\n지식 기반(Signature-Based) 스캔: 기존에 알려진 취약점(CVE 등)을 기반으로 스캔 수행\n행동 기반(Behavioral-Based) 스캔: 컨테이너 실행 시점에 비정상적인 프로세스나 네트워크 트래픽 같은 이상 행동에 대해 스캔 수행\n\n컨테이너 보안 스캐닝 툴\n\nTrivy: 컨테이너 이미지, 파일 시스템, Kubernetes 클러스터 등 다양한 대상에 대해 보안 이슈와 취약점 스캔 가능합니다. CLI로 스캔 작업을 실행합니다. (관련 문서)\nClair: 함께 설치되는 DB에 스캔 대상 컨테이너 이미지에 대한 정보와 레이어를 저장 후 보안 취약점을 스캔하는 것이 특징입니다. CLI로 스캔 작업을 실행하며, Webhook을 통한 알림 기능도 지원합니다. (관련 문서)\n\nReferences\n\nwww.wiz.io/academy/container-security-scanning\nsnyk.io/learn/container-security/container-scanning\n"},"blog/컨테이너-엔진-관련-취약점-(CVE-2024-21626)":{"title":"컨테이너 엔진 관련 취약점: CVE-2024-21626","links":[],"tags":["CVE","Container"],"content":"2024년 1월 말, 컨테이너 엔진으로 널리 쓰이는 runc에서 취약점(CVE-2024-21626)이 발견되었습니다. runc는 Docker와 Kubernetes 등이 컨테이너 이미지를 빌드하거나 실행할 때 사용하는 컨테이너 엔진입니다.\n\nCVE-2024-21626 취약점이란\n\n공격자가 컨테이너에서 탈출(Container Escape)하여 호스트 시스템에 비허가 접근 위험이 있는 취약점(CVE-2024-21626)이 runc에서 발견되었습니다.\n해당 취약점은 runc가 컨테이너 이미지를 생성하거나 실행하는 특정 시점에 호스트 내 파일레 접근 가능한 통로가 여전히 남아있기 때문에 발생한 것입니다.\n악성 컨테이너 이미지를 실행하거나, 악성 Dockerfile 또는 악성 Base Image로 컨테이너 이미지를 빌드할 때 해당 취약점의 영향을 받을 수 있습니다.\n\nCVE-2024-21626이 위험한 이유\n\n공격자가 컨테이너에서 벗어나 호스트 시스템에 저장되어있던 모든 데이터(계정 정보, 사용자 정보 등)를 획득 가능합니다.\n또한 공격자가 컨테이너에서 벗어난 뒤 호스트 시스템에서 추가 공격이 가능합니다.\n컨테이너 엔진인 runc에 영향을 주는 취약점이므로, runc를 기반으로 작동하면서 널리 사용되는 컨테이너 툴 Docker, Kubernetes도 영향을 받게 되어 파급력이 큽니다.\n\nCVE-2024-21626에 대한 대처 방법\n\n\n사용 중인 컨테이너 툴 버전을 해당 취약점에 대해 보완된 버전으로 최신 업데이트합니다.\n\nrunc &gt;= 1.1.12\ncontainerd &gt;= 1.6.28\nDocker Desktop &gt;= 4.27.1\n\n\n\n현재 사용 중인 컨테이너 툴을 바로 업데이트하기 어렵다면…\n\n신뢰할 수 있는 컨테이너 이미지만 사용합니다.\n컨테이너 이미지 빌드 시, 신뢰할 수 있는 Dockerfile 또는 신뢰할 수 있는 Base Image만 사용합니다.\n\n\n"},"blog/플랫폼-엔지니어링이란":{"title":"플랫폼 엔지니어링이란?","links":[],"tags":["PlatformEngineering"],"content":"⚙️플랫폼 엔지니어링이란?\n우리가 DevOps에 대해 검색을 하다보면 함께 떠오르는 키워드가 하나 있습니다. 바로 플랫폼 엔지니어링인데요.\n최근 저도 플랫폼 엔지니어링이란 키워드를 많이 접하게 되면서 플랫폼 엔지니어링이 구체적으로 무엇을 말하는지 궁금했고, 이번 기회에 정리를 해봤습니다.\n그럼 같이 알아보시죠!\n\n플랫폼 엔지니어링은 개발자의 생산성을 높여주는 내부 플랫폼(Internal Developer Platform, IDP)을 개발하고 유지보수하는 활동을 의미합니다. 즉, 플랫폼 엔지니어링이란 개발자를 위한 개발이라고 볼 수 있겠는데요.\n그렇다면 플랫폼 엔지니어링은 DevOps와 어떤 차이가 있을까요?\n🔎DevOps와 플랫폼 엔지니어링의 차이\n두 가지 모두 조직 내 개발자를 대상으로 하는 활동이기 때문에 그 차이점이 분명하지 않을 수 있습니다. 그래서 두 활동을 함께 비교해보겠습니다.\n\n먼저 DevOps는 소프트웨어 개발 접근 방식 중 하나로, 개발 주체과 운영 주체 간의 긴밀한 협업을 위한 것입니다.\n그리고 플랫폼 엔지니어링은 DevOps에 필요한 각종 도구와 작업 프로세스 한 곳에 모은 플랫폼을 제공하는 활동입니다.\n즉, DevOps 팀은 조직에게 적절한 개발 ,빌드, 테스트, 배포, Configuration, 자동화 도구를 도입함으로써 개발 프로세스를 정립하는 역할을 수행하고, 플랫폼 엔지니어링 팀은 이렇게 선정된 도구들을 파악, 추가 구현, 유지보수하여 조직 내 개발자들이 사용할 수 있도록 플랫폼(IDP)으로 만들어내는 역할을 수행하는 것이죠.\n이렇게 IDP를 구성하게 된다면, 조직 내 다른 개발자들은 플랫폼이나 도구 자체에 대한 세세한 이해 없이도 플랫폼이 제공하는 일관성과 생산성을 누릴 수 있게 됩니다.\n지금까지 살펴본 것처럼, 플랫폼 엔지니어링과 IDP는 실과 바늘같은 관계이기 때문에 IDP에 대해서도 그냥 넘어갈 수는 없겠습니다.\nIDP는 내부 개발자 플랫폼이라고 했었죠. 이런 IDP는 대규모 개발 팀에게만 필요하다고 생각될 수도 있지만, 개발부터 배포까지 꾸준히 이뤄지는 팀이라면 안정적인 작업 프로세스를 위해 IDP를 고민해볼만 합니다.\n사실 IDP에 대한 표준이 정립된 것은 아니기 때문에 각 조직마다 IDP에 대한 요구사항은 다를 수 있습니다. 또한 여러 개의 툴을 유기적으로 활용해서 IDP로 구성할 수도 있는 것이죠.\n조직마다 차이는 있더라도 IDP가 가져야 할 필수 요소는 아래와 같이 7가지로 정리할 수 있습니다.\n🛠️IDP의 필수 구성요소 7가지\n파이프라인\n소프트웨어 코드를 개발 중이라면 코드에 문제가 없는지 검증이 필요한데요. 이런 코드 검증을 자동으로 해주는 파이프라인이 있다면 개발자들의 생산성이 향상될 것입니다.\n파이프라인 관련 툴로는 GitHub Action, GitLab CI Runner, Jenkins 등이 있습니다.\n아티팩트 저장소\n컴파일이 완료되었거나 컨테이너화된 이미지를 저장하는 아티팩트 저장소는 조직 내 개발 프로세스와 보안을 위해서 필수적입니다. 또한 아티팩트 저장소가 있다면 문제가 발생했을 경우 신속하게 이전 버전의 이미지를 가져와서 사용할 수도 있겠죠.\n아티팩트 저장소 관련 툴로는 CRI-O, Docker, Nexus 등이 있습니다.\n런타임 관련\n애플리케이션이 배포되어 동작하는 런타임 기간 중에 보안, 네트워크 관련 이슈 등을 실시간으로 제어하는 것이 중요합니다. 런타임 보안 관련 툴로는 Falco 등이 있습니다.\n또한 애플리케이션 배포할 수 있는 오케스트레이션 툴 역시 여기에 포함되는데요. 오토스케일링이나 셀프 힐링(Self-healing), 로드 밸런싱 기능이 강력한 Kubernetes가 대표적인 툴이라고 할 수 있을 것입니다.\nAPI Gateway / Service Proxy\n두 요소 모두 애플리케이션을 외부로 서비스하기 위해 필수적인 것입니다.\n관련 툴로는 Envoy, Nginx, Traefik 등이 있습니다.\n모니터링\n소프트웨어 사이클에서 모니터링과 로그 추적은 정말 중요한 요소입니다. 애플리케이션에 발생한 문제를 해결하고 안정적인 서비스를 보장하는 데에 결정적인 역할을 하기 때문이죠.\n모니터링 관련 툴로는 ELK 스택, PLG 스택이 있습니다.\nFinOps 및 지속가능성 관련\nFinOps는 클라우드 운영에 소요되는 비용을 관리하는 활동을 의미합니다.\n그리고 여기서 말하는 지속가능성이란, 사용하지 않는 리소스의 동작을 멈춰 불필요한 에너지 소비를 줄이는 것을 말합니다.\n지속가능성과 비용 절약을 위한 툴로는 kube-green 등이 있습니다.\n데이터 관리\n애플리케이션 동작에 필요한 데이터뿐만 아니라, 서비스 운영에 사용되는 다양한 시스템으로부터 실시간으로 발생하는 데이터 역시 중요할 수 있습니다.\nReferences\n\nplatformengineering.org/blog/what-is-platform-engineering\nwww.techtarget.com/searchitoperations/tip/Platform-engineering-vs-DevOps-Whats-the-difference\nwww.cncf.io/blog/2023/04/28/7-core-components-of-an-internal-developer-platform/\n"},"index":{"title":"🔭 DevOps 여행을 위한 안내서","links":[],"tags":[],"content":"\n\n\nAiden Kim\nDevOps Engineer / Kubestronaut\n💻Working as a DevOps engineer\n🔗Passionate in pipelining and processing\n✨️Interested in sharing DevOps and Cloud knowledge\n안녕하세요, DevOps와 클라우드 관련 지식을 공유하는 DevOps 여행을 위한 안내서에 오신 것을 환영합니다.\n뉴스레터 DevOps 여행을 위한 소식지를 구독하시면 DevOps 여행을 위한 안내서의 글을 주기적으로 받아보실 수 있습니다.\nContact\nEmail: edu.ukulelekim@gmail.com"}}